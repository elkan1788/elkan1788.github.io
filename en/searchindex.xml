<?xml version="1.0" encoding="utf-8" standalone="yes"?><search><entry><title>GitExtensions push Github failed logging</title><url>https://lisenhui.cn/en/2021/02/22/git-extensions-push-fail.html</url><categories><category>Tools</category></categories><tags><tag>Git</tag><tag>Tools</tag></tags><content type="html"> Problem Description 555, today experienced a &ldquo;pleasure&rdquo; to install the latest program!!!
When you used GitExtensisons to push the latest written article to Github, I encountered an inexplicable error that invalidated SSH KEY authentication. Here&rsquo;s how it happened: Today, when you first open Git Extensions, it&rsquo;s very friendly to pop up the update prompt window, and then you click the confirmation button unconsciously. When the results are updated, an error blocking as magical as the following occurs when pushing the article to Github:
FATAL ERROR: No supported authentication methods available (server sent: publickey) fatal: Could not read from remote repository. Please make sure you have the correct access rights and the repository exists. See this error is really a face of hair, ah, did not modify any SSH KEY-related configuration, there is no corresponding permissions to operate it?
The first thing that came to mind was, was the local SSH KEY cleaned up? However, after examining the file, everything is fine, and it is normal to use the git push command. It&rsquo;s really hard to understand, for the time being can only refer to the wrong prompt to come out and try to fix the operation.
Preliminary Solution Based on the prompts of the error window, use the Putty tool to generate a Private mode for the local SSH KEY, as follows:
Then load this Private KEY into the push process and click the push button again to see the action success prompt.
Problem positioning Although the problem of pushing was solved, I still found things a little strange and strange. So I thought about whether the configuration of Git Extensions has changed, after some search testing, confirmed that it is due to the official current default in Windows to use Putty as a client, adjust it to OpenSSH mode, the problem no longer occurs.
Summary In case of non-necessity, it is still less recommended to upgrade the software version, and a stable environment is more valuable than new features that are not available.</content></entry><entry><title>The process of optimizing the Hugo Next theme</title><url>https://lisenhui.cn/en/2020/10/02/make-next-theme-pithy.html</url><categories><category>Blog</category></categories><tags><tag>NexT</tag><tag>Blog</tag></tags><content type="html"> 1. Background After some consideration or moving your personal blog from the Hexo engine to the Hugo engine, the blog theme is still NexT. In fact, I was worried about tossing up a brand new blog theme, and then I saw the &ldquo;NexT&rdquo;(https://github.com/xtfly/hugo-theme-next) theme shared by Lanlings on Github and quotedit directly. However, after deployment, there are areas that need to be improved, and the process of retrofitting optimization is documented here. If you also like this topic, then welcome to use, also welcome to exchange feedback.
2. Comment function Comment function is a more important function of blog space, as an important bridge between bloggers and readers, it is naturally indispensable. LiveRe,which had been used before, recently found that access was not stable and did not support visitor review mode, so consider using Valine as comment support, but in the endboth were implemented.
2.1 LiveRe LiveRe(http://livere.com/) is South Korea&rsquo;s largest third-party social commentary system and has been using it as a comment box for blogs since itwas posted. Individual developers can register a free version of City on the official website, and its integration is simple, loading the following code directly on the blog&rsquo;s JavaScript page:
{{ if and (. IsPage) (isset . Site.Params "comment") (eq . Site.Params.Comment "LiveRe") }} &lt;script type="text/javascript"> (function(d, s) { var j, e -getElementsByTagName(s)[0]; if (typeof LivereTower === 'function') { return; } j-d.createElement(s); j.src = '//cdn-city.livere.com/js/embed.dist.js'; j.async = true; e.parentNode.insertBefore(j, e); })(document, 'script'); &lt;/script> {{ end }} Then define a Div element where you want to appear in the comment box, as follows:
{{ if and (isset . Site.Params "comment") (eq . Site.Params.Comment "LiveRe") }} &lt;div id="lv-container" data-id="city" data-uid="{{ . Site.Params.LiveReId }}"> {{ end }} The final effect is as follows:
2.2 Valine Valine, a fast, concise, and efficient back-end review system based on LeanCloud. The official documentation is very detailed and will not be repeated here, with the following results:
Note that because LeanCloud&rsquo;s SDK references are integrated in TheValine, you no longer need to reference the relevant SDK to use learnCloud features yourself, and then conflict.
3. Access statistics As a blogger, you&rsquo;re definitely more concerned about access to your space and related data, such as PU and UV traffic, and you can help us collect it with some existing platforms.
Blog Space Access Statistics
Statistics and related data collection for blog space access can be achieved by the likes of CNZZ, Baidu,Google (which may be walled),GrowingIO, etc. (you can also integrate your own familiar platforms), and the buried scripts of these platforms are also supported for integration. This time is mainly integrated CNZZ, Baidu and Google, but the data of these platforms only the station director has permission to view, so the introduction of non-garlic counter, the site PU and UV data public display, the effect is as follows:
Article access statistics
In addition to spatial access data, the heat of the article can also be counted, previously using LearnCloud as a background count on NexT, this time with the article counter feature that comes with the Valine comment plug-in above. But considering the possibility of referencing a LiveRe comment plug-in, port the relevant code above the original Hexo and update the latest LearnCloud SDK code to achieve article heat statistics regardless of which comment plug-in you are referring to.
When you add this statistical function, fix the ICON icon associated with the original article.
4. SEO optimization In order for the blog space to be better, not only need to write more original works, but also need a certain amount of site SEO optimization support.
4.1 sitemap .xml build
Sitemap file generation is conducive to the site inged platform, Hugo generation sitemap file should pay attention to the file header generation, the overall code is as follows:
{{ printf "&lt;?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"yes\" ?>" | safeHTML }} &lt;urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"> {{ range . Data.Pages }} &lt;url> &lt;loc>{{ . Permalink }}&lt;/loc> &lt;lastmod>{{ safeHTML ( . Date.Format "2006-01-02T15:04:05-07:00" ) }}&lt;/lastmod> {{ with . Sitemap.ChangeFreq }} &lt;changefreq>{{ . }} &lt;/changefreq> {{ end }} {{ if ge . Sitemap.Priority 0.0 }} &lt;priority>{{ . Sitemap.Priority }}&lt;/priority> {{ end }} &lt;/url> {{ end }} &lt;/urlset> Finally, submit this file path to the corresponding ingest platform, such as the following:
Baidu: (https://ziyuan.baidu.com / siteadd? siteurl) Google: (https://search.google.com/search-console/welcome) 4.2 bshare share
In addition, through the site&rsquo;s own sharing function, you can quickly share articles to different readers or other platforms. This time using the BShare plug-in, you can quickly generate sharing links from different platforms, readers only need a click to quickly share, the effect is as follows:
The HTTPS reference issue with BShare is currently resolved through the Meta tag, but several references inside are invalid and output some error messages in the console without affecting rendering of the entire page. This issue has been submitted for BShare feedback and looks forward to further upgrade fixes.
5. Introduce yourself The original NexT theme does not have a self-introduction page, refer to the original Hexo theme in the personal introduction page, add some shortcode code, to achieve a different personal information introduction page from the article, the effect is as follows:
6. Local search Local search can quickly retrieve relevant content through article titles or content keywords, and the principle is simpler, which is to extract the article title and content into an XML text record, and then read the parsing through the JavaScript script. The implementation in the original theme is parsed through sitemap.xml, but this will conflict with the real sitemap.xml file product, and then use robots.txt files for storage, while correcting related styles and icons such as positively related pop-ups, the final effect is as follows:
7. Public service 404 The introduction of Tencent&rsquo;s 404 public welfare page, although there is no traffic on this site, but also hope that in this way so that more separated children can go home as soon as possible.
8. Chat online Comment function can achieve communication with readers, but real-time is not very strong, then online communication is a good way. DaoVoice(http://blog.daovoice.io/) is a great online chat product, but also for free version opportunities, integration is also quite simple, as long as theScript reference place to add the following code:
daovoice('init', { app_id: "xxxxx" }); daovoice('update'); &lt;script>(function(i,s,o,g,r,a,m){i["DaoVoiceObject"]=r;i[r]=i[r]|| function(){(i[r].q=i[r].q|| []).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;a.charset="utf-8";m.parentNode.insertBefore(a,m)})(window,document,"script",('https:' == document.location.protocol ? 'https:':'http:') + "//widget.daovoice.io/widget/xxxxx. js","daovoice")&lt;/script> The results are as follows:
9. Picture browsing By directly referencing the picture in the article will be compressed, or zoomed out, can not see the original image clear. Previously NextT&rsquo;s own picture browsing plug-in wasn&rsquo;t very useful, so replacing it with ImageViewer for image browsing within the article would have a slide-like effect, as follows:
10. Other optimizations For HTTPS traffic billing reasons, all unrelated factors on the page are excluded, various JavaScript class libraries and CSS styles are replaced with CDN links, and compression mode is turned on to make web pages smaller.
In addition, the addition of tag cloud, 3D access display, reward function, fix display problems and other small details of theprocessing, so that the entire blog site features look more complete.
Finally, the effect of the whole station, as you can see now, still maintains the refreshing interface style of the NexT theme.</content></entry><entry><title>Several pit records during CDH6 installation</title><url>https://lisenhui.cn/en/2020/09/28/install-cdh-issues-notes.html</url><categories><category>Big data</category></categories><tags><tag>Big data</tag><tag>CDH</tag></tags><content type="html"> In fact, CDH environment deployment and installation is not difficult, is the so-called mature can make a coincidence. But it just doesn&rsquo;t happen that it&rsquo;s been too long to operate, that is, there will be some &ldquo;strange&rdquo; problems, and then take some effort to solve the problem, and then record them down the road, to avoid recides later.
1. CdH&rsquo;s metadata initialization script As everyone who has installed the CDH environment knows, after the CM installation is complete, there is a script named:&lsquo;scm_prepare_database.sh&rsquo;, which is officially used to initialize the CDH meta-database, so everyone is sure to move the execution step by step. But I don&rsquo;t know if you&rsquo;ve ever tried to figure out if there&rsquo;s really any work behind it. In other words, what&rsquo;s wrong with not executing this script?
In past experience installing CDH environments, CM and MySQL databases have generally been installed on the same machine (non-production environment). But this time it happens to be built in a cloud environment, so MySQL is directly using the cloud service, the result is that after installing cm, executing the &lsquo;scm_prepare_database.sh&rsquo; script, the start-up CM did not appear the expected success message. Looking at the startup log found the following error:
Tip &lsquo;scm.cm_version&rsquo;table does not exist, is there a problem before executing scm_prepare_database.sh&rsquo;s ' script? The script is then re-executed to determine that the output was successful, but the CM boot still fails. At that time really wondered, this CM meta-database is at what stage of initialization?
After some attempt and validation, confirm that the &lsquo;scm_prepare_database.sh&rsquo; script does not initialize the CM&rsquo;s metadata database, but simply generatesa&rsquo;db.properties&rsquo; file,and creates a database with a specified name, which is performed when the CMstarts for the first time.
Conclusion: The installation of CM does not necessarily require the execution of a &lsquo;scm_prepare_database.sh&rsquo; script, you can manually create a databaseand configure a &lsquo;db.properties&rsquo;file.
2. MySQL 5.7 plus version issue The problem encountered in step 1 earlier, in fact, when analyzing the log later, found that the root cause was that some DDL syntax did not support the initialization when CM performed the database initialization, which resulted in the initialization not being completed. Some of the error logs are as follows:
However, CM&rsquo;s prompt information is not friendly, did not tell the CM metadata database initialization is completed, resulting in positioning problems a bit difficult challenges, and then manually adjust the DDL syntax to complete the initialization work.
The lesson here is that under normal circumstances the CM meta-database generates a table of 54 sheets, which can be used as a way to determine whether the CM initialization work is complete.
Another problem is MySQL GTID, which causes the build table to fail all the time:
Error code: 1786 Statement violates GTID consistency: CREATE TABLE ... SELECT. Re-reference: smh.com.u sqL Statements violates GTID consistency: CREATE TABLE &hellip; SELECT. Error code: 1786 Questions (https://www.cnblogs.com/zzw-zyba/p/8044960.html).
It is said that the 2 parameter configuration is to be turned off, but the system configuration parameter modification is not supported due to the use of database components on the cloud. Finally, we had to set up a MySQL service on-premises, wait for the CM initialization work to be completed, and then sync the table structure and data to the cloud database, the problem can be solved.
3. Hosts profile error When installing the Yarn service component, you always report that the JAR package that uploaded Mapreduce failed, and viewing the log information indicates that the HDFS directory could not be created, informing you that there is no permission to execute. So go to temporarily adjust the directory permissions, but disappointed that the installation is still a failure, or report the same error.
When I reanalysed the log, I found that there appeared to be something wrong with HostName&rsquo;s writing, so I compared the Hosts file with the machine&rsquo;s HostName,and theresults were really different. Since the Hadoop node machine was ready, the cloud-on-cloud synchronization creation feature was used to automatically add the corresponding serial number after HostName, but I didn&rsquo;t expect the serial number to be a 4-digit number, but only 3 digits were written when filling it in the Hosts file. It&rsquo;s true: &ldquo;It&rsquo;s a thousand miles, it&rsquo;s a thousand miles.&rdquo; When the Hosts file configuration is re-adjusted, all installations and startups succeed.
Unfinished updates&hellip;</content></entry><entry><title>Stop using the original Chinese domain name announcement</title><url>https://lisenhui.cn/en/2020/09/17/stop-use-chinese-domain-notice.html</url><categories><category>Blog</category></categories><tags><tag>Blog</tag></tags><content type="html"> From today, &lsquo;lisenhui.cn&rsquo; will be officially launched as the unique domain name for this blog space.
In the morning received a notification text message from the domain name service provider, suggesting that the domain name needs to be renewed. Only found that unknowingly, the original work after the registration of the first domain name, has accompanied themselves through 7 years. At that time, Chinese the domain name is more special, and then on their own name registered &lsquo;李森辉.cn&rsquo;sdomain name.
However, it is still decided to deprecation of this Chinese domain name, because considering that the Chinese domain name is not yet mature, in these years the process of use is always encountered a variety of problems, although later found a way around the past to solve, but after all, it is not convenient.
So the impact, can only be said to be the follow-up slowly revised it. (But the traffic on this site is also general)</content></entry><entry><title>Blog Engine Migrates to Hugo Program</title><url>https://lisenhui.cn/en/2020/08/15/blog-move-2-hugo-plan.html</url><categories><category>Blog</category></categories><tags><tag>Blog</tag><tag>Hugo</tag></tags><content type="html"> Recently found that their personal blog space suddenly ind accessible, after a check found that the original is the previous use of page service providers have stopped providing services. I had no choice but to relocate back to Github Pages. But that&rsquo;s the development environment that&rsquo;s been re-prepared for Hexo, and it&rsquo;s going to take a hard time debugging to be successful. And just then on the Internet have seen Hugo static site engine article, a Go language-based development of the extremely fast framework, development environment deployment is also simple and fast. In addition, the original domain name is also about to expire, just together to organize the blog space.
Visit Hugo&rsquo;s official website and look through the official documents, which are actually easier to use. But when you browse an existing topic, you don&rsquo;t find the NexT theme you&rsquo;re using in your blog space, does that mean you&rsquo;re going to have to do it all over again!
But it&rsquo;s good to finally find someone in Github who has transplanted the Hexo NexT theme:&ldquo;Hugo-theme-next&rdquo; (https://github.com/xtfly/hugo-theme-next), so the subsequent migration plan is based on this, and the overall thinking and plan is as follows:
Considering that they are all done in their spare time, the timeline is longer and I don&rsquo;t know if the problems encountered can be solved smoothly. No matter how much you do, everything has to have a Flag first, follow-up efforts to achieve Flag is good.</content></entry><entry><title>Install a font on linux Chinese font</title><url>https://lisenhui.cn/en/2019/10/21/install-linux-chinese-fonts.html</url><categories><category>Linux</category></categories><tags><tag>Linux</tag></tags><content type="html"> Background
Usually rarely use the UI desktop on Linux service machines, but there is still a chance to encounter, this does not today encounter the Linux version of firefox browser display Chinese garbled code. No matter how to debug the browser&rsquo;s related settings, there is no way to work together, even a little depressed.
The font is installed
Before debugging browser settings, in the font settings column found that there is no font library suitable for Chinese display, that means installing a font can solve the problem. Find a Microsoft Ya black font library (&lsquo;msyh.&rsquo; from the Windows system ttc,msyhl. ttc',&lsquo;msyhbd.ttc&rsquo;)and upload it to the specified directory of theLinux server: &lsquo;/usr/share/fonts&rsquo;
Note: You can create a folder in this directory to hold Microsoft&rsquo;s 3 font library files for easy management
Then install the font management tool with the &lsquo;yum&rsquo; command, as follows:
yum install -y fontconfig mkfontscale Finally, verify that the font installation was successful, and the command is as follows:
[root@quickstart fonts]# fc-list :lang=zh Microsoft YaHei,微软雅黑:style=Regular,Normal,oby?ejné,Standard,Κανονικ?,Normaali,Normál,Normale,Standaard,Normalny,Обычный,Normálne,Navadno,Arrunta Microsoft YaHei UI,Microsoft YaHei UI Light:style=Light,Regular Microsoft YaHei UI:style=Regular,Normal,oby?ejné,Standard,Κανονικ?,Normaali,Normál,Normale,Standaard,Normalny,Обычный,Normálne,Navadno,Arrunta Microsoft YaHei, Microsoft YaHei Light, Microsoft YaHei Light, Microsoft YaHe Light: Style,General Microsoft YaHei UI:style'Bold,Negreta,tu?né,fed,Fett,,Negrita,Lihavoitu,Gras,Félk?vér,Grassetto,Vet,Halvfet,Pogrubiony,Negrito,Fet,Kal?n,Krepko,Lodia Microsoft YaHei,微软雅黑:Style Bold,Negreta,born,fed,Fett,,Negrita,Lihavoitu,Gras,Félk?vér,Grassetto,Vet,Halvfet,Pogrubiony,Negrito,Fet,Kal?n,Krepko,Lodia,Lodia If you can display the words &ldquo;Microsoft Ya Black&rdquo;, it means that the display is successful, and then to Firefox&rsquo;s advanced settings to adjust the font options to Microsoft Ya Black is available, the effect is as follows:
Summary
In addition to finding the &lsquo;lang&rsquo; setting, you need to be concerned about the font library.
Think differently!!!</content></entry><entry><title>Temporary files cannot be created on the HDFS Data node</title><url>https://lisenhui.cn/en/2019/03/21/unable-create-tmp-file-in-hdfs-nodes.html</url><categories><category>Big data</category></categories><tags><tag>Hive</tag><tag>Big data</tag></tags><content type="html"> On the newly created &lsquo;Hadoop&rsquo; edge node, an attempt was made to insert data through the &lsquo;Hive CLI&rsquo; mode, resulting in no intended success, but instead an exception was caught as follows:
FAILED: SemanticException [Error 10293]: Unable to create temp file for insert values File /tmp/hive/kylin/9c84de0a-fca2-4d3c-8f72-47436a4adb83/_tmp_space.db/Values__Tmp__Table__1/data_file could only be replicated to 0 nodes instead of minReplication (=1). There are 1 datanode(s) running and 1 node(s) are excluded in this operation. at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1720) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3440) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:686) at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:217) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:506) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2226) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2222) at java.security. AccessController.doPrivileged(Native Method) at javax.security.auth. Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2220) ERROR: Current user has no permission to create Hive table in working directory: /user/kylin From the exception prompt information, initially determined that the &lsquo;/user/kylin&rsquo; directory does not have permissions (a little strange is why &lsquo;kylin&rsquo; users do not have permission to operate), simply and directly reduce its permissions to 777, the error is still present. Then try switching to Hive&rsquo;s &lsquo;Beeline&rsquo; connection, repeating the original insert statement, and the operation was successful! So what caused the mistake above?
With the help of a powerful Google search to find a bit, the results are different: some say &lsquo;HDFS&rsquo; storage space is insufficient, some say the firewall of the cluster node is not turned off, some say &lsquo;DataNode&rsquo; service exception, and so on. Online programs have been tried, the problem is still unsolved. By the previous firewall association will not be IP caused by the problem.
Because the cluster is built by a local virtual machine and happens to have a dual network card configured, the edge node is connected to a set of static &lsquo;IP&rsquo; addresses. Here&rsquo;s the following:
eth0 Link encap:Ethernet HWaddr 08:00:27:B2:38:58 inet addr:10.0.2. 15 Bcast:10.0.2.255 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:797 errors:0 dropped:0 overruns:0 frame:0 TX packets:944 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:98791 (96.4 KiB) TX bytes:84770 (82.7 KiB) eth1 Link encap:Ethernet HWaddr 08:00:27:B5:9D:6A inet addr:192.168.56. 104 Bcast:192.168.56.255 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:3523935 errors:0 dropped:0 overruns:0 frame:0 TX packets:443589 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:5073146719 (4.7 GiB) TX bytes:163351146 (155.7 MiB) lo Link encap:Local Loopback inet addr:127.0.0. 1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:342031 errors:0 dropped:0 overruns:0 frame:0 TX packets:342031 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:405110832 (386.3 MiB) TX bytes:405110832 (386.3 MiB) Then check the file configuration of the next'/etc/hosts',and the result is that the default setting &lsquo;10.0.2.15&rsquo; address is cluster &lsquo;IP&rsquo;, modify it to a static &lsquo;IP&rsquo; address and restart all services of the &lsquo;Hadoop&rsquo; cluster, connect Hive again through &lsquo;Hive CLI&rsquo; mode, and execute all the previous insert statements normally.
Summary
Configuring a &lsquo;Hadoop&rsquo; cluster pays special attention to the allocation of &lsquo;IP&rsquo; addresses,and it is recommended to avoid &lsquo;IP&rsquo; address issues in the form of&rsquo;HostName'. Also, when there are already cases that do not help resolve the issue, you can carefully examine the configuration of the environment.</content></entry><entry><title>About</title><url>https://lisenhui.cn/en/about.html</url><categories/><tags/><content type="html"> Hope is always there,but you can say it is never there.
This is just like the way on ground:it becomes a way when people walk on it.
Xun.Lu
NickName
Elkan.Li(elkan1788)
Email： elkan1788@gmail.com
Old Blog History
CSDN Blog ITEye Blog1 ITEye Blog2 GitHub Page A few words of long-winded A sieged lion/code farmer running for survival in the metropolis, perfection is the basic requirement for code. (Once)
Quiet appearance hides a heart of &ldquo;revolutionary rebellion&rdquo;, not afraid of the secular view dare to break through the bondage, like to pursue the real freedom. With a dream in my heart, hoping to make it come true one day.
Thanks!</content></entry><entry><title>Win10's most common shortcuts</title><url>https://lisenhui.cn/en/2018/08/08/win10-quick-operations.html</url><categories><category>Windows</category></categories><tags><tag>Windows</tag><tag>Tips</tag></tags><content type="html"> As the saying goes, if you want to do good things, you must first use the best. ”
But the transition back from &lsquo;Mac&rsquo; to &lsquo;Windows&rsquo; is really a lot of unaccustomed, but still to learn to overcome it, without it, work is the fundamental skill of survival. So it&rsquo;s interesting to pick up the distribution of the &lsquo;Win10&rsquo; shortcut from the web. The records are also some of the common shortcuts in the following sections, as follows:
The action gesture
Two-fingered click on the touchpad, simulate the right mouse button, that is, pop-up menu
Three fingers click on the touchpad and pop up the search box
Four-finger click on the touchpad, eject the operation center, that is, simulate Win-A
Three fingers are crossed at the same time, and the multitasth interface is ejected, i.e. the simulated Win-Tab
Three fingers are underlined at the same time, minimizing all windows, i.e. simulating the display of the desktop
Three fingers are crossed to the left/right at the same time, quickly switching tasks, i.e. simulating Alt-Tab
Double finger at the same time left/right, switch the last/next item, for browsing the picture or &ldquo;start&rdquo; and other horizontal layout program scroll
The shortcut key
Create a new virtual desktop: &lsquo;Win s Ctrl s D&rsquo;
Close the current virtual desktop: &lsquo;Win s Ctrl s F4&rsquo;
Multi-desktop switching: &lsquo;Win s Ctrl s/left/right&rsquo;
Quickly open the search: &lsquo;Win and Q&rsquo;
Quickly open the Win10 settings bar: &lsquo;Win plus I&rsquo;
Temporarily view the desktop: &lsquo;Win plus,&rsquo;
Minimize all windows:&lsquo;Win-M&rsquo;
Open a new instance of the program located in the taskbar specified location:&lsquo;Win-Shift-NumericKey&rsquo;
Maximize Window (Traditional Desktop): &lsquo;Win - Up Key&rsquo;
Minimize window (traditional desktop): &lsquo;Win - Down Key&rsquo;
Maximize the window to the left side of the screen (traditional desktop): &lsquo;Win - left key&rsquo;
Maximize the window to the right side of the screen (traditional desktop): &lsquo;Win and right&rsquo;
Forward: &lsquo;Alt and Right Key&rsquo;
Back: &lsquo;Alt - Left Key&rsquo;
Screenshot (save to memory): &lsquo;Win s Shift s&rsquo;
The above quick operation is personally verified after available, for reference only, the follow-up of new discoveries will continue to update, welcome to pay attention to, thank you.</content></entry><entry><title>Axure tutorial: Dynamic panel content is displayed beyond the bounds</title><url>https://lisenhui.cn/en/2018/03/12/axure-lightbox-shade.html</url><categories><category>Products</category></categories><tags><tag>Axure</tag><tag>Products</tag></tags><content type="html"> The problem
As user needs are constantly updated and product prototype designs are being upgraded iteratively, it is important to make the overall design more complex, with more factors affecting each component. This is not now encountered when a hidden component is displayed on the dynamic panel, the resulting pull-down component is not fully displayed, really good is depressed, as shown in the following image:
The main key points can be analyzed from the appearance of the problem as follows:
The hidden component layer location is not the top layer, causing the display to be in the wrong position The size of the dynamic panel limits the area where the hidden element is displayed The solution
After trying multiple solutions, you find an optimal solution in just two steps, as follows:
Top-level settings
Navigate to the click event that shows the hidden symbol and place it at the top when displayed, as shown in the following image:
Panel adaptation
Position to the panel where the hidden element is located, and on the properties of the panel, the content size is automatically adjusted to tick, as shown in the following image:
A preview of the effect
After you&rsquo;ve done the above 2 steps, you can see the following effect:
OK, so far we have achieved the problem we want to solve, encountered problems can be a little more Axure&rsquo;s various settings, there will be unexpected effects, ha.</content></entry><entry><title>Axure tutorial: Implement table data presentation</title><url>https://lisenhui.cn/en/2017/12/29/axure-datalist-table.html</url><categories><category>Axure</category></categories><tags><tag>Products</tag><tag>Axure</tag></tags><content type="html"> Typically, in the system management background, it is most common to present data in the form of lists (tables). It&rsquo;s not easy to implement this list of data when you&rsquo;re using Axure to prototype your product, perhaps using rectangles to piece it together, or to lay it out directly using table controls. But this is not very convenient, first of all, layout trouble, second, data modification is more troublesome. Next, let&rsquo;s show you how to implement a list of data using the Table plus Repeater control.
In fact, in the actual prototyping process, will be in the table and repeater on the basis of the addition of a rectangular box to use together. This is also due to helplessness, the table above Axe can not achieve cell consolidation. Therefore, tables can usually only be used as headers in the data list, and then use the repeater&rsquo;s data padding capabilities to present parts of the data. Rectangular boxes play a powerful role when you encounter cells that need to be merged. Here&rsquo;s a look at how the next repeater displays the data:
Create a repeater and double-click into the repeater to remove the initial contents
Create a rectangular box with the same number of columns as the table, be customizable in height, keep the width the same as the corresponding column in the table, and name each component (with your favorite style, the current style will be copied by subsequent data)
Select the repeater, find &lsquo;Repeater&rsquo; in the property (&lsquo;Properties&rsquo;), create a column with the same number of columns in the table and name it (it is recommended to keep the same name as the previous step), and finally populate the sample data Note: You can edit the data directly in Excel and copy it directly into the repeater
Select the repeater and add a Case to bind the relationship between the data and the rectangular box Set the interlaced color change effect, select the repeater, find &lsquo;Item Background&rsquo; in &lsquo;Style&rsquo; check &lsquo;Alternating&rsquo; and pair the foremost colors of the odd even lines Note: If you use a rectangular box in the repeater, be sure to set its background color to None, otherwise interlaced color change will not work, this lesson is painful.
These are about implementing table data implementations in Axure, which can be used as a reference to freely imagine if you encounter some complex requirements.
The overall effect is as follows:
PS:
Sample source file download:data table. . rp</content></entry><entry><title>Axure Tutorial : Implementing a dynamic matte layer</title><url>https://lisenhui.cn/en/2017/12/15/axure-lightbox-shade.html</url><categories><category>Axure</category></categories><tags><tag>Products</tag><tag>Axure</tag></tags><content type="html"> Today, while doing product prototyping, I encountered a difficulty with dynamically emerging the matte layer. &ldquo;Helpless&rdquo; in pursuit of high fidelity effect, or spent a little effort to do a prototype to achieve. To do a good job back to see the words, in fact, the effect of the difficulty is not great, just to see whether the personal will want to do it. Axure itself provides the functionality of a template, that is, as long as it is implemented once but once and for all. Let&rsquo;s take a look at the process and effect of this mask layer.
Students who do front-end development know that implementing a mask layer in HTML requires only adding a floating DIV to make it easy. So how do you implement it in Axe?
As shown in the figure above, the implementation of this mask layer can be divided into two parts:
The main body content, i.e. the part of the mask layer to cover The matte layer component, i.e. the mask layer and other decorative parts (in Demo, only a loaded animated picture is added to distinguish). So the implementation of the mask layer is clear as follows:
Prepare a rectangular box the same size as the one you want to cover, note to subtract the size of the border, for example: the body content size is &lsquo;600x400&rsquo;, the border width is 1px, then the size of the mask layer is &lsquo;598 x 398&rsquo; and is borderless
Set the fill color of the matte layer with relative transparency
Enhance the animation effect of the mask layer display (not many effects are supported on Axure, and if they are not met, the effect requirements can be clarified with a text description).
Then use three buttons to show the different effects:
Open the mask layer Close the mask layer Automatic presentation So now let&rsquo;s take a look at the final implementation as follows, see the big screen below (https://7m9t2k.axshare.com/).
It&rsquo;s easier if you want to do a full-screen matte layer, just add the effect of a light box to the display component.
PS:
Sample source filedownload: rp</content></entry><entry><title>Apache Nifi builds pseudo-clusters and certificate logins in a Windows environment</title><url>https://lisenhui.cn/en/2017/10/26/nifi-windows-local-cluster.html</url><categories><category>Big data</category></categories><tags><tag>Big data</tag><tag>Nifi</tag></tags><content type="html"> Some time ago did about the &lsquo;Apache Nifi&rsquo;distributed clusterset-up sharing, but a lot of times to build distributed cluster machine resources is a problem, and now the stand-alone configuration is quite good, so now do a share about building a pseudo-distributed cluster on Windows, and through another way to achieve the authorization of the &ldquo;Apache Nifi.
The system environment and software version
Windows8.1
JDK1.8.0_131
Nifi-1.4.0
Nifi installation directory The WEB port xxx\nifi-ncm 9443 xxx\nifi-cluster01 9444 xxx\nifi-cluster02 9445 (Other versions can refer to this article) Another problem in testing, using&rsquo;Zookeyer&rsquo; embedded in &lsquo;Apache Nifi&rsquo;to build pseudo-clusters, always prompts for port occupancy issues, so give up using only single-node startup.
The servicecertificate of Nifi
Build a localNifiservice certificate
After unziwing the &lsquo;nifi-toolkit-1.4.0-bin.tar.gz&rsquo; file, enter the &lsquo;bin&rsquo; directory via cmD to execute the following commands:
D:\DevelopTools\nifi-toolkit-1.4.0\bin>tls-toolkit.bat standalone -n "localhost( 3)" -C "CN=Admin, OU=ApacheNIFI" -o ".. \target" 2017/10/26 18:21:32 INFO [play] org.apache.nifi.toolkit.tls.standalone. TlsToolki tStandaloneCommandLine: No nifiPropertiesFile specified, using embedded one. 2017/10/26 18:21:32 INFO [play] org.apache.nifi.toolkit.tls.standalone. TlsToolki tStandalone: Running standalone certificate generation with output directory .. \ target ****************************************************************************** 2017/10/26 18:21:34 INFO [play] org.apache.nifi.toolkit.tls.standalone. TlsToolki tStandalone: Successfully generated client certificate .. \target\CN=Admin_OU=Apa cheNIFI.p12 2017/10/26 18:21:34 INFO [play] org.apache.nifi.toolkit.tls.standalone. TlsToolki tStandalone: tls-toolkit standalone completed successfully The resulting directory structure is as follows:
Folder PATH listing for volume senhui.li Volume serial number is 000000F0 FA46:A0EB D:. │ CN=Admin_OU=ApacheNIFI.p12 │ CN=Admin_OU=ApacheNIFI.password │-true.pem │ nifi-key.key │ ├─localhost │ keystore.jks │ nifi.properties │ truststore.jks │ ├─localhost_2 │ keystore.jks │ nifi.properties │ truststore.jks │ └─localhost_3 keystore.jks nifi.properties truststore.jks Note: the spaces in the middle of the &ldquo;CN-Admin,OU-ApacheNIFI"must be retained
Copy theNifiservice certificate
Copy files from the &lsquo;localhost&rsquo; directory to the&rsquo;nifi-ncm&rsquo;directory to replace all files Copy the localhost_2 the &lsquo;Created&rsquo; directory to the &lsquo;nifi-cluster01&rsquo; directory to replace all files Copy the localhost_3 the &lsquo;Created&rsquo; directory to the &lsquo;nifi-cluster02&rsquo; directory to replace all files Copy&rsquo;CN-Admin_OU-Admin_OU-ApacheNIFI.p12&rsquo;and &lsquo;CN-Admin_OU-ApacheNIFI.password&rsquo;to the desktop standby forsubsequent logins Configure a single point Zookeeper-related
Create a directory and id
Go to the&rsquo;nifi-ncm&rsquo;directory, create the woker directory, and write the server id into the file with the following command:
D:\DevelopTools\nifi-ncm>mkdir -p state\zookeeper D:\DevelopTools\nifi-ncm>echo -n '1' > state/zookeeper/myid UpdateZK Placement Go tothe &lsquo;nifi-ncm&rsquo;conf directory andopen the&rsquo;zookeyer.properties&rsquo;file, which is updated as follows:
clientPort=2181 initLimit=10 autopurge.purgeInterval=24 syncLimit=5 tickTime=2000 dataDir=./state/zookeeper autopurge.snapRetainCount=30 Only the port service needs to be configured server.1=localhost:2181 Update Nifi Deployment Go tothe &lsquo;nifi-ncm&rsquo;confdirectory andopen the&rsquo;nifi.properties&rsquo;file to update the following configuration properties:
nifi.state.management.embedded.zookeeper.start=true # zookeeper properties, used for cluster management # The other two nodes, as long as you edit this field nifi.zookeeper.connect.string=localhost:2181 Update State Placement Go tothe &lsquo;nifi-ncm&rsquo;conf directory, open the &lsquo;state-management.xml&rsquo; file, and update the &lsquo;zookeyer&rsquo; configuration as follows:
&lt;cluster-provider> &lt;id>zk-provider&lt;/id> &lt;class>org.apache.nifi.controller.state.providers.zookeeper.ZooKeeperStateProvider&lt;/class> &lt;property name="Connect String">localhost:2181&lt;/property> &lt;property name="Root Node">/nifi&lt;/property> &lt;property name="Session Timeout">10 seconds&lt;/property> &lt;property name="Access Control">Open&lt;/property> &lt;/cluster-provider> Then copy this file to the same directory as &lsquo;nifi-cluster01&rsquo; and &lsquo;nifi-cluster02&rsquo;
Configure Nifi Admin
Add Admin users
Go tothe &lsquo;nifi-ncm&rsquo;confdirectory, open the &lsquo;.xml&rsquo; file, and find the configuration added by the file-provider:
&lt;authorizer> &lt;identifier>file-provider&lt;/identifier> &lt;class>org.apache.nifi.authorization.FileAuthorizer&lt;/class> &lt;property name="Authorizations File">./conf/authorizations.xml&lt;/property> &lt;property name="Users File">./conf/users.xml&lt;/property> &lt;property name="Initial Admin Identity">CN=Admin, OU=ApacheNifi&lt;/property> &lt;property name="Legacy Authorized Users File">&lt;/property> &lt;property name="Node Identity 1">CN=localhost, OU=NIFI&lt;/property> &lt;property name="Node Identity 2">CN=localhost_2, OU=NIFI&lt;/property> &lt;property name="Node Identity 3">CN=localhost_3, OU=NIFI&lt;/property> &lt;/authorizer> The file is then copied to two other node directories at the same time.
Note: The OU in &lsquo;Node Identity x&rsquo; is to be written as NIFI, and has tried to use a different name as if it were unsuccessful, the specific reason is unknown, interested can explore one or two on its own.
The installation certificate
Open Google Chrome, find the security options in the settings, click Onport to start importing the certificate generated above: &lsquo;CNs Admin_OUsapacheNIFI.p12&rsquo;, password in the suffix &lsquo;.password&rsquo; file, as shown in the following image: Start the Nifi service
Enter the Nifi installation directory, then find the run-nifi.bat file in the bin directory and double-click on the run, paying attention to the order of startup: nifi-ncm&ndash; nifi-cluster01/2, wait a moment (which may be a little too long, require an election process) to open the browser input &ldquo;https://localhost:9443/nifi"and select the certificate you justimported,as shown in the following screento start successfully: The user policy
When you first log on to the NIFI page, you&rsquo;ll notice that the icons are gray and you need to give permission to start editing before you can start editing. Clicking on the key icon on the left panel of the page will pop up the window for the access policy, as shown in the following image:
If you see that the list of users is empty here, you&rsquo;ll add users to the appropriate behavior, and click on the &ldquo;Create&rdquo; link to start adding them, as shown in the following image: Once all permissions have been added, you can see that the buttons on the NIFI page are lit and you can start the creation process.
An example demonstration
The template is uploaded
Download the &ldquo;DEMO&rdquo; () compression pack and unzip out a &lsquo;WordCountDemo.xml&rsquo; file. Then open the browser and enter the NIFI access address: &lsquo;https://localhost:9443/nifi/&rsquo;, click the upload button in the left panel to upload the template, as shown in the image below: The creation process
Drag the template button at the top of the NIFI page into the blank space of the artboard, click the add button, and double-clickon the&rsquo;WordCountDemo&rsquo;group to find the&rsquo;PutFile&rsquo; componentand modify the directory address for yourmachine&rsquo;sactual accessiblepath, as shown in the following image:
Start the process
Click the &lsquo;NiFi Flow&rsquo; link in the lower left corner of the NIFI page to return to the main panel, click on the&rsquo;WordCountDemo&rsquo; group,and then click the Start button in the left panel to startthe process, as shown in the following image:
If there are no exceptions, you can now find a filenamed &ldquo;Telltale_heart_wordcount"in the directory, and when you open it, you&rsquo;ll see the statistics for the following image:
At this point in the local construction of NIFI pseudo-cluster is complete, there are questions welcome message.</content></entry><entry><title>Apache Nifi clustering and user authentication with kerberos</title><url>https://lisenhui.cn/en/2017/10/22/ninfi-cluster-deploy-with-kerberos.html</url><categories><category>Big data</category></categories><tags><tag>Big data</tag><tag>Nifi</tag></tags><content type="html"> Recently, some of the lessons learned from the installation configuration are being shared with the help of the .Apache NIFI for contact with data streaming processing. This article is primarily about clusters and user rights, and the description of the &ldquo;Apache NIFI&rdquo; (https://nifi.apache.org/) is not much of a description, with a direct reference to the official home page as follows:
Apahce NIFI&rsquo;s stand-alone operation is fairly simple, easy to use, and completely fooly. Download the decompression and perform &lsquo;nifi.sh start&rsquo; on the &lsquo;bin&rsquo; directory to open the browser input &lsquo;http://127.0.0.1:8080/nifi&rsquo; to see a simple and beautiful WEB UI. So what we&rsquo;re going to configure next is its cluster pattern, which is officially stated, with each node in the cluster performing the same tasks on the dataset, but each node is running on a different dataset (see the official documentation for detailed instructions( https://nifi.apache.org/docs.html
The system environment and software version
CentOS7
JDK1.8.0_91
Nifi-1.4.0
Kerberos5
(Other versions can refer to this article)
HostName IP Services centos7-master 192.168.56.100 Kerberos5 Server, Nifi Cluster Manager centos7-cluster01 192.168.56.101 Kerberos5 Client, Nifi Cluster Build the Kerberos 5 service
Install KDC services and configurations
Enter the Master machine and perform the following command to install the KDC service:
yum -y install krb5-server krb5-libs krb5-workstation Note: The &lsquo;krb5-auth-dialo&rsquo; component found in the test is not available and does not need to be installed
Modify the KDC default configuration
Go to the &lsquo;/etc&rsquo;directory to find&rsquo;/etc/krb5.conf&rsquo;file to openand modify, refer to the following:
# Configuration snippets may be placed in this directory as well includedir /etc/krb5.conf.d/ [logging] default = FILE:/var/log/krb5libs.log kdc = FILE:/var/log/krb5kdc.log admin_server = FILE:/var/log/kadmind.log [libdefaults] dns_lookup_realm = false ticket_lifetime - 24 hours renew_lifetime - 7d forwardable = true rdns = false This comment needs to be turned on and fill in the default field default_realm = CENTOS7-MASTER.COM default_ccache_name = KEYRING:persistent:%{uid} [realms] The EXAMPLE file here .COM your own domain CENTOS7-MASTER.COM = { kdc = centos7-master admin_server = centos7-master Add the default domain default_domain = CENTOS7-MASTER.COM } [domain_realm] Change the EXAMPLE .COM to your own domain name .centos7-master.com = CENTOS7-MASTER.COM centos7-master.com = CENTOS7-MASTER.COM Modify the KRB5KDC profile
Go to the'/etc&rsquo;directoryto find'/var/kerberos/krb5kdc/kdc.conf&rsquo;file to open, refer to the following modifications:
[kdcdefaults] kdc_ports = 88 kdc_tcp_ports = 88 [realms] Modify the EXAMPLE domain name .COM here CENTOS7-MASTER.COM = { #master_key_type = aes256-cts acl_file = /var/kerberos/krb5kdc/kadm5.acl dict_file = /usr/share/dict/words admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab supported_enctypes = aes256-cts:normal aes128-cts:normal des3-hmac-sha1:normal arcfour-hmac:normal camellia256-cts:normal camellia128-cts:normal des-hmac-sha1:normal des-cbc-md5:normal des-cbc-crc:normal kdc_ports = 88 kadmind_port = 749 } Initialize the database
[root@centos7-master ~]# kdb5_util create -s Loading random data Initializing database '/var/kerberos/principal' for realm 'CENTOS7-MASTER. COM', master key name 'K/M@CENTOS7-MASTER. COM' You will be prompted for the database Master Password. It is important that you NOT FORGET this password. Enter KDC database master key: Re-enter KDC database master key to verify: Modify the database permissions
Find the &lsquo;/var/kerberos/krb5kdc/kadm5.acl&rsquo; profile, add ACL permissions to the database administrator, and act on behalf of all permissions, as follows:
[root@centos7 master ~]# we /var/kerberos/krb5kdc/kadm5.acl */admin@CENTOS7-MASTER.COM * Start the KDC service
service krb5kdc start service kadmin start Create a database administrator
Refer to the following command to create an administrator user, save the password you set when you create it (if you forget to update laterusing the &lsquo;cpw&rsquo; command),and exportthe keytab
[root@centos7-master ~]# kadmin.local -q "addprinc root/admin" Authenticating as principal root/admin@CENTOS7-MASTER.COM with password. WARNING: no policy specified for root/admin@CENTOS7-MASTER.COM; defaulting to no policy Enter password for principal "root/admin@CENTOS7-MASTER.COM": Re-enter password for principal "root/admin@CENTOS7-MASTER.COM": Principal "root/admin@CENTOS7-MASTER.COM" created. [root@centos7-master ~]# kadmin.local kadmin: ktadd -k /data/root.keytab root/admin kadmin: q [root@centos7-master ~]# kinit root/admin Install the KDC Client service
Enter the machine from cluster and perform the following command to install the KDC Clienteservice:
yum -y install krb5-libs krb5-workstation Update the configuration and test it
Copy the &lsquo;krb5.conf&rsquo;and&rsquo;root.keytab&rsquo;of the primarynode to the from node service, as follows:
[root@centos7-cluster01 ~]# scp root@centos7-master:/etc/krb5.conf /etc/krb5.conf [root@centos7-cluster01 ~]# scp root@centos7-master:/data/root.keytab /data/root.keytab [root@centos7-cluster01 ~]# kadmin -p root/admin Authenticating as principal root/admin with password. Password for root/admin@CENTOS7-MASTER.COM: kadmin: Copy the keytab file
Copy the&rsquo;root.keytab&rsquo;to the &lsquo;/data/root.keytab&rsquo;directory,note that this refers to all machines
Create a Nifi service certificate
Create a certificate
Unzim the &lsquo;nifi-toolkit-1.4.0-bin.tar.gz file and enter the &lsquo;bin&rsquo; directory to execute the following commands:
[root@centos7-master ~]# ./tls-toolkit.sh standalone -n 'centos7-master, centos7-cluster01' -C 'CN=admin, OU=ApacheNIFI' -o './target' -f '/usr/local/bin/nifi-ncm/conf/nifi.properties' [root@centos7-master target]# tree . ├── centos7-cluster01 │ ├── keystore.jks │ ├── nifi.properties │ └── truststore.jks ├── centos7-master │ ├── keystore.jks │ ├── nifi.properties │ └── truststore.jks ├── CN=admin_OU=ApacheNIFI.p12 ├── CN=admin_OU=ApacheNIFI.password yes-true.pem └── nifi-key.key -n indicates the hostname of the machine The directory of the output of -o
-f Nifi&rsquo;s profile location Copy the certificate
The copy generates a good certificate to the conf folder in the &lsquo;NIFI&rsquo; installation directory under the primary node server, as follows:
[root@centos7-master target]# scp centos7-cluster01/* centos7-cluster01:/usr/local/bin/nifi-cluster01/conf [root@centos7-master target]# cp target/centos7-master/* /usr/local/bin/nifi-ncm/conf/ Configure the Zookeeper service
Note: All master-access nodes require action
Create an id file
Go into the NIFI installation directory, create a &lsquo;state/zookeeper&rsquo;directory and a&rsquo;myid&rsquo;file, and then write thecorresponding ID to the file as follows:
[root@centos7-master nifi-ncm]# mkdir -p state/zookeeper [root@centos7-master nifi-ncm]# echo -n '1' > state/zookeeper/myid Note: The myid created from the node is 2, such as: &lsquo;echo -n &lsquo;2&rsquo; sgt; state/zookeyer/myid&rsquo;
Modify the profile
clientPort=2181 initLimit=10 autopurge.purgeInterval=24 syncLimit=5 tickTime=2000 dataDir=./state/zookeeper autopurge.snapRetainCount=30 # # Specifies the servers that are part of this zookeeper ensemble. For # every NiFi instance running an embedded zookeeper, there needs to be # a server entry below. For instance: # # server.1=nifi-node1-hostname:2888:3888 # server.2=nifi-node2-hostname:2888:3888 # server.3=nifi-node3-hostname:2888:3888 # # The index of the server corresponds to the myid file that gets created # in the dataDir of each node running an embedded zookeeper. See the # administration guide for more details. # Note that it is modified to your corresponding server address server.1=centos7-master:2888:3888 server.2=centos7-cluster01:2888:3888 Update the status configuration
Go to the Nifif installation directory to modify the &lsquo;conf/state-management.xml&rsquo; configurationand add the connection string under thezk-providernode
&lt;cluster-provider> &lt;id>zk-provider&lt;/id> &lt;class>org.apache.nifi.controller.state.providers.zookeeper.ZooKeeperStateProvider&lt;/class> &lt;property name="Connect String">centos7-master:2181,centos7-cluster01:2181&lt;/property> &lt;property name="Root Node">/nifi&lt;/property> &lt;property name="Session Timeout">10 seconds&lt;/property> &lt;property name="Access Control">Open&lt;/property> &lt;/cluster-provider> Update NIFI Deployment Go to the Nifif installation directory to modify the &lsquo;conf/nifi.properties&rsquo;file, set the built-in &lsquo;zookeeper&rsquo; boot and &lsquo;cluster&rsquo; to &lsquo;true&rsquo;, as follows:
nifi.state.management.embedded.zookeeper.start=true nifi.cluster.is.node=true # zookeeper properties, used for cluster management # nifi.zookeeper.connect.string=centos7-master:2181,centos7-cluster01:2181 nifi.zookeeper.connect.timeout nifi.zookeeper.session.timeout nifi.zookeeper.root.node=/nifi Configure Nifi Admin initialization
Update NIFI Deployment Go to the Nifif installation directory to modify the&rsquo;conf/nifi.properties&rsquo;fileand add the &lsquo;kerberos5&rsquo; login fit as follows:
nifi.kerberos.krb5.file=/etc/krb5.conf # kerberos service principal # nifi.kerberos.service.principal=root/admin@CENTOS7-MASTER.COM nifi.kerberos.service.keytab.location=/data/root.keytab Update the user configuration
Go to the conf directory in the Nifif installation directory, add &lsquo;authorizer&rsquo; to &lsquo;authorizers.xml&rsquo;, open the &lsquo;file-provider&rsquo; node comment, and add the following:
&lt;authorizer> &lt;identifier>file-provider&lt;/identifier> &lt;class>org.apache.nifi.authorization.FileAuthorizer&lt;/class> &lt;property name="Authorizations File">./conf/authorizations.xml&lt;/property> &lt;property name="Users File">./conf/users.xml&lt;/property> &lt;property name="Initial Admin Identity">root/admin@CENTOS7-MASTER.COM&lt;/property> &lt;property name="Legacy Authorized Users File">&lt;/property> &lt;property name="Node Identity 1">CN=centos7-master, OU=NIFI&lt;/property> &lt;property name="Node Identity 2">CN=centos7-cluster01, OU=NIFI&lt;/property> &lt;/authorizer> Update the login configuration
Go to the conf directory in the Nifif installation directory, modify the &lsquo;login-identity-providers.xml&rsquo;file, andopen the&rsquo;kerberos-provider&rsquo;node comment:
&lt;provider> &lt;identifier>kerberos-provider&lt;/identifier> &lt;class>org.apache.nifi.kerberos.KerberosProvider&lt;/class> &lt;property name="Default Realm">CENTOS7-MASTER.COM&lt;/property> &lt;property name="Kerberos Config File">/etc/krb5.conf&lt;/property> &lt;property name="Authentication Expiration">12 hours&lt;/property> &lt;/provider> Start the NIFI service
Start the NIFI of the primary node, then start the NIFI from the node, execute the command &lsquo;./bin/nifi.sh start&rsquo;, then open the browser input &lsquo;https://centos7-master:9443/nifi/&rsquo; and you will jump to the login page and enter the user and password created in step 1 to log in successfully. The interface appears as follows:
As shown in the above two figures, in the upper left corner of the interface can clearly see that the current number of nodes is 2, the user is &lsquo;root/admin@CENTOS7-MASTER.COM&rsquo;, where &lsquo;centos7-master&rsquo; is the coordinator, &lsquo;centos 7-cluster01&rsquo; is the main node, the main menu also added &lsquo;Cluster&rsquo;, &lsquo;User&rsquo; and &lsquo;Policies&rsquo; options.
At this point, &lsquo;Apache NIFI&rsquo; cluster services and user authentication are complete, and then we can take the next step.
The pits encountered: The user name or password is not valid when you log on for the first time, and the user&rsquo;s password can be updated via kadmin
After successful login, the user is prompted that there is no corresponding policy, just restart the NIFI service Citation reference
nifi-security-user-authentication-with-kerberos.html -Nifi- Build) (http://blog.csdn.net/sinat_34233802/article/details/68942176?locationNum=1&amp;fps=1). -The principle of kerberos certification &mdash; is very detailed and easy to understand. Kerberos Basic Installation and Configuration Bounty received Thank you for your generosity!
The serial number Nickname Source The amount (yuan) Leave a message 1 Lin is a common WeChat 2 Thanks to bloggers, thanks for</content></entry><entry><title>An issue with the service in Ambari that is functioning properly but the service stops</title><url>https://lisenhui.cn/en/2017/10/18/ambari-monitor-status-issues.html</url><categories><category>Big data</category></categories><tags><tag>Ambari</tag><tag>HDP</tag><tag>Big data</tag></tags><content type="html"> Many times the maintenance of the environment is indeed a headache event, which does not originally show the monitoring of normal services on Ambari&rsquo;s Dashboard page, there is a strange phenomenon: in the machine query service running process is normal, but partial Ambari&rsquo;s UI interface is displayed as a stop, but the port check shows normal. Here&rsquo;s the picture:
Ben can also let go (anyway, the service is running normally), but helpless obsessive compulsive disorder &ldquo;fault&rdquo; and committed, had to eliminate it out of the heart is comfortable. After several unsuccessful attempts, it turned out that it was as if a colleague had manually started some components, which is why. The processusers who used &lsquo;ps&rsquo; to examine these components found that this was the case, forced the killing of these components, and then restarted them using the Ambari UI, but the end result remained the same.
Really very depressed, at this time also had to use google, and then found a similar problem of the article, which mentioned the runtime&rsquo;xx.pid&rsquo;permission problem, really a bit ofwake-up call dreamers, hasten to look at these components of the pid file permissions, indeed, because the previous start-up is with super-tube users, and in fact these components have corresponding user maintenance. Delete these&rsquo;xxx.pid&rsquo;files, re-open these services on theAmbari UI, everything is back to normal, and the beautiful green interface is back.
Reference quote:
service-is-running-but-ambari-shows-serice-is-stop</content></entry><entry><title>Problems with HiveServer2 due to the JDBC version</title><url>https://lisenhui.cn/en/2017/10/17/hive2-jdbc-connector-issues.html</url><categories><category>Big data</category></categories><tags><tag>Big data</tag><tag>Hive</tag><tag>Ambari</tag></tags><content type="html"> Previously, &lsquo;HDP&rsquo; was used to build and manage the &lsquo;Hadoop&rsquo; environment, and there were no difficult problems when the installation was debugged, but this time it was a strange problem when it was deployed on the &lsquo;Centos6x&rsquo; system:
On the face of it, the Hive service is functioning normally, the process is running normally, the page UI is normal, and the logs are not output incorrectly. Simple table-building statements can be executed, which can be caused by throwing exceptions when importing local/HDFSdata. The wrong stack information is as follows:
com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'OPTION SQL_SELECT_LIMIT=DEFAULT' at line 1 Another problem is that the HDFS data import prompt file does not exist using the&rsquo;HiveView&rsquo;UI provided by Ambari,with the error message as follows:
org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: SemanticException Line 1:17 Invalid path ''/tmp/xxx/xxxxx.csv'': No files matching path hdfs:/... Briefly describe the environment used:
Hive - 1.2.1000
MySQL - 5.6.17
MySQL JDBC - 5.1.17
Question one
The information from the error can obviously be known to be a syntax error problem, but the trouble is that it did not print out the problematic SQL statement, through google found the same problem article, which pointed out that this is mySQL JDBC driver 5.1.17 version below the BUG, only need to update the JDBC driver version. Then it seems that the problem becomes simple, find the new JDBC driver file, and do the following:
Copy-driven files Copy to the resource directory of Amabri Server mv mysql-connector-java-5.1.44.jar /var/lib/ambari-server/resources/mysql-connector-java-5.1.44.jar ln -s -f /var/lib/ambari-server/resources/mysql-connector-java-5.1.44.jar /var/lib/ambari-server/resources/mysql-connector-java.jar Copy to the share directory mv mysql-connector-java-5.1.44.jar /usr/share/java/mysql-connector-java-5.1.44.jar ln -s -f /usr/share/java/mysql-connector-java-5.1.44.jar /usr/share/java/mysql-connector-java.jar Reset the Ambari driver reference ambari-server setup --jdbc-db=mysql --jdbc-driver=/usr/share/java/mysql-connector-java.jar Remove the old driver above the Ambari Agent and restart it Note the path to backup and deletion rm -rf /var/lib/ambari-agent/tmp/mysql-* Restart the service ambari-agent restart Restart the Hive component service on the Ambari UI Theoretically there are these operations can solve the problem, but after running the data import is still the same problem, indicating that the above file update operation did not succeed, switch to the Hive Master machine to find the driver file in the lib directory, decompression found that the version is really not changed, then can only be manually forced to replace, the Hive Master, Slave machine drive all replaced with the latest version, and then restart the Hive component service, and then a new problem.
Question two
It is not possible to determine the problem itself from the log above alone, because it is possible to determine with certainty that the file exists on top of HDFS. So switch to the Hive service log and find the following log:
ERROR [HiveServer2-Background-Pool: Thread-4456]: hdfs. KeyProviderCache (KeyProviderCache.java:createKeyProviderURI(87)) - Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !! ERROR [HiveServer2-Background-Pool: Thread-4456]: metadata. Hive (Hive.java:copyFiles(2853)) - Failed to move: org.apache.hadoop.security.AccessControlException: Permission denied. user=admin is not the owner of inode=xxxxx.csv at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkOwner(FSPermissionChecker.java:250) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:227) It is clear from this log that there is a problem with user rights, but there is a bit of a puzzle here as to why Ambari Hive View does not operate directly with super users and can now only be the source of forced changes to the file, as follows:
hdfs dfs -chown hdfs:hadoop /tmp/XXX/XXX.CSV At this point, all the problems are fixed, the import operation is re-executed, everything is working properly, and the data is successfully imported.
Reference quote:
hive-metastore-not-working-syntax-error-option-sql</content></entry><entry><title>The job types and examples supported by Azkaban</title><url>https://lisenhui.cn/en/2017/09/09/azkaban-execute-jobs.html</url><categories><category>Big data</category></categories><tags><tag>Scheduled scheduling</tag><tag>Big data</tag><tag>Azkaban</tag></tags><content type="html"> In the introduction to the official documentation, it is understood that &lsquo;Azkaban&rsquo; supportsa wide range of types of work, such as: &lsquo;Command&rsquo;,&lsquo;HadoopShell&rsquo;,&lsquo;Python&rsquo;, &lsquo;Java&rsquo;, &lsquo;Hive&rsquo;, &lsquo;Pig&rsquo; and so on. However, here we mainly only to explain the &lsquo;Python&rsquo; and &lsquo;Java&rsquo;job type tasks, other types of work, such as&rsquo;Commnad&rsquo;,&lsquo;Hive&rsquo;,&lsquo;HadoopShell&rsquo;relatively simple without explanation, if necessary, you can practice ityourself.
Regardless of which task is submitted, &lsquo;Azkaban&rsquo; is managed by uploading a compressed package by default, so it is recommended that you get into the habit ofnot packing the executed files (code) into the &lsquo;Azkaban&rsquo; engineering package. The benefits are obvious, such as:
The project is created so fast that you don&rsquo;t need to pass on some files to execute
Avoid modifying the &lsquo;I&rsquo;m max_allow_packet&rsquo;parameterin &lsquo;MySQL&rsquo;to resolve the issue of failed transmissions on engineering files
In a distributed environment, when performing Task eliminates the hassle of copying engineering packages in different nodes
Java work tasks
Since most of the code in the work business scenario is written in &lsquo;Java&rsquo;, this is why &lsquo;Azkaban&rsquo; was chosen. It&rsquo;s not much different from the usual &lsquo;Java&rsquo; program, the only difference is that the method of program entry is different. You need to add a &lsquo;run&rsquo; method to the class at the portal, which is the key to starting the overall Task. The sample code is as follows:
package io.github.elkan1788.azkabantasks; import azkaban.utils. Props; import org.slf4j.Logger; import org.slf4j.LoggerFactory; /** * Azkaban java job example * @author elkan1788@gmail.com */ public class JobMain { private static final Logger logger = LoggerFactory.getLogger(JobMain.class); private int fileRows; private int fileLine; /** * Dynamic parameters set */ public JobMain(String name, Props props) { this.fileRows = props.getInt("file.rows"); this.fileLine = props.getInt("file.line"); } public void run() throws Exception { logger.info(" ### this is JavaMain method ###"); logger.info("fileRows value is ==> " + fileRows); logger.info("fileLine value is ==> " + fileLine); } } In the example code above, dynamic parameter settings are added, and you don&rsquo;tneed to specify where&rsquo;MainClass&rsquo;is when you package it, as long as all the relevant code and dependencies in theproject are packaged into a separate file.
We also need to build an &lsquo;Azkaban&rsquo; engineering script that tells it how to perform our tasks, as shown in the following script:
type=java job.class=io.github.elkan1788.azkabantasks.JobMain #jar lib path classpath=/home/azkaban/javademo/* #paramter1 file.rows=10 #paramter2 file.line=50 In order not to make the task too monotonous, by the way, a simple command output forms an FLOW output, the reference script is as follows:
type=command dependencies=java command=whoami The effect is as follows:
Pthon work tasks
Python&rsquo;s task is relatively simple, but no method of dynamic entry has been found. There are two ways to perform Python work tasks, and the reference code is as follows:
&lsquo;command&rsquo; type type=command command=python -u /home/azkaban/pythondemo/helloworld.py &lsquo;python&rsquo; type type=python python=python script=/home/azkaban/pythondemo/helloworld.py The effect is as follows:
A summary
Overall,the scripting of &lsquo;Azkaban&rsquo; tasks is simple and flexible, but there are also pit people. For example, the previous &lsquo;Java&rsquo; work task, in the actual operation process is the need to add&rsquo;hadoop&rsquo;dependency package and related configuration, can lookthrough the official documentation to know because the &lsquo;Java&rsquo;task type is derived from&rsquo;Hadoop Java&rsquo;,so itis no wonder. Fortunately, this is only the configuration of the environment when the problem, the subsequent application development is quite convenient.</content></entry><entry><title>Timed scheduling tasker Azkaban installation</title><url>https://lisenhui.cn/en/2017/09/08/azkaban-install-use-share.html</url><categories><category>Big data</category></categories><tags><tag>Scheduled scheduling</tag><tag>Big data</tag><tag>Azkaban</tag></tags><content type="html"> Background and introduction
In the big data complex ETL or other data processing processes, some tasks need to be performed on a timed basis, although Linux brings itsown&rsquo;cron' commandfunction, but still can not meet the biggest point is that it does notprovide centralized management and visual editing. In fact, in the big data ecology has been integrated with a timed scheduling framework &lsquo;Oozie&rsquo;, but the practice found that its learning costs are not low, the process of distribution is more complex. After trying other distribution worker scheduling frameworks, such as Ali&rsquo;s Zeus &lsquo;Zeus&rsquo;, or &lsquo;Azkaban&rsquo;, which is used by more people in the community.
&lsquo;Azkaban3&rsquo; is still relatively large compared to the changes made in the previous version, and interested parties can be found on its official website, Azkaban. Next, mainly to share the installation of &lsquo;Azkaban 3&rsquo;, the following is &lsquo;Azkaban 3&rsquo; system architecture design:
The three components in the figure are an important part of &lsquo;Azkaban3&rsquo;:
MySQL relationship data storage data Web Server GUI management service provider Executor Server Distributed Node Service The database is initialized
It is recommended to start with a database called&rsquo;azkaban' using &lsquo;MySQL5.6&rsquo;and above:
mysql> CREATE DATABASE azkaban DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci; Specify a database user to give it&rsquo;SELECT&rsquo;, &lsquo;INSERT&rsquo;, &lsquo;UPDATE&rsquo;, &lsquo;DELETE&rsquo; operating rights to the &lsquo;azkaban&rsquo;database:
mysql> GRANT SELECT,INSERT,UPDATE,DELETE ON azkaban.* to 'azkaban-dba'@'%' WITH GRANT OPTION; Finally, the import of the &lsquo;SQL&rsquo; statement that created the table, the official table statement is more scattered, for this purpose specially organized a complete table-building statement (Azkaban Create Tables password: 8ne8) (https://pan.baidu.com/s/1Dvcky7UJT02O9det4UG0cQ):
mysql> source /opt/download/azkaban-create-tables.sql Note: Since the project release of &lsquo;Azkaban3&rsquo; is achieved by uploading files, it is necessary to adjust the energy size of the allowed upload package in &lsquo;MySQL&rsquo;, and this parameter islocated under &lsquo;mysqld&rsquo;:max_allowed_packet s64M, depending on the actual situation to modify the appropriatesize.
In fact, there is a way not to modify this parameter, that is, when packaging the &lsquo;Azkaban&rsquo; project, try not to pack dependent files into it, through the reference of the relative path can be. **
Web Server
There are seven folders in the &lsquo;Web Server&rsquo; directory, described as follows:
The folder Describes bin Script-related files, such as startup, stop Jetty service conf The accessories file for the Solo lib Web Server must rely on the package library extlib Third-party extensions rely on package plugins The Azkaban plug-in is installed web Web Server files such as css,js,html, etc. The &lsquo;Web Server&rsquo; will need to be modified in at least 4 places, as follows:
&lsquo;azkaban.properties&rsquo;configuration
The parameter settings for the multi-executor are missing from the official default configuration file, as follows:
# Azkaban Personalization Settings The site name, which Chinese available for UTF-8 encoding, is not supported by mail notifications azkaban.name=Azkaban azkaban.label=Production Environment azkaban.version=3.25 azkaban.color=#FF3601 azkaban.default.servlet.path=/index web.resource.dir=web/ default.timezone.id=Asia/Shanghai # Azkaban UserManager class user.manager.class =azka ban.user.XmlUserManager user.manager.xml.file=conf/azkaban-users.xml # Loader for projects executor.global.properties=conf/global.properties azkaban.project.dir=projects # DB settings database.type=mysql mysql.port=3306 mysql.host=127.0.0.1 mysql.database=azkaban mysql.user=azkaban-dba mysql.password=12345678 mysql.numconnections=100 # Velocity dev mode velocity.dev.mode=false # Azkaban Jetty server properties. jetty.hostname=localhost jetty.use.ssl=false jetty.maxThreads=25 jetty.port=8081 # Azkaban Executor settings executor.port=12321 # Notification Email Settings mail.sender=elkan1788@gmail.com mail.host=stmp.gmail.com mail.user=elkan1788@gmail.com mail.password=12345678 lockdown.create.projects=false cache.directory=cache # JMX stats jetty.connector.stats=true executor.connector.stats=true # Azkaban plugin settings azkaban.jobtype.plugin.dir=plugins/jobtypes # Multiple executors settings azkaban.use.multiple.executors=true azkaban.executorselector.filters=StaticRemainingFlowSize,CpuStatus azkaban.executorselector.comparator.NumberOfAssignedFlowComparator=1 azkaban.executorselector.comparator.Memory=1 azkaban.executorselector.comparator.LastDispatched=1 azkaban.executorselector.comparator.CpuUsage=1 &lsquo;azkaban-users.xml&rsquo; configuration
&lsquo;Azkaban&rsquo; uses an account information configuration similar to&rsquo;Spring Securities&rsquo; and refers to the following:
&lt;azkaban-users> &lt;!-- UserAccount Info --> &lt;user groups="azkaban" password="azkaban" roles="admin" username="azkaban"/> &lt;user password="metrics" roles="metrics" username="metrics"/> &lt;!-- Role Info --> &lt;role name="admin" permissions="ADMIN"/> &lt;role name="metrics" permissions="METRICS"/> &lt;/azkaban-users> &lsquo;log4j.properties&rsquo; configuration
Only terminal output is configured in the official default configuration file, and log output is redirected through the &lsquo;Shell&rsquo; script, while &lsquo;log4j&rsquo; itself is supported in incremental management by date, as follows:
log4j.rootLogger=INFO,C,file log4j.appender.C=org.apache.log4j.ConsoleAppender log4j.appender.C.Target=System.err log4j.appender.C.layout=org.apache.log4j.PatternLayout log4j.appender.C.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n log4j.appender.file=org.apache.log4j.DailyRollingFileAppender log4j.appender.file.File=/opt/azkaban3/web-server/logs/web-server.log log4j.appender.file.DatePattern='.' yyyy-MM-dd log4j.appender.file.layout=org.apache.log4j.PatternLayout log4j.appender.file.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss} %5p [%t](%F:%L) - %m%n Start and stop the script
The official default start-stop script is not very friendly, with a few modifications, as follows:
Start script (bin/start-web.sh) !/bin/bash Official default # bin/azkaban-web-start.sh "$@" >> logs/webServerLog_`date +%Y%m%d`.out 2>&amp;1 &amp; bin/azkaban-web-start.sh > /dev/null 2>&amp;1 &amp; Stop Script (bin/shutdown-web.sh) !/bin/bash Official default # bin/azkaban-web-shutdown.sh "$@" >> logs/webServerLog_`date +%Y%m%d`.out 2>&amp;1 &amp; bin/azkaban-web-shutdown.sh > /dev/null 2>&amp;1 &amp; Other configurations
Some common configurations can be unified into the &lsquo;conf/global.properties&rsquo;configuration, such as successful project execution, failednotified email, default execution type, and so on.
Executor Server
&lsquo;Executor Server&rsquo; is just one less directory than &lsquo;Web Server&rsquo;, as described below:
The folder Describes bin Script-related files, such as startup, stop Jetty service conf The accessories file for the Solo lib Executor Server must rely on the package library extlib Third-party extensions rely on package plugins The Azkaban plug-in is installed In principle, it is recommended that only one &lsquo;Executor Server&rsquo; node be assigned to a machine, as it is automatically added to the database &lsquo;executors&rsquo; list when the node starts, and of course the local pseudo-distribution pattern can be configured by port intervention.
The &lsquo;Executor Server&rsquo; is relatively simple to place, with only three updates, as follows:
&lsquo;azkaban.properties&rsquo;configuration
Simply modify the value of the official default profile, as follows:
# Azkaban Personalization Settings default.timezone.id=Asia/Shanghai # Loader for projects executor.global.properties=conf/global.properties azkaban.project.dir=projects # DB settings database.type=mysql mysql.port=3306 mysql.host=127.0.0.1 mysql.database=azkaban mysql.user=azkaban-dba mysql.password=12345678 mysql.numconnections=100 # Velocity dev mode velocity.dev.mode=false # Azkaban Executor settings executor.host=127.0.0.1 executor.port=12321 executor.maxThreads=50 executor.flow.threads=30 # JMX stats jetty.connector.stats=true jetty.host=127.0.0.1 executor.connector.stats=true # Azkaban plugin settings azkaban.jobtype.plugin.dir=plugins/jobtypes &lsquo;log4j.properties&rsquo; configuration
Refer to the configuration in &lsquo;Web Server&rsquo; above and be careful to modify the output path of the log file.
Start and stop the script
The same changes have been made to the start-stop script to facilitate playback of the post-log, as follows:
Start script (bin/start-exec.sh) #!/bin/bash export HADOOP_HOME=/opt/hdp/2.3.2.0-2950/hadoop # pass along command line arguments to azkaban-executor-start.sh script # bin/azkaban-executor-start.sh "$@" >> logs/executorServerLog_`date +%Y%m%d`.out 2>&amp;1 &amp; bin/azkaban-executor-start.sh > /dev/null 2>&amp;1 &amp; Stop script (bin/shutdown-exec.sh) #!/bin/bash export HADOOP_HOME=/opt/hdp/2.3.2.0-2950/hadoop # pass along command line arguments to azkaban-executor-start.sh script # bin/azkaban-executor-shutdown.sh "$@" >> logs/executorServerLog_`date +%Y%m%d`.out 2>&amp;1 &amp; bin/azkaban-executor-shutdown.sh > /dev/null 2>&amp;1 &amp; Start the preview
By this point, the key configuration required for the &lsquo;Azkaban3&rsquo; operation has been configured, and the next step is to start the corresponding service preview of the labor results.
Start- and start-ups
The startup order is as follows:
Start the &lsquo;Executor Server&rsquo; service:&lsquo;sh bin/start-exec.sh&rsquo;
Update the data sheet: Find thecorresponding record for &lsquo;executor server hostname&rsquo; in the&rsquo;excecutors&rsquo;table and update the value of the last column &lsquo;active&rsquo; to
Launch the &lsquo;Web Server&rsquo; service:&lsquo;sh bin/start-.sh&rsquo;
Note: Make sure that the port is not properly occupied before starting, and keep an eye out for memory usage.
After successful launch, enter &lsquo;http://localhost:8081&rsquo; in your browser to see the &lsquo;Web Server&rsquo; interface, as shown in the following image:
Demo Create a new project in the &lsquo;Web Server&rsquo; that startedsuccessfully above, named:ShellJob-Demo,and then download and upload the following example, Job, to the project you just created,note that you don&rsquo;t need to unziw it. Then find the&rsquo;Exectue Flow&rsquo; button in your project, and you&rsquo;ll continue to take the next step, as shown in the following group:
Base Flow Demo Password: 4f4f (https://pan.baidu.com/s/19iEqfyShVTnVrfzfdST__A)
&lsquo;Azkaban3&rsquo; is performed by submitting a task (Job) to &lsquo;Executor Server&rsquo; through &lsquo;Web Server&rsquo;, so it is not intuitive to see the program execution process visually on the interface, but you can understand the running process by finding the running task in the execution list and viewing its logs, as shown in the following group:
Well, isn&rsquo;t it relatively simple to complete the &lsquo;Azkaban 3&rsquo; service and simple examples. At the initial stage, the use of &lsquo;Azkaban 3&rsquo; can still be used in a business-appropriate scenario, but the later promotion process finds that it is not perfect, such as manually updating the database during the above startup process to activate &lsquo;Executor Server&rsquo; (only the first time it starts), and the official does not provide the management of the &lsquo;Executor Server&rsquo; run, the distributed runtime needs to manually specify &lsquo;Executor Server&rsquo;s ID&rsquo; and so on. But it&rsquo;s basically enough for everyday use, especially its &lsquo;Job Flow&rsquo; design. Finally, if you have questions about &lsquo;Azkaban3&rsquo; that can be discussed together in the comments, the relevant usage tutorials will be updated later, so stay tuned.</content></entry><entry><title>Use Github, Travis CI to automatically deploy HexoCoding to OSChina Server</title><url>https://lisenhui.cn/en/2017/08/19/use-travis-ci-push-hexo-blog.html</url><categories><category>Hexo</category></categories><tags><tag>Hexo</tag><tag>Github</tag><tag>Travis</tag><tag>Ci</tag><tag>Coding</tag><tag>OSChina</tag></tags><content type="html"> Usually we use &lsquo;hexo deploy&rsquo; locally to post blog posts to remote Pages servers, but don&rsquo;t forget that we still need to submit code, so don&rsquo;t feel a bit of trouble and score two steps to do it. Then it occurred to me whether the &lsquo;Travis CI&rsquo; tool could be used to complete the deployment? The answer is yes, and the overall process is roughly as follows:
Edit articles locally (or on the Github website). Submit an article to the Github server Travis CI receives notifications to synchronize the latest Github code and execute user-customized &lsquo;Travis&rsquo; scripts to generate static blogs Finally push the generated blog to the specified Pages server It&rsquo;s just that one of the more troubling questions is how to protect our private keys, and if &lsquo;Travis CI&rsquo; is ready for us, start our journey.
Prepare the Travis Client tool
Get ready for ruby&rsquo;s environment
Ruby&rsquo;s installation please move the search engine, which is only recommended for versions above 2.0 at this prompt, and also pay attention to updating the mirror address of the &lsquo;gem&rsquo;: ruby China( https://gems.ruby-china.org).
Travis CI account
If you need to register your account separately, it is recommended to log in directly using &lsquo;Github Token&rsquo;. The next step is to generate a &lsquo;Github Token&rsquo;, find it in the &lsquo;Github&rsquo; settings panel, or create it directly by clicking on the &ldquo;Github Tokens&rdquo; (https://github.com/settings/tokens, as shown in the following image:
Save the &lsquo;Token&rsquo; you just created, then sign in to &lsquo;Travis CI&rsquo; with the &lsquo;Github&rsquo; authorization and jump to the control panel (https://travis-ci.org/profile/) to select the item you want to create (that is, your blog project) as shown in the following image
Travis Client installation
The &lsquo;Travis Client&rsquo; installation is very simple and the commands are as follows:
sudo gem install travis -v 1.8.8 --no-rdoc --no-ri After the installation is successful, check with the following commands that the successful installation will have the output of the version number.
travis version Use the following command to verify the &lsquo;Github Token&rsquo; generated in the next step and log in to &lsquo;Travis CI&rsquo; and return a welcome message.
travis login -g fb25xxxxxxxxxxx Successfully logged in as xxxx! SSH private key encryption
Switch to the blog&rsquo;s directory, create a directory called &lsquo;.travis&rsquo; and copy the private key for Coding and OSChina to this point, using the following commands to generate an encrypted file that Travis can recognize:
travis encrypt-file id_rsa Detected repository as elkan1788/my-hexo-blog, is this correct? |yes| yes encrypting id_rsa for elkan1788/my-hexo-blog storing result as id_rsa.enc storing secure env variables for decryption Please add the following to your build script (before_install stage in your .travis.yml, for instance): openssl aes-256-cbc -K $encrypted_c9744fe6174f_key -iv $encrypted_c9744fe6174f_iv -in id_rsa.enc -out id_rsa -d Pro Tip: You can add it automatically by running with --add. Make sure to add id_rsa.enc to the git repository. Make sure not to add id_rsa to the git repository. Commit all changes to your .travis.yml. After the encryption is successful, remember to delete the &lsquo;id_rsa&rsquo; file and save the following statements, which will be used later in the script:
openssl aes-256-cbc -K $encrypted_c9744fe6174f_key -iv $encrypted_c9744fe6174f_iv -in id_rsa.enc -out id_rsa -d Prepare the Travis script
Write a Travis script
The &lsquo;Travis&rsquo; script uses the &lsquo;yml&rsquo; syntax, which is not difficult to write, just pay attention to the indentation of spaces. Create a file called &lsquo;.travis.yml&rsquo; at the blog root, as follows:
# Define language environment language: node_js # use root accout or not sudo: false # node js version node_js: stable # setting timezone before_install: - export TZ='Asia/Shanghai' # cache installed modules cache: apt: true directories: - node_modules # add pages server domain Addons: ssh_known_hosts: - git.coding.net - git.oschina.net # auto deploy blog to pages server deploy: provider: script script: sh .travis/deploy.sh skip_cleanup: true on: branch: master # offical request dist: precise # which branch trigger branches: only: - master If you&rsquo;re not sure if the script you&rsquo;re writing is correct, check it with &lsquo;Travis CI&rsquo; and the command is as follows:
travis lint .travis.yml Hooray, .travis.yml looks valid :) Edit the &lsquo;.sh&rsquo; script
Next, write a script to post a blog post to the Pages server, and the main process is as follows:
Decrypt the &lsquo;SSH&rsquo; private key and output it to the specified directory Modify the file permissions of the private key, start &lsquo;SSH Agent&rsquo;, and add the private key Set up the &lsquo;Git&rsquo; configuration, mainly the user name, email address Use the &lsquo;Hexo&rsquo; command - clean up, build, publish The script is referenced as follows:
#!/bin/bash # Decrypt the private SSH key openssl aes-256-cbc -K $encrypted_c9744fe6174f_key -iv $encrypted_c9744fe6174f_iv -in .travis/id_rsa.enc -out ~/.ssh/id_rsa -d # Set the permission of the private SSH key chmod 600 ~/.ssh/id_rsa # Start SSH agent eval $(ssh-agent) # Add the SSH private key to the system ssh-add ~/.ssh/id_rsa # Set Git config git config --global user.name "Dream Stardust" git config --global user.email elkan1788@gmail.com # Clean, generate and deploy to Pages server hexo clean &amp;&amp; hexo g &amp;&amp; hexo deploy Publish an article
Create an article using the &lsquo;hexo new &lsquo;article tittle&rsquo;command&rsquo; command, then add the idea you want to spit, save the content, and so on, and then push the code to the &lsquo;Github&rsquo; server with the &lsquo;git push&rsquo; command, where you can log in to &lsquo;Travis CI&rsquo; and you&rsquo;ll see the &lsquo;gorgeous&rsquo; log output in the corresponding project as shown in the following image:
If the final result is green, then congratulations, your blog has been successfully put in place, hurry to refresh the page.
At the end of all configuration, how, feel is not very dazzling, as long as a simple &lsquo;git push&rsquo; command to save the code and do the blog site allocation, if there is a problem welcome spit slot.
Reference:
Using Github, Travis-CI, and Coding.net Auto-Deploy Blogs [Using Github, Travis-CI, and Coding.net Auto-Deploy Blogs( https://huangyijie.com/2016/10/05/blog-with-github-travis-ci-and-coding-net-2/) [Using Github, Travis-CI, and Coding.net Auto-Deploy Blogs(https://huangyijie.com/2017/06/22/blog-with-github-travis-ci-and-coding-net-3/)</content></entry><entry><title>pymsql connects to the MSSQL database of the azure cloud</title><url>https://lisenhui.cn/en/2017/08/17/pymssql-azure-mssql-datasource-connect.html</url><categories><category>Azure</category></categories><tags><tag>Python</tag></tags><content type="html"> Code good code in the test environment to do a good job of testing, full of confidence to the operation of the online to the production environment, the result is a bunch of exceptions, specifically looked at the problem found to be connected to the database, abnormal information is as follows:
(40532, 'Cannot open server "1433D" requested by the login. The login failed. DB-Lib error message 20018, severity 20:\n General SQL Server error: Check messages from the SQL Server\n DB-Lib error message 20002, severity 9:\nAdaptive Server connection failed\n') Is there a problem with the environment installation, switch under the test environment and no problem, well, had to turn to &lsquo;Google&rsquo; again, finally found the reason, should be Microsoft Cloud&rsquo;s own rules, in the user name to add the host name is good, reference as follows:
import pymssql conn = pymssql.connect(server='yourserver.database.chinacloudapi.cn', user='yourusername@yourserver', password='yourpassword', database='AdventureWorks') That&rsquo;s @yourserver keyword
Reference:
Using Python to query Azure SQL Database
Cannot open server &ldquo;1433D&rdquo; requested by the login</content></entry><entry><title>Install the pymsql module under Mac/Linux</title><url>https://lisenhui.cn/en/2017/08/16/mac-install-pymssql-module.html</url><categories><category>Python</category></categories><tags><tag>Mac</tag><tag>Linux</tag><tag>Python</tag></tags><content type="html"> Accessing and connecting MSSQL data in a non-&lsquo;Windows&rsquo; environment is a hassle in itself. Since writing the &lsquo;Python&rsquo;program in the &lsquo;ORM&rsquo; aspect are the use of the&rsquo;pyxxx&rsquo;module, it is not true that the connectionMSSQLalso has a module called&rsquo;pymsql&rsquo;,but the actual use is notparticularly smooth. If the author is in the environment is so, the development environment is &lsquo;OSX 10.11&rsquo;, the release environment is &lsquo;CentOS 6.4&rsquo;, according to the official installation steps, the Linux environment is &lsquo;OK&rsquo;, but the Mac environment installation failure, the wrong stack information is as follows:
Running setup.py install for pymssql ... error Complete output from command /Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python -u -c "import setuptools, tokenize;__file__='/private/tmp/pip-build-KA5ksi/pymssql/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))" install --record /tmp/pip-A3wRBy-record/install-record.txt --single-version-externally-managed --compile: setup.py: platform.system() => 'Darwin' setup.py: platform.architecture() => ('64bit', '') setup.py: platform.libc_ver() => ('', '') setup.py: Detected Darwin/Mac OS X. You can install FreeTDS with Homebrew or MacPorts, or by downloading and compiling it yourself. Homebrew (http://brew.sh/) -------------------------- brew install freetds MacPorts (http://www.macports.org/) ----------------------------------- sudo port install freetds ...... /usr/bin/clang -fno-strict-aliasing -fno-common -dynamic -arch i386 -arch x86_64 -g -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/usr/local/include -I/opt/local/include -I/opt/local/include/freetds -I/Library/Frameworks/Python.framework/Versions/2.7/include/python2.7 -c _mssql.c -o build/temp.macosx-10.6-intel-2.7/_mssql.o -DMSDBLIB _mssql.c:18924:15: error: use of undeclared identifier 'DBVERSION_80' __pyx_r = DBVERSION_80; Before installing&rsquo;pymssql',there was a problem with the component that appeared on the component,&lsquo;FreeTDS&rsquo;. The installation commands in the &lsquo;Linux&rsquo; and &lsquo;OSX&rsquo; environments are as follows:
# Linux yum install freetds-devel.x86_64 # Mac brew install freetds In a &lsquo;Mac&rsquo; environment, you need to be awareof the issues caused by the version of&rsquo;freetds',which can be used normally for'0.91', and the corrected installation commands are as follows:
brew uninstall --force freetds brew install freetds@0.91 brew link --force freetds@0.91 You will also need to install a Python module with the following installation commands:
pip install cython Once the above environment is ready, you can successfully install the &lsquo;pymsql&rsquo;module and perform the following installation commands:
pip install pymssql Write a simple test code like this:
#!/usr/bin/env python # -*- coding: utf_8 -*- # coding=utf8 import pymssql server = "192.168.1.2" user = "in" password = "123456" conn = pymssql.connect(server, user, password, database="platform") cursor = conn.cursor() cursor.execute("SELECT * FROM Table") row = cursor.fetchone() while row: row = cursor.fetchone() print row conn.close() OK, it&rsquo;s all done, go ahead with the code code.
The reference is as follows:
pymssql-isseues432
mac-pip-install-pymssql-error</content></entry><entry><title>The integrated MySQL data in Hue shows garbled code</title><url>https://lisenhui.cn/en/2017/08/15/hue-rdbms-mysql-chinese.html</url><categories><category>Big data</category></categories><tags><tag>hue</tag><tag>Big data</tag></tags><content type="html"> Hue is a Web applications that enables you to easily interact with an Hadoop cluster. Hue applications let you browse HDFS, Jobs, run Hive, Pig and Cloudera Impala queries, manage the Hive Metastore, HBase, Sqoop, ZooKeeper, MapReduce jobs, and create and schedule worklows with Oozie.
For more information and presentations on HUE,visit its officialwebsite: http://gethue.com.
In this main solution is the integration of MYSQL management in the HUE process, encountered a common problem in Chinese and garbled code. Let&rsquo;s take a look at the configuration description for integrated MySQL:
########################################################################### # Settings for the RDBMS application ########################################################################### [librdbms] # The RDBMS app can have any number of databases configured in the databases # section. A database is known by its section name # (IE sqlite, mysql, psql, and oracle in the list below). [[databases]] # sqlite configuration. ## [[[sqlite]]] # Name to show in the UI. ## nice_name=SQLite # For SQLite, name defines the path to the database. ## name=/tmp/sqlite.db # Database backend to use. ## engine=sqlite # Database options to send to the server when connecting. # https://docs.djangoproject.com/en/1.4/ref/databases/ ## options={} # mysql, oracle, or postgresql configuration. [[[mysql]]] # Name to show in the UI. nice_name="MY SQL DB" # For MySQL and PostgreSQL, name is the name of the database. # For Oracle, Name is instance of the Oracle server. For express edition # this is 'xe' by default. name=mysql # Database backend to use. This can be: # 1. mysql # 2. postgresql # 3. oracle engine=mysql # IP or hostname of the database to connect to. host=localhost # Port the database server is listening to. Defaults are: # 1. MySQL: 3306 # 2. PostgreSQL: 5432 # 3. Oracle Express Edition: 1521 port=3306 # Username to authenticate with when connecting to the database. user=USER # Password matching the username to authenticate with when # connecting to the database. password=PASSWORD # Database options to send to the server when connecting. # https://docs.djangoproject.com/en/1.4/ref/databases/ # options={} This configuration is very simple to understand is also difficult, but in the actual operation process encountered two difficulties, first show the problem of garbled code, and then the configuration to the document link address is &lsquo;404&rsquo;, it is really embarrassing. Go back and find out about the settings for sql-mode (https://docs.djangoproject.com/en/1.11/ref/databases/#setting-sql-mode), think about the commands that should support MySQL, and then add the following commands in the last paragraph of the configuration:
options={ "init_command":"SET NAMES `utf8`"} Experimented for a little, garbled code problem OK, Chinese display normal.
In fact, if you do not have the production environment, you do not have to toss, the simplest way is to update the &lsquo;my.ini&rsquo; configuration, you know.</content></entry><entry><title>Python outputs instantly on the command line</title><url>https://lisenhui.cn/en/2017/08/13/python-output-conosle-intime.html</url><categories><category>Python</category></categories><tags><tag>Python</tag></tags><content type="html"> When a program encounters problems that require DEBUG, output by adding some &lsquo;print&rsquo; statements. So by convention also in the &lsquo;Python&rsquo; code to add print debugging, and then enter &lsquo;python xxxx.py&rsquo;, confidently looking forward to debugging information full screen scrolling, the result is after a long time to show. Why is this so?
According to the netizen suggested to add a &lsquo;-u&rsquo; parameter OK, and later checked the reason: &lsquo;Python&rsquo; in the default situation will first &lsquo;print&rsquo; output to the buffer, until the buffer is full or the program before output. So it&rsquo;s useful to add this parameter when you&rsquo;re running a &lsquo;Python&rsquo; program.
python -u xxxx.py Other parameters are supported, as follows
The &lsquo;-B&rsquo; parameter does not produce a pyc or pyo file at the time of import &lsquo;-c&rsquo; parameter, run the python statement directly -'-i&rsquo;parameter, open a python environment after running thepython script file for easy viewing of the results of the operation &lsquo;-m&rsquo; parameter, the module will be executed according to the script &lsquo;-V&rsquo; parameter, output version of Python &lsquo;-O&rsquo; parameter, resulting in an optimized pyo file (not valid with the -B parameter). The &lsquo;-v&rsquo; parameter outputs each module reference information, including where it was referenced and when it was cleared The &lsquo;-u&rsquo; parameter, which is useful when printing records, forces stdin, stdout, and stderr to become unbuffered and outputs immediately, rather than waiting for the buffer to fill up before printing the data. Reference:
Python Command Line Parameter Learning</content></entry><entry><title>Python pip China Mirror Server Address</title><url>https://lisenhui.cn/en/2017/08/11/python-pip-install-chinese-mirror.html</url><categories><category>Python</category></categories><tags><tag>Mirror</tag><tag>Python</tag></tags><content type="html"> Today in the installation of a &lsquo;Python&rsquo;module &ndash;&lsquo;pymysql&rsquo; resultswaiting time is particularly long, the final timeout failed, at first thought it was a network bandwidth problem, let IT adjustment is still a failure, and then try to find the domestic mirror, and real people haveencountered the same problem. The list of mirrors is as follows:
https://pypi.douban.com/simple/ pods http://mirrors.aliyun.com/pypi/simple/ Ali http://pypi.hustunique.com/simple/ Central China University of Technology http://pypi.sdutlinux.org/simple/ Shandong University of Technology http://pypi.mirrors.ustc.edu.cn/simple/ University of Science and Technology https://pypi.tuna.tsinghua.edu.cn/simple to Tsinghua Then, when installing the module, use the following commands:
pip install xxxx -i https://pypi.douban.com/simple Netizens also introduced the method of writing the mirror address to the profile, but the attempt was unsuccessful, do not understand the reasons for it, to be followed up.
Reference:
(Python pip Domestic Mirroring and Use)</content></entry><entry><title>Rebuild your personal blog site with Hexo</title><url>https://lisenhui.cn/en/2017/08/02/use-hexo-rebuild-blog-site.html</url><categories><category>Hexo</category></categories><tags><tag>Hexo</tag><tag>Study</tag><tag>Blog</tag></tags><content type="html"> In fact,in the &lsquo;Github Page&rsquo; above is also mixed for a long time, although now a variety of blog sites are emerging, but as the IT industry&rsquo;sprogram apes still like to do their own trouble, success is certainly happy failure will also be inappropriate. &lsquo;Github Page&rsquo; was first launched using &lsquo;Jekyll&rsquo;, a simple explanation that is actually a static web site tool, and now there&rsquo;sa tool called&rsquo;Hexo'(&lsquo;Nodejs&rsquo;) implementation.' Both goals are the same, but by contrast, it&rsquo;s reallyeasier to get started with Hexo,plus it&rsquo;s easy to debug locally, so there&rsquo;san idea to flip over again and build a&rsquo;Hexo&rsquo;version of your personal blog.
Introducing another static website tool, &lsquo;Gor&rsquo; (https://github.com/wendal/gor), it&rsquo;s a masterpiece of The GreatErnest (Wendal) (http://wendal.net/) that people familiar with the &lsquo;GO&rsquo; language have to follow.
Do a simple survey of the &lsquo;Pages&rsquo;service before you do it, nothing else, that is, now&rsquo;Github&rsquo;users more and more and servers are abroad,living in the heavens we understand. Surprise found that the current domestic &lsquo;Git&rsquo;service providers have provided &lsquo;Pages&rsquo; implementation, and finally selectedthe &lsquo;Gitee&rsquo; (http://gite.com) and &ldquo;Coding&rdquo; (https://coding.net) as the new blog landing point, where &lsquo;Coding&rsquo;as the first/default service,&lsquo;Gitee&rsquo;as an alternative service, the reason for this choice issimple: &lsquo;Coding&rsquo;not only provides a custom domain name, but also comes witha &lsquo;https&rsquo; free certificate.
For the&rsquo;Hexo&rsquo;environment is not exhausted here, the official documentation gives a detailed description (the operation is also quite simple) please move:https://hexo.io/zh-cn/docs/index.html (https://hexo.io/zh-cn/docs/index.html). When you&rsquo;re done, you can choose your favorite style on the website&rsquo;s &ldquo;Themes&rdquo; (https://hexo.io/themes/) page,and personally choose the more popular &ldquo;NexT&rdquo;(http://theme-next.iissnan.com/) and like its simplicity and lightth.
The&rsquo;NexT&rsquo;configuration is also easy to use, and here&rsquo;s a simple summary of the problems individuals encounter during the build process:
The installation of the plug-in &lsquo;Hexo&rsquo; is fairlyflexible in providing rich plug-in support that can be installed on its own according to individual needs, with the following individual installation records:
The RSS is generated npm install hexo-generator-feed --save The Site map is generated to prepare for the reptile service npm install hexo-generator-sitemap --save npm install hexo-generator-baidu-sitemap --save The site file is compressed npm install hexo-all-minifier --save Publish to the Git server npm install hexo-deployer-git --save Third Party Services Integration As a blog site is certainly an interactive link, the Internet world has long provided this feature, in this mainly used functions are: article reading, article digital statistics, site PV/UV, comment reply. Other functional integration should not be difficult, as long as the corresponding service provider site registration, fill in the corresponding ID, KEY can be. Mainly mentions the function of the following article digital statistics:
Sign in toleanCloud(https://leancloud.cn) to find your app and click on the settings button in the upper right corner, as shown in the following image: Then click on &ldquo;Storage&rdquo; in the menu on the left, then click &ldquo;Create Class&rdquo; in the middle of the list, enter the name click create, as shown in the following image: At this point don&rsquo;t set a secure domain name, directly start the Hexo service locally, keep refreshing the page, you can see you want the results, it&rsquo;s as simple as that.
During the build process, it was found that the&rsquo;jiathis&rsquo;code template of&rsquo;NexT&rsquo;had passed and did not have the concept of&rsquo;uid&rsquo;,and that there was also a problem with where the code was stored, and that the pull request submitted after fork could be followed if you wereinterested: (pull-1796) (https://github.com/iissnan/hexo-theme-next/pull/1796).
Publish to multiple Git servers at the same time Because&rsquo;Gitee&rsquo; and&rsquo;Coding&rsquo; are selected as the running servers for the blog, you need to push at the same time when you publish, as follows:
# Deployment ## Docs: https://hexo.io/docs/deployment.html deploy: type: git repository: coding: git@git.coding.net:lisenhui/lisenhui.git,master oschina: git@git.oschina.net:lisenhui/lisenhui.git,master Custom domain name binding In addition to binding custom domain names to the &lsquo;Git Pages&rsquo; service provider, we also need to add a file called CNME to the site for the domain name you want to point to. It is recommended touse the DNSPod(https://www.dnspod.cn) for domain name resolution, you can fine-tune the source to access different services (is not a bit of a distribution taste), the specific operation can refer to the official / online tutorial.
To this individual&rsquo;s blog site is to build the completion, the effect demonstration such as this site, if it is lazy configuration, directly clone contemptuous blog project can be (remember to change the name ah, ha), as follows:
git clone https://git.oschina.net/lisenhui/my-hexo-blog.git Bricks and flowers are welcomeIn fact, the&rsquo;Hexo' blog needs only a few steps to build:
npm install -g hexo-cli mkdir hexo-blog hexo init hexo-blog cd hexo-blog git clone https://github.com/iissnan/hexo-theme-next themes/next vi _config.yml (change theme: next) hexo g and hexo s (open browser input: http://127.0.0.1:4000). Reference article: 1.Hexo Common Commands. 2.LeanCloud,Implementing Article ReadingStatistics. 3.Thehexo-Next themed blog was submitted to Baidu Google 4. &ldquo;Build a static blog using Hexo and Next&rdquo;.</content></entry><entry><title>Hexo blog Hello World</title><url>https://lisenhui.cn/en/2017/07/31/hexo-hello-world.html</url><categories><category>Hexo</category></categories><tags><tag>test</tag><tag>learn</tag></tags><content type="html"> Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.
Quick Start Create a new post $ hexo new "My New Post" More info: Writing
Run server $ hexo server More info: Server
Generate static files $ hexo generate More info: Generating
Deploy to remote sites $ hexo deploy More info: Deployment</content></entry><entry><title>APIDoc automatically generates interface documentation</title><url>https://lisenhui.cn/en/2017/07/18/nodejs-apidoc-generator.html</url><categories><category>Tools</category></categories><tags><tag>API</tag><tag>Study</tag></tags><content type="html"> For the project development common front-end separation mode, the middle in the back end to complete interface development delivery docking, front-end personnel often suffer from no interface documentation will often &ldquo;run away&rdquo; to harass back-end personnel, it is really painful. If there is a documented description at this time it is much easier, now the back-end popular document generation weapon has &lsquo;Swagger&rsquo;, although it is convenient, but also has the disadvantage of writing in the background code, and start the entire background project to access. Perhaps sometimes really not convenient, in addition to the project at the beginning of the interface to do a planning can not use this method, there is no other way?
Finally, in the vast network or find a good tool -&ldquo;Nodejs APIDoc&rdquo;(http://apidocjs.com/), very powerful, support the current popular development languages such as &lsquo;Java&rsquo;,&lsquo;PHP&rsquo;, &lsquo;JavaScript&rsquo;, &lsquo;Python&rsquo;, &lsquo;Ruby&rsquo;and soon, here&rsquo;s a brief introduction to how to use it.
The module is installed
As already said in the previous introduction, it is based on the &lsquo;NodeJS&rsquo; environment,soyou must first have a &lsquo;NodeJS&rsquo; environment, and then install the&rsquo;APIDoc&rsquo;module, withthe following reference commands:
npm install apidoc -g The project profile
Next, create a project folder and include a project profile, as follows:
{ "name": "XXXX Open Interface Platform", "version": "1.0.1", "Description": "XXXX open interface platform, design all interface services docking with third-party services. Note that all interface data interactions are in JSON format. ", "title": "XXXX Open Interface Platform", "generator": { "name": "XXXX", "time": "2017-07-18 15:46:55", "url": "https://xxxx.com", "version": "1.0.1" } } Interface documentation
Once all the relevant preparations are complete, then we need to write a documentation about the interface description, depending on the development language of your actual project in the future, and it is recommended that you choose the same as possible, and here I will take &lsquo;Java&rsquo; as an example, do not need specific code, just populate the contents of the code comment section, refer to the following:
hello-api.java
/** * @apiDefine xxxxx * :: XXX current interface document name (generally butt customer name) */ /** * @apiDefine Err400 * :: @apiError date the transaction took place on the date of the transaction at the time of the transaction :: @apiError the time of the transaction at the time of the transaction :: @apiError the serviceId agency code (provided by Dong Wu) at the time :: @apiError number of resultCode 1 for success and 0 for failure :: @apiError the reason for the failure of the .String?resultComment * * @apiErrorExample Error400-Response: * HTTP/1.1 400 * { * "tranDate": "20170718", * "tranTime": "131223", * "serviceId": "xxxx", * "resultCode": 0, ::"resultComment":"Request data syntax format is incorrect." * } */ /** * @apiDefine Suc200 * :: @apiSuccess date the transaction took place at the date of the transaction at the time of the transaction at the time of the transaction :: @apiSuccess the time of the transaction at the time of the transaction :: @apiSuccess the serviceId agency code (provided by Dong Wu) at the time of the issue :: @apiSuccess number of resultCode 1, which indicates success, and 0 indicates failure :: @apiSuccess description of the reason for the failure of the .String?resultComment * * @apiSuccessExample Success-Response: * HTTP/1.1 200 * { * "tranDate": "20170718", * "tranTime": "131223", * "serviceId": "xxxx", * "resultCode": 1, * "resultComment": "Success" * } */ /** * @apiDefine AccessKey * :: @apiHeader the current date and specifies the string's 32-bit MD5 encryption string. * * @apiHeaderExample {json} Header-Example: * { * "access-key": "cfa1fd55a89f45c9800120d6cbff0b33" * } */ /** :: @api the customer base information at the website :: @apiDescription customer's basic information in bulk, it is recommended that each batch not be larger than 1000 records. * * @apiVersion 1.0.1 * @apiName customer * @apiGroup dwHealth * * @apiUse AccessKey * :: @apiParam the name of the .String?cusName :: @apiParam the sex of the .String?cusSex :: @apiParam the birthday of the .String, cusBirthday :: @apiParam the type of document you're using for :: @apiParam the number of your .String's cusIdNo ID number :: @apiParam the work unit of the cusCompanyId :: @apiParam the program number of the "String" cusServItemNo * * @apiParamExample {json} Request-Example: * { * "serviceId": "xxxx", * "data": [ * { ::"CusName":"Zhang San", ::"cusSex":"male", * "cusBirthday": "2017-07-18", ::"cusIdType":"ID card", * "cusIdNo": "4419381788902217652", * "cusCompanyId": "1024", * "cusServItemNo": "201707181313132", * } * ...... * ] * } * * @apiUse Suc200 * * @apiUse Err400 * */ The interface document is generated
Finally, we need only a simple command to generate the interface document, as follows:
apidoc -and apidoc/-o apidoc/ The folder where the i project is located o Interface document output folder The document works as follows:
A frequently asked question
提示 error: Can not read: package.json, please check the format (e.g. missing comma). Solution: Save the file in UTF-8 format, or check for other formatting issues</content></entry><entry><title>Kylin integrates Zeppelin presentation data</title><url>https://lisenhui.cn/en/2017/06/02/kylin-integrate-with-zeppelin.html</url><categories><category>Kylin</category></categories><tags><tag>Big data</tag><tag>Kylin</tag><tag>Zeppelin</tag></tags><content type="html"> In fact, kylin&rsquo;s own WEB UI has integrated the recommended graphical reports, with common line, column, and pie charts, which are perfectly sufficient for the initial presentation of the data. If you want a richer presentation, consider using other tools, and try the officially recommended Apache Zeppelin now.
Open the official website of Apache Zeppelin and choose to download the version of &lsquo;zeppelin-0.7.1-bin-netinst.tgz&rsquo;, and other plug-ins can be installed later. Download and unzip to the directory you want to run, and then copy &lsquo;conf/zeppelin-site.xml.template&rsquo;to modify the binding address and business sloganfor &lsquo;conf/zeppelin-site.xml&rsquo; . Then install the kylin plug-in, and the command is as follows:
bin/install-interpreter.sh --name kylin --artifact org.apache.zeppelin:zeppelin-kylin:0.7.1 When the installation is complete, start zeppelin with the following command:
bin/zeppelin-daemon.sh start Stop stop You can now open the browser and access Zeppelin&rsquo;s WEB UI, as shown in the following image:
OK, the next step is to create a connection to Kylin, called &lsquo;Interpreter&rsquo; in Zeppelin, click &lsquo;anonymous&rsquo; in the upper right corner of the page to select it as follows:
Also click on the &lsquo;Create&rsquo; button in the upper right corner and fill in your real data with the data filled in in the figure below:
Once saved, click on the &lsquo;Notebook&rsquo; in the upper left corner &ndash; as shown in the figure below:
Write the following SQL statement to notebook:
select fact.part_dt, lookup.categ_lvl2_name, count(distinct seller_id) as sellers from kylin_sales fact inner join kylin_category_groupings lookup on fact.leaf_categ_id = lookup.leaf_categ_id and fact.lstg_site_id = lookup.site_id group by fact.part_dt, lookup.categ_lvl2_name order by fact.part_dt desc Click the start button on the right to complete the query, come out a table data, and then choose the graphical report form you need, the data will automatically render, click &lsquo;sets&rsquo; can have more adjustments.
Other applications about &lsquo;Zeppelin&rsquo; need to be understood slowly and followed up.
Reference:
interpreter-installation kylin</content></entry><entry><title>The Sqoop tool imports data into the Hive note</title><url>https://lisenhui.cn/en/2017/05/24/sqoop-import-data-to-hive.html</url><categories><category>Big data</category></categories><tags><tag>Sqoop</tag><tag>Hive</tag><tag>Big data</tag></tags><content type="html"> Recently, the construction of a data warehouse is being messed up, just some of the dimension table data needs to come from RDBMS data, in the HADOOP environment is the most popular than Apache&rsquo;s Sqoop tool, according to the official documentation operation is also smooth, but when to apply to the business scenario when the problem arises.
Create a Dimension table on Hiveand store it in ORC format (reference toHive: ORC File Format Storage Format) and throw the following exception when performing the Sqoop import:
FAILED: SemanticException Unable to load data to destination table. Error: The file that you are trying to load does not match the file format of the destination table. After several tests, it was found that Sqoop&rsquo;s default imported data format isTXTFILE, so when the table is built, using the TXTFILE storage format can import data normally, but this is not what we want, and then looked at the documentation, found that it provides an hcatalog command after version 1.4.5 can support &lsquo;ORC Format File&rsquo;, the reference command is as follows:
sqoop import --connect jdbc:mysql://master01:3306/data_pipeline --username dw --password-file hdfs:///user/hdfs/dw.txt --table dim_calendar --split-by ek_cal_id --compress --fields-terminated-by "," --lines-terminated-by "\n" --hcatalog-database default --hcatalog-table dim_calendar --map-column-hive cal_date=DATE,ts=TIMESTAMP --hcatalog-storage-stanza 'stored as orc tblproperties ("orc.compress"="SNAPPY")' From the above commands you can see that you can freely define the storage and compression formats later, but there is a problem here that will be a warning, as follows:
WARN hcat. SqoopHCatUtilities: Column cal_date had to be cast to a less precise type DATE in hcatalog WARN hcat. SqoopHCatUtilities: Column ts had to be cast to a less precise type TIMESTAMP in hcatalog There is no way to solve this problem forthe time being, HIVE also seems to support both types of data formats, follow up later.
When executing the Sqoop command, remember to switch to a cluster machine with both Sqoop Client and Hive Client installed, or you will experience a data import failure.
Reference:
Sqoop Manual (https://www.cnblogs.com/xiaodf/p/6030102.html) -Hive:ORC File Format Storage Format Details( https://www.iteblog.com/archives/1014.html). (Hive adds Chinese after comment when creating table) (https://www.58jb.com/html/103.html) SQOOP Import to Snappy ORC qoop Hive table import, Table dataType doesn&rsquo;t match with database</content></entry><entry><title>Supervisor introduction and use</title><url>https://lisenhui.cn/en/2017/05/18/linux-daemon-supervisor.html</url><categories><category>Supervisor</category></categories><tags><tag>Supervisor</tag><tag>Tools</tag><tag>Linux</tag></tags><content type="html"> Many times our own development or other services do not have a background daemon, then the process can easily be accidentally killed, at this time need to have a program to monitor and maintain these program services. An online search revealed that the &lsquo;Supervisor&rsquo; component was exactly what we wanted, while also supporting the unified management of these programs, Nice!
Supervisor is a client/server system that allows its users to monitor and control a number of processes on UNIX-like operating systems. After reading the description of Supervisor&rsquo;s definition on the official website, I immediately felt like I had to experiment. Fortunately, Python is inherently supported in Linux systems, so just install PIP to get everything you want.
Install pip: easy_install pip Install Supervisor: pip install supervisor Profile echo_supervisord_conf>/etc/supervisord.conf 3.1 Profile details
[unix_http_server] file=/tmp/supervisor.sock ; UNIX socket file,supervisorctl will use ; chmod=0700 ; The mode of the socket file, which defaults to 0700 ; chown=nobody:nogroup ; Owner of socket file, format: uid:gid ; [inet_http_server] ; HTTP server, providing a web management interface ; port=0.0.0.0:9001 ; The web manages IP and ports running in the background, and if it is open to the public network, you need to be aware of security ; username=user ; Sign in to manage the user name in the background ; password=123 ; Sign in to manage passwords in the background [supervisord] logfile=/tmp/supervisord.log ; Log file, which defaults to $CWD/supervisord .log logfile_maxbytes=50MB ; Log file size, beyond will rotate, default 50MB logfile_backups=10 ; The log file retains the default number of backups by 10 loglevel=info ; Log level, default info, others: debug, warn,trace pidfile=/tmp/supervisord.pid ; Pid file nodaemon=false ; Whether to start in the fore desk, the default is false, which is started as daemon minfds=1024 ; The minimum value of the file descriptor that can be opened, default 1024 minprocs=200 ; The minimum number of processes that can be opened, which is 200 by default ; the below section must remain in the config file for RPC ; (supervisorctl/web interface) to work, additional interfaces may be ; added by defining them in separate rpcinterface: sections [rpcinterface:supervisor] supervisor.rpcinterface_factory = supervisor.rpcinterface:make_main_rpcinterface [supervisorctl] serverurl=unix:///tmp/supervisor.sock ; By connecting supervisord through UNIX socket,the path unix_http_server file in the other sections ; serverurl=http://127.0.0.1:9001 ; Connect supervisord over HTTP ; Contains additional profiles [include] files = /etc/supervisor/*.conf ; It can be .conf or .. . ini Daemon configuration instructions [program:kafka] directory = /root/kafka_2.10-0.10.1.1/bin/ ; The startup directory of the program command = kafka-server-start.sh /root/kafka_2.10-0.10.1.1/config/server.properties ; Start commands, and you can see that the commands that are manually started on the command line are the same autostart = true ; It also starts automatically when supervisord starts startsecs = 5 ; If there is no abnormal exit after 5 seconds of startup, it is considered to have started normally autorestart = true ; The program restarts automatically after it exits abnormally startretries = 3 ; The number of automatic retrys for startup failures, the default is 3 user = root ; Which user to start with redirect_stderr = true ; Redirect stderr to stdout,default false stdout_logfile_maxbytes = 20MB ; Stdout log file size, default 50MB stdout_logfile_backups = 20 ; The number of stdout log file backups ; Stdout log files, you need to be aware that the specified directory doesnot start properly when it does not exist, so you need tomanually create the directory (supervisord automatically creates the log file). stdout_logfile = /var/log/kafka-server.log ; Environment can be used to add the required environment variables, a common use of which is to modify PYTHONPATH ; environment=PYTHONPATH=$PYTHONPATH:/path/to/somewhere Start the service supervisord -c /etc/supervisord.conf Manage theguardian process with &lsquo;supervisorctl&rsquo; All can be changed to a specific process name supervisorctl status all supervisorctl start all Reload the configuration supervisorctl reload Enable the WEB interface ; [inet_http_server] ; HTTP server, providing a web management interface ; port=127.0.0.1:9001 ; The web manages IP and ports running in the background, and if it is open to the public network, you need to be aware of security ; username=user ; Sign in to manage the user name in the background ; password=123 ; Sign in to manage passwords in the background Then enter in the browser: http://127.0.0.1:9001, enter login information, the interface shows as follows:
Now you can try killing the daemon and see if it restarts automatically again, but if the supervisord service is killed then it won&rsquo;t work.</content></entry><entry><title>The gpg signature failed when publishing jar to Maven</title><url>https://lisenhui.cn/en/2017/05/17/maven-deploy-center-sign-failed.html</url><categories><category>Maven</category></categories><tags><tag>Maven</tag><tag>Tools</tag><tag>Mac</tag></tags><content type="html"> For a long time did not maintain their own open source projects, this time in the repair bug release encountered failure, inspection found that the reason is because of gpg signature failure, no way to change mac computer some operations are not familiar with is a bit depressed.
On how to share their JAR to Maven central warehouse, there are a lot of resources on the Internet, you can try it yourself, in fact, it is not difficult, there is no need to worry about English.
Share a GitBookorganized by someone else: &ldquo;Published to CentralWarehouse&rdquo;.
[INFO] --- maven-gpg-plugin:1. 6:sign (sign-artifacts) @ mpsdk4j --- gpg: Failed to sign: Inged ioctl for device gpg: signing failed: Inappropriate ioctl for device Above is the GPG in the signature of the problem, simply literally, there is an inappropriate ioctl for this device,do not understand what is. Finally, a step-by-step exploration found that because the servers that manage GPG are not available, a new server was found on the Internet to re-upload as follows:
gpg --keyserver hkp://pgp.mit.edu --send-keys DAB131AA5564DCF176 #如果不放心的话, you can check it out using the command below gpg --keyserver hkp://pgp.mit.edu --recv-keys DAB131AA5564DCF176 Well, repack the release jar package, very happy to see the results of SUCCESS, closed.</content></entry><entry><title>Github push fail：Could not resolve hostname</title><url>https://lisenhui.cn/en/2017/05/16/github-push-failed.html</url><categories><category>Git</category></categories><tags><tag>Git</tag><tag>GitHub</tag><tag>Mac</tag></tags><content type="html"> Usually the most commonly used git push command suddenly can not be used (error log as follows), the brain first jumped out of the idea is: Is Github again by the wall! This has happened before and requires accelerated access by specifying hosts.
Error log returned after git push execution:
ssh: Could not resolve hostname github.com:elkan1788: nodename nor servname provided, or not known fatal: Could not read from remote repository. Please make sure you have the correct access rights Start with the simplest SSH command, and the results are as follows:
ssh -T git@github.com Hi elkan1788! You've successfully authenticated, but GitHub does not provide shell access. ssh -T git@git.oschina.net Welcome to Git@OSC,where dream stardust! That means git sever is normal, so why does push fail? Netizen methods have been tried, such as specifying hosts, updating ssh key, adding DNS: 8.8.8.8 and so on. Finally, there is no way to temporarily replace ssh with https mode, the execution of git push input user and password submission success. But the fundamental problem was not solved, and finally wanted not to try again clone project, so to re-create the directory, clone project modification file submission, the result was successful.
At this time can only be said to be too strange, carefully recall whether the configuration has been changed? But sure not, but think of the last time the source code was installed to update the software, is this the problem, the output git version is as follows:
git --version git version 2.11.0 (Apple Git-81) Sure git is updated, but at present no exact root cause of the problem has been found, the main solution is to reclone project, the problem solves itself, follow up with updates.</content></entry><entry><title>Zookeyer could not load transaction logs after crash</title><url>https://lisenhui.cn/en/2017/05/15/zookeeper-unload-data-exception.html</url><categories><category>Zookeeper</category></categories><tags><tag>Zookeeper</tag><tag>Big data</tag></tags><content type="html"> Today in a production HDP environment, encountered a very strange thing. Mingming set up 2 zookeyer cluster, but it is inexplicable missing, and HDP services are not reported wrong, carefully check the environment or did not find abnormal information, really do not understand.
Let&rsquo;s put it this way: The production environment zookeyer crashes and the log finds that disk space is full. At first thought it was a very simple operation, delete the useless log files to free up disk space (this is having to spit under the SLOT HDP log files are super, helpless production environment and dare not set aside a longer time), and then restart zookeyer happy waiting for the service to return to normal. This time, however, there was no hint of success, and the unusually constant service connections to zookeyer failed. At this time is really depressed, space is clearly already sufficient. The exception information is as follows:
2017-05-15 11:02:24,421 - INFO [main:FileSnap@83] - Reading snapshot /hadoop/zookeeper/version-2/snapshot.5ff3bc 2017-05-15 11:02:26,492 - ERROR [main:Util@239] - Last transaction was partial. 2017-05-15 11:02:26,494 - ERROR [main:QuorumPeer@530] - Unable to load database on disk Online a search, looking forward to finding relevant cases to share, the case was found however, those just encountered problems have not been completely resolved, the case is as follows:
ZOOKEEPER-1621 &ldquo;Data file read exception&rdquo; (http://futeng.iteye.com/blog/2120173) At this time is really a little speechless, in the beginning to view zookeyer&rsquo;s source code, while switching to Baidu search engine to find the case (we all prefer to use Google, you know), did not expect to really find a solution, netizens share the case:
-ZooKeeper Starts Error List transaction was partial. Solution ( http://blog.sina.com.cn/s/blog_3fe961ae0102xmiv.html).
The original text is as follows: ZooKeeper can't start again after the hard drive is full and throws Last transaction was partial. . Bugs see: https://issues.apache.org/jira/browse/ZOOKEYER-1621 First of all, my environment issingle-node, zooKeeper version is 3.4.8. Because it is a single node,ZooKeeper can not start the impact is very large, multi-node may also appear at the same time the hard disk is full, if the problem occurs online, the consequences can not be imagined. Toss a bit, found that the ZooKeeper installation directory under the data/log/version-2, size 0 (abnormal) log, deleted, and then restart, the problem solved! A check of the corresponding directory really found a size 0 log file, delete and then start zookeyer, OK output log normal, through the zookeyer client connection to view the data back to normal. Finally hanging heart can be put down, but before that zookeeper inexhned disappearance problem still did not find the reason. The lesson of this time is that in the future similar to these important directories must do hot backup, in the big data environment zookeeper&rsquo;s raw nature can be imagined, but also this time there is no risk.</content></entry><entry><title>Install HDP2.6 (1)-Ambari Server offline</title><url>https://lisenhui.cn/en/2017/04/17/offline-install-hdp-ambari-notes.html</url><categories><category>Big data</category></categories><tags><tag>Big data</tag><tag>HDP</tag></tags><content type="html"> Reference documentation FYI: HDP Install Documents
HDP Install Manual
2. The hardware environment The first is to prepare 3 machines, install the latest CentOS 7.2, the configuration reference requirements of the machine are as follows:
CPU Memory Disk Remark 4 nuclear 26G 200G Primary node/1 4 nuclear 16G 200G From node/2 3. HDP installation files Download files installed offline:
File Name Download Link ambari-2.5.0.3 http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.5.0.3/ambari-2.5.0.3-centos7.tar.gz HDP-2.6.0.3 http://public-repo-1.hortonworks.com/HDP/centos7/2.x/updates/2.6.0.3/HDP-2.6.0.3-centos7-rpm.tar.gz HDP-UTILS-1.1.0.21 http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/centos7/HDP-UTILS-1.1.0.21-centos7.tar.gz 4. SSH is secret-free Configure password-free login, note that this is mainly the master machine login to other cluster machines. So it&rsquo;s a good idea to give the machine a specific hostname logo to separate first, as follows:
IP Host Name 192.168.1.1 test-hdp-master01 192.168.1.2 test-hdp-cluster01 192.168.1.3 test-hdp-cluster02 It is important to note that the modification of the &lsquo;'/etc/hosts&rsquo; file in CentOS7 is no longer possible to modify the machine name and requires the use of a new command: &lsquo;hostnamectl set-hostname test-hdp-master01&rsquo;
Then use the &lsquo;ssh-keygen -t RSA&rsquo; secret order on the master machine to generate the SSH key, and then use the command ssh-copy-id -i-.ssh/id_rsa.pub root@test the h-hdp-cluster01', copied to the other two cluster machines, and finally used the SSH login command to check if the installation was successful, while writing the hostname, IP address to each machine&rsquo;s &lsquo;/etc/hosts&rsquo; file.
5. Time synchronization Install the NTP service (http://blog.csdn.net/andy_yf/article/details/8027635).
Install and start the ntpd service on the Master machine with the following commands:
yum install ntp -y systemctl start ntpd Modify the profile to allow synchronization time with the machine below the segment.
vi /etc/ntp.conf # Hosts on local network are less restricted. #restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap Find this configuration and rewrite it to the following configuration restrict 192.168.51.0 mask 255.255.255.0 nomodify Install the NTP client on the other two Cluster machines and execute the following commands:
yum install ntpdate -y crontab -e Sync every minute */1 * * * * ntpdate -u 192.168.51.21 &amp;&amp; hwclock -w systemctl start crond.service 6. Configure the YUM image Unzim the file
Unzim the three files downloaded in Step 2 Note that &lsquo;HDP-UTIL&rsquo; has no root, so it&rsquo;s a good idea to create a directory with a decompressed directory structure as follows:
[root@test-hdp-master01 hdp-download]# ll total 8676352 drwxr-xr-x 3 root root 20 Apr 25 16:03 ambari -rw-r--r-- 1 root root 1657013486 Apr 6 11:14 ambari-2.5.0.3-centos7.tar.gz drwxr-xr-x 3 1001 users 20 Apr 3 08:58 HDP -rw-r--r-- 1 root root 6356134913 Apr 3 09:25 HDP-2.6.0.3-centos7-rpm.tar.gz drwxr-xr-x 21 root root 4096 Apr 25 16:16 HDP-UTILS -rw-r--r-- 1 root root 871424874 Mar 31 03:11 HDP-UTILS-1.1.0.21-centos7.tar.gz Start the HTTP service
Start the HTTPServer service without installing Apache to start directly with the following Python command:
python -m SimpleHTTPServer 88 Modify the Repo file
Find the &lsquo;ambari.repo&rsquo; and &lsquo;hdp.repo&rsquo; files under the 6.1 decompression directory and update the inside of the &lsquo;baseurl&rsquo; and &lsquo;gpgkey&rsquo; to the local HTTP service address, as follows:
ambari.repo
#VERSION_NUMBER=2.5.0.3-7 [ambari-2.5.0.3] name=ambari Version - ambari-2.5.0.3 # baseurl=http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.5.0.3 baseurl=http://192.168.51.21:88/ambari/centos7 gpgcheck=1 # gpgkey=http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.5.0.3/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins gpgkey=http://192.168.51.21:88/ambari/centos7/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins enabled=1 priority=1 hdp.repo property
#VERSION_NUMBER=2.6.0.3-8 [HDP-2.6.0.3] name=HDP Version - HDP-2.6.0.3 # baseurl=http://public-repo-1.hortonworks.com/HDP/centos7/2.x/updates/2.6.0.3 baseurl=http://192.168.1.1:88/HDP/centos7 gpgcheck=1 # gpgkey=http://public-repo-1.hortonworks.com/HDP/centos7/2.x/updates/2.6.0.3/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins gpgkey=http://192.168.1.1:88/HDP/centos7/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins enabled=1 priority=1 [HDP-UTILS-1.1.0.21] name=HDP-UTILS Version - HDP-UTILS-1.1.0.21 # baseurl=http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/rest/centos7 baseurl=http://192.168.1.1:88/HDP-UTILS gpgcheck=1 # gpgkey=http://public-repo-1.hortonworks.com/HDP/centos7/2.x/updates/2.6.0.3/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins gpgkey=http://192.168.1.1:88/HDP-UTILS/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins enabled=1 priority=1 Install ambari-server
Install the ambari-server service on the Master machine
yum install ambari-server 7. Configure the JDK environment Unzip the downloaded JDK package into the &lsquo;/user/share/jdk&rsquo; directory, and then edit the &lsquo;/etc/profile&rsquo; file to end with the following code:
export JAVA_HOME=/usr/share/jdk/jdk1.8.0_131 export CLASSPATH=.:$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar export PATH=$JAVA_HOME/bin:$PATH Finally command &lsquo;source /etc/profile&rsquo; to edit the file, repeat this on the other two Clubs, and remember to verify that the installation was successful with &lsquo;java-version&rsquo;.
8. Install the MySQL database Reference
Download the mysql source installation package and install
wget http://dev.mysql.com/get/mysql57-community-release-el7-8.noarch.rpm yum localinstall mysql57-community-release-el7-8.noarch.rpm #检查mysql源是否安装成功 yum repolist enabled | grep "mysql.*-community.*" mysql-connectors-community/x86_64 MySQL Connectors Community 33 mysql-tools-community/x86_64 MySQL Tools Community 47 mysql57-community/x86_64 MySQL 5.7 Community Server 187 yum install mysql-community-server -y Find the default password in the installation log and modify it
grep 'temporary password' /var/log/mysqld.log 2017-04-25T23:51:03.380340Z 1 [Note] A temporary password is generated for root@localhost: dCAdHOM+H4z% ALTER USER 'root'@'localhost' IDENTIFIED BY 'dCAdHOM+H4zz%'; UPDATE user SET host='%' WHERE user='root'; CREATE USER 'ambari'@'192.168.51.%' IDENTIFIED BY '1wVhZ7nd@T'; GRANT ALL PRIVILEGES ON hive.* TO 'hive'@'192.168.51.%' IDENTIFIED BY '1wVhZ7nd@T' FLUSH PRIVILEGES; Modify the default character set
vi /etc/my.cnf Add under the mysqld option character_set_server=utf8 init_connect='SET NAMES utf8' Set the boot to start
systemctl enable mysqld systemctl daemon-reload #启动mysql systemctl start mysqld 9. Install the mysql connector jar file yum install yum-utils yumdownloader mysql-connector-java rpm -ivh mysql-connector-java-5.1.25-3.el7.noarch.rpm --force --nodeps ambari-server setup --jdbc-db=mysql --jdbc-driver=/usr/share/java/mysql-connector-java.jar 10. Set up Ambari Server Setup [root@test-hdp-master01 hdp-download]# ambari-server setup -j /usr/share/jdk/jdk1.8.0_131 Using python /usr/bin/python Setup ambari-server Checking SELinux... SELinux status is 'disabled' Customize user account for ambari-server daemon [y/n](n)? y Enter user account for ambari-server daemon (root): Adjusting ambari-server permissions and ownership... Checking firewall status... Checking JDK... WARNING: JAVA_HOME /usr/share/jdk/jdk1.8.0_131 must be valid on ALL hosts WARNING: JCE Policy files are required for configuring Kerberos security. If you plan to use Kerberos,please make sure JCE Unlimited Strength Jurisdiction Policy Files are valid on all hosts. Completing setup... Configuring database... Enter advanced database configuration [y/n](n)? y Configuring database... ============================================================================== Choose one of the following options: [1] - PostgreSQL (Embedded) [2] - Oracle [3] - MySQL / MariaDB [4] - PostgreSQL [5] - Microsoft SQL Server (Tech Preview) [6] - SQL Anywhere [7] - BDB ============================================================================== Enter choice (3): 3 Hostname (localhost): test-hdp-master01 Port (3306): Database name (ambari): Username (ambari): Enter Database Password (sDgu-5H1sW): Configuring ambari database... Copying JDBC drivers to server resources... Configuring remote database connection properties... WARNING: Before starting Ambari Server, you must run the following DDL against the database to create the schema: /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql Proceed with configuring remote database connection properties [y/n](y)? y Extracting system views... ............ Adjusting ambari-server permissions and ownership... Ambari Server 'setup' completed successfully. Note that in selecting a database you will enter the database name, user name, password, etc
11. Create a metadata database CREATE DATABASE `ambari` CHARACTER SET utf8 COLLATE utf8_general_ci; CREATE USER 'ambari'@'localhost' IDENTIFIED BY 'sDgu-5H1sW' GRANT USAGE ON `ambari`.* TO 'ambari'@'localhost' IDENTIFIED BY 'sDgu-5H1sW' FLUSH PRIVILEGES; USE 'ambari' GO; source /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql 12. Start Ambari Server Start with the command &lsquo;ambari-server start&rsquo; and open the browser input &lsquo;http://192.168.1.1:8080/&rsquo;, you can see Ambari&rsquo;s login interface, enter the default user password to log in, and then install the Hadoop component service.
13. A record of the problem during installation HostName specifies a problem
In this process, if the hostname of ambari-server appears that cannot be specified, it is currently implemented by rewriting the code directly.
vi /usr/lib/python2.6/site-packages/ambari_server/setupAgent.py Update the 315 lines of code as follows # hostname = args[2] hostname = "test-hdp-master01" The MySQL connection failed
Testing the MySQL connection at installation failed, similar to the problem on the last face, and can only be modified under the code:
Note that here is the machine where you install the Hive, oozie service vi /var/lib/ambari-agent/cache/custom_actions/scripts/check_host.py Update the 279 lines of code as follows # jdk_location = config['commandParams']['jdk_location'] jdk_location = 'http://' + ambari_server_hostname + ':8080/resources/' If you are prompted not to download files when you install each service, you should also modify the code, mainly to find that the installation of Hive will appear:
Note that here is the machine where you install the Hive service vi /usr/lib/python2.6/site-packages/resource_management/core/source.py Update the 169 lines of code as follows: # self.url = self.name self.url = self.name.replace('localhost','test-hdp-master01') 14. Uninstall the HDP service Refer to the official documentation: Uninstall</content></entry><entry><title>Linux uses SSH password-free login</title><url>https://lisenhui.cn/en/2016/05/29/ssh-login-without-password.html</url><categories><category>Linux</category></categories><tags><tag>Linux</tag><tag>SSH</tag></tags><content type="html"> Now distributed clusters are very popular, often switch back and forth on different machines that is common. If each switch needs to enter a username and password, that is to crash the rhythm ah. Fortunately, &lsquo;SSH-KEY&rsquo; gives us convenience, as long as the &lsquo;master&rsquo; generates a &lsquo;PUB_KEY&rsquo;, and then copied to &lsquo;clusters&rsquo;, you can use&rsquo;ssh hostname' directly later to quickly and easily switch to the machine that needs to operate
Let&rsquo;s start with the machine&rsquo;s environment:
Both servers are Centos 6.7 x86_64 systems
Main node &lsquo;master&rsquo;, IP address: 192.168.8.200
From node &lsquo;cluster01&rsquo;, IP address: 192.168.8.201
The following first generates a &lsquo;SSH-KEY&rsquo;on the main node, enters&rsquo;ssh-keygen -t rsa&rsquo;,here using the default stored directory, no password, press the enter keytwice in a row, as shown in the following image:
The resulting &lsquo;PUB_KEY&rsquo;file is then used to output a file with thename &lsquo;authorized_keys&rsquo;using the &lsquo;cat&rsquo; pipeline command, and thencopied to thenode server with the&rsquo;scp&rsquo;command (where the passwordis to be entered), as shown in the following image:
If the &lsquo;scp&rsquo; commandcannot be executed, execute the installationcommand: &lsquo;yum install -y openssh-clients&rsquo;
Log on to the node server and execute the following command at the user&rsquo;s root:
chmod 700 . ssh/ chmod 600 . ssh/authorized_keys Then we can achieve the &lsquo;SSH&rsquo; password-free login function. Backto the primary node server,with &lsquo;ssh hostname&rsquo; you can switch to the node machine that you want to operate on, Good Luck
Note:
The&rsquo;authorized_keys&rsquo;file must be generated on the primary node server, otherwise it isinvalid, i.e. copying the &lsquo;PUB_KEY&rsquo; file to the node server is still required for password login
[If it is arelatively new &lsquo;sshd&rsquo;,youcanquickly implement the above steps using the&rsquo;ssh-copy-id hostname&rsquo; command,but remember toinstall &lsquo;openssh-clients&rsquo; first
Reference:
SSH password-free login under Linux &ldquo;Linux tutorial: SSH password-free sign-in method&rdquo; ::The password is still required for password-free login (http://segmentfault.com/q/1010000002903000). SSH password-free login details
Principles:
In order to better understand the principle of SSH password-free login, let&rsquo;s start with SSH&rsquo;s security verification, which uses an &ldquo;unsyscionable key system&rdquo;, a familiar public key private key encryption system, which is divided into two levels of security authentication.
Security verification based on password
This way we use a username password to log in online, which is generally the way we use it. The whole process is roughly asfollows: (1) The client initiates a connection request.
(2) The remote host receives the user&rsquo;s login request and sends its public key to the client.
(3)The client receives the public key of the remote host, and then encrypts the login password using the public key of the remote host, and then sends the encrypted login password together with its own public key to the remote host.
(4) The remote host receives the client&rsquo;s public key and the encrypted login password, decrypts the received login password with its own private key, and allows the login if the password is correct, so far the two parties have each other&rsquo;s public key and begin to decrypt the encryption in both directions.
PS: When another fake server in the network impersonates a remote host, the client&rsquo;s connection request is intercepted by server B, server B sends its own public key to the client, the client encrypts the password and sends it to the counterfeit server, the counterfeit server can take its own private key to get the password, and then do whatever it wants. Therefore, when the remote host is first linked, in step (3) of the above steps, you will be prompted to the current remote host&rsquo;s &ldquo;public key fingerprint&rdquo; to confirm whetherthe remote host is a genuine remote host, if you choose to continue, you can enter a password to log on, when the remote host accepts, the server&rsquo;s public key will be saved to ssh/known_hosts file.
Security verification based on keys
This way you need to create a pair of keys for yourself in the current user&rsquo;s home directory and place the keys on the server you need to log on to. When you want to connect to the server, the client requests security verification using a key from the server. After the server receives the request, it looks for your key in the home directory of the user you requested to log on to on that server and compares it to the one you sent. If the two keys are the same, the server encrypts the &ldquo;challenge&rdquo; with the key and sends it to the client. The client receives the Challenge and decrypts it with its own private key before sending it to the server. Compared to the first level, the second level does not require the password to be transmitted over the network.
PS: Simply put the client&rsquo;s public key on the server, then the client can log on to the server without a password, then the client&rsquo;s public key should be placed on the server where? The default is the . under the home directory of the user you want to sign in to The file in the ssh directory authorized_keys (i.e., : . . . . . ssh/authorized_keys）。</content></entry><entry><title>Git action command collection</title><url>https://lisenhui.cn/en/2016/01/29/git-commands-collect.html</url><categories><category>Git</category></categories><tags><tag>Linux</tag><tag>Git</tag></tags><content type="html"> All say good sex is not as bad as bad writing, there is nothing wrong with it. Although learning Git has been for more than a year, but sometimes the less used commands are always a moment to remember. So decided to write it into the blog, not only to share the experience, but also easy to find their own, this blog post will continue to add up. .
:: Git command alias (very practical)
git config --global alias.co checkout Interpretation : Replace &lsquo;checkout&rsquo; with &lsquo;co&rsquo;, in addition to setting some combined commands with aliases, such as :
Alias Name Description co checkout ci commit br broke l log &ndash;oneline :: Fallback to the first commit (estimated to be encountered by very few people).
git update-ref -d HEAD :: Tag operation
:: View the label
git tag -l :: Create a label
git tag -a 1.0.1-Release -m "Release 1.0.1 version" :: Remove the label
git tag -d 1.0.1-Release :: Remote push
git push --tag :: Remote deletion
git push origin :refs/tags/1.0.1-Release Git Learning Recommendations:
(http://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000)</content></entry><entry><title>mpsdk4j's bit-by-bit record- MPAccount</title><url>https://lisenhui.cn/en/2016/01/23/mpsdk4j-intro-mapaccount.html</url><categories><category>MPSDK4J</category></categories><tags><tag>MPSDK4J</tag><tag>WeChat</tag></tags><content type="html"> &lsquo;mpsdk4j&rsquo; is an open source sharing project that has been pulled out of the actual production project, and its growth has been a lot of experience so far, recently has been busy with work and life things neglected to care about it. Since last year&rsquo;s determination to refactor it and establish a QQ communication group (486192816), Gradually there are not industry friends come to pay attention to, in this thank you very much for their support. All said that the use of the party know its good, but the actual situation is not so optimistic ah, in the process of everyone&rsquo;s use found that &lsquo;mpsdk4j&rsquo; there are many deficiencies and deficiencies. Before the New Year&rsquo;s Day release of the 2.b.1 version has not yet been delayed, in this to say sorry, will be congratulated. Below is also advanced into this theme - first understanding of &lsquo;mpsdk4j'&lsquo;MPAccount&rsquo;. (Note: For 1 WeChat development Basics and project experience can be skimed).
The goal of &lsquo;mpsdk4j&rsquo; since its release is to be original and easy to use. However, in the actual exchange process found a common phenomenon, that is, for the first contact with WeChat development friends, the use of &lsquo;mpsdk4j&rsquo; is still a bit difficult (that is why we write this blog post). Then we first simply understand the WeChat public platform opened the required elements, and their mapping relationship with &lsquo;mpsdk4j&rsquo;
WeChat Public Number property The serial number The property Example Note 1 The original ID of the public gh_20e50b3b4r9u The one that beginswith a gh s(don&rsquo;t understand what it means) 2 The public nickname is mpsdk4j The user-defined public number alias 3 The user&rsquo;s unique credentials (app ID) wxa822bd879532187 Starting with the letter wx, the meaning is probably WeChat&rsquo;s Pinyin initials 4 The user&rsquo;s unique credential key (app key) 613d3ce897hgf71a875d1342c8325f3d The 32-bit random string 5 AES plus decryption key JwAsfZH4p9iuuvfxjry6cLtlOgZAd853kJQ5hNv5OI4 A 43-bit random string 6 Developer service tokens weixindev Custom tokens and devices when a user receives a WeChat developer 7 The public number type S D: Subscription number, S: service number, E: Enterprise number (reserved field) 8 Whether to certify true true: Certified, false: Failed certification (i.e. also reserved field) Code mapping relationships in &lsquo;mpsdk4j&rsquo; So in &lsquo;mpsdk4j&rsquo; I designed an object, located in the &lsquo;io.github.elkan1788.mpsdk4j.vo&rsquo;package whose name is&rsquo;MPAccount&rsquo;,which translates directly tothe weChat public number. It will be above the code corresponding to the &ldquo;8&rdquo; public number attribute as follows:
/** :: WeChat Public Number Information * :: @author Dream Stardust (elkan1788@gmail.com). * @since 2.0 */ public class MPAccount { /** The original ID of the public number */ private String mpId; /** :: Public nickname */ private String nickName; /** :: App Id */ private String appId; /** :: Apply the key */ private String appSecret; /** :: Token */ private String token; /** AES secure encryption key */ private String AESKey; /** Public number type :: D: Subscription number :: E: Enterprise No :: S: Service number */ private String mpType; /** Whether it is certified or not */ private boolean pass; ...... } Friends with weChat development experience, it is not difficult to see that these properties are WeChat public number necessary, The previous 6 properties can be weChat public number management platform intuitively see, the back 2 attributes are extracted from the experience, in many of the later development there is a need to use. For example, when calling WeChat&rsquo;s speech recognition interface is to first understand whether the current public number is certified, only certified public number has the right to use speech recognition interface. In the actual project experience, you design this type of database table to manage WeChat public number information .
Maybe you&rsquo;ll think that reading these articles doesn&rsquo;t pay much, that doesn&rsquo;t matter, you can collect it and use it as a documentation tool. One day when you&rsquo;re developing you don&rsquo;t remember the meaning of one of these properties, you can go back and pick it up. If you have suggestions or comments, you can reply below
Reference:
Access Guide (https://mp.weixin.qq.com/wiki/8/f9a0b8382e0b77d87b3bcc1ce6fbc104.html) (https://mp.weixin.qq.com/wiki/14/70e73cedf9fd958d2e23264ba9333ad2.html)</content></entry><entry><title>MapDB synchronous read and write example</title><url>https://lisenhui.cn/en/2016/01/19/mapdb-write-read-sync.html</url><categories><category>Memory database</category></categories><tags><tag>Java</tag><tag>MapDB</tag><tag>Memory database</tag></tags><content type="html"> MapDB is a fast, easy-to-use embedded Java database engine. One of the most important features is the support of disk storage, directly in-memory Hash Map synchronous write to disk. Another particular surprise is that it supports ACED transactions, MVCC isolation, and full-time developer support
After reading the official documentation and examples, you can basically be sure that it meets the requirements of the business scenario. It was also found that officials were refactoring the 3.x version, but it shouldn&rsquo;t be released so soon. Use google to search the next use case for MapDB, not a lot. Maybe the original official documentation is complete about it, the API is not complicated, follow the official documentation to get started
After testing a simple example, a question pops up: How do you implement a database on a disk at the same time, and the same HashMap? It&rsquo;s important to understand here that mapDB stores database files on disk, not just a HashMap, which is a bit like the concept of multiple tables in a database. So the database can support multiple connections, mapDB is also supported? ( The ideal is really plump, but the reality is too bone-chilling!)
The preliminary test result is that MapDB does not support simultaneous access to the same file on disk. Then you can only create a long connection until business functionality processing is complete and then close it. Fortunately, it supports reading and writing to the same HashMap that already exists or is running. Here&rsquo;s a look at the simple sample code:
import org.mapdb.BTreeMap; import org.mapdb.DB; import org.mapdb.DBMaker; import org.testng.annotations.AfterTest; import org.testng.annotations.BeforeTest; import org.testng.annotations.Test; import java.io.File; import java.util.Map; import java.util.Random; import java.util.SortedMap; import static org.testng.Assert.*; /** :: MapDB test * :: @author Dream Stardust (elkan1788@gmail.com) * @since 2016.1.19 */ public class MapDBTest { private DB diskDB; Map&lt;Integer, String> data; @BeforeTest public void init() { The file name can be defined by itself File dbFile = new File("D:/mapdb.data"); DB has and only opens the connection once diskDB = DBMaker.fileDB(dbFile) Curious, turn off the lock, or not support multi-transaction access to the same database file . fileLockDisable() It is best to turn on, in a degree of abnormality or JVM shutdown can normally shut down the database There was one exception that could not access a database file that was not closed . closeOnJvmShutdown() Improve read and write efficiency if you can turn it off if you don't need to roll back . transactionDisable() The test here does not keep disk files . deleteFilesAfterClose() There are no read APIs found here, or multi-connections are not supported .make(); } @AfterTest public void destroy() { assertTrue(! data.isEmpty()); It was supposed to be 99, but other data in memory is merged Map&lt;Integer, String> temp = diskDB.treeMap("sort_mapdb"); assertEquals(temp.size(), 100); It is important to note here that databases that are likely to succeed in make are also closed If you don't check, you mightthrow: IllegalAccessError("DB was already closed"). if (diskDB.isClosed()) { diskDB.isClosed(); } } @Test(invocationCount = 10) public void testSyncWrite() throws Exception { Support for a variety of types of Map, such as B-tree, sort, and so on However, value appears to support reference types, does not support Object, it may be It's about serializing to disk storage data = diskDB.treeMapCreate("nice_mapdb") Turn on the quick counter . counterEnable() This step is critical, if you don't bring get, then it's just make, you can't support multiple connections . makeOrGet(); int len = 99; int ran = new Random(). nextInt(100)+1; while (--len >= 0) { data.put(len * ran, "value-"+len * ran); } assertFalse(data.isEmpty()); } @Test(invocationCount = 10) public void testReadAndDel() throws Exception { data = diskDB.treeMapCreate("nice_mapdb") . counterEnable() . makeOrGet(); if (! data.isEmpty()) { for (Integer key : data.keySet()) { if (key % 2 == 0 || key % 5 == 0) { data.remove(key); } } assertTrue(data.size() > 0); } } @Test public void testOtherMap() throws Exception { SortedMap&lt;Integer, String> data = diskDB.treeMapCreate("sort_mapdb") . counterEnable() . makeOrGet(); int len = 99; while (--len >= 0) { data.put(len, "sorted-"+len); } assertNotNull(data); Create another map BTreeMap&lt;Integer, String> btree = diskDB.treeMapCreate("sort_mapdb2") . counterEnable() . makeOrGet(); It's strange why the name here doesn't work and is automatically merged into all treeMaps in memory at the same time SortedMap&lt;Integer, String> tree = diskDB.treeMap("sort_mapdb1"); tree.put(100, "sorted-100"); btree.put(100, "sorted-101"); assertEquals(tree.get(100), "sorted-100"); assertEquals(data.get(100), "sorted-100"); } } There is no detailed discussion here about how MapDB achieves disk persistence synchronization. Directly using the official default values, of course, you can also configure your own read and write synchronization heartbeat interval. During the test process observation found that MapDB in the creation of disk database files, the initial case of 2MB, and then when syncing memory data, will first produce a temporary file, when this temporary file reaches a certain size will be merged into the main database files. As for other functions and code questions, to continue to observe, welcome to communicate together
Resources:
-MapDB&rsquo;s official website (http://www.mapdb.org/).
(official example) (https://github.com/jankotek/MapDB/tree/master/src/test/java/examples) -MapDB Implementation Analysis (http://hill007299.iteye.com/blog/2031208).</content></entry><entry><title>Redis installation and configuration</title><url>https://lisenhui.cn/en/2015/06/29/redis-install-settings.html</url><categories><category>Redis</category></categories><tags><tag>Cache</tag><tag>Redis</tag></tags><content type="html"> &lsquo;Redis&rsquo; is a high-performance &lsquo;Cache and Store&rsquo; storage system released under the &lsquo;BSD&rsquo; open source protocol. It is often referred to as a data structure server because values (&lsquo;value&rsquo;) can be types such as strings (&lsquo;String&rsquo;), hash (&lsquo;Map&rsquo;), lists (&lsquo;lists&rsquo;), collections (&lsquo;sets&rsquo;), and ordered collections (&lsquo;sorted sets&rsquo;). All of Redis' data is stored in memory and then saved asynchronously to disk from time to time (this is called &ldquo;semi-persistent mode&rdquo;), or every change in data can be written to an&rsquo;appendonly file'(this is called &lsquo;full persistence mode&rsquo;). (More introduction) (http://www.redis.cn/topics/introduction.html)
System environment: Linux 3.10.0-229.el7.x86_64 x86_64 x86_64 x86_64 GNU/Linux(Centos 7.1).
Download $ wget http://download.redis.io/releases/redis-3.0.1.tar.gz Unzim the installation $ tar -zxf redis-3.0.1.tar.gz $ CD. /redis-3.0.1 $ make &amp;&amp; make install Configure the Redis service $cp . /redis-3.0.1/utils/redis_init_script /etc/rc.d/init.d/redis $ mkdir P /etc/redis $ cp ./redis-3.0.1/redis.conf /etc/reddis/6379.conf Start Redis $ service redis start $ ps -ef|grep redis $ root 8687 1 0 12:06 ? 00:00:00 /usr/local/bin/redis-server *:6379 Descriptionof the &lsquo;redis.conf&rsquo;parameter &lsquo;daemonize&rsquo;:Whether to run the daemon way later
&lsquo;pidfile&rsquo;:&lsquo;pid&rsquo;file location
&lsquo;port&rsquo;: The port number of the listening
&lsquo;timeout&rsquo;: Request a timeout
&lsquo;loglevel&rsquo;:&lsquo;log&rsquo; information level
&lsquo;logfile&rsquo;: &lsquo;log&rsquo; file location
&lsquo;databases&rsquo;: The number of databases that are turned on
&lsquo;Save&rsquo;: How often the snapshot is saved, the first one indicates how long, and the third indicates how many writes are performed. When a certain number of writes are performed over a period of time, the snapshot is automatically saved. You can set more than one condition.
&lsquo;rdbcompression&rsquo;:Whether compression is used
&lsquo;dbfilename&rsquo;:Data snapshot file name (file name only, not directory).
&lsquo;dir&rsquo;:The saved directory of the data snapshot (this is the directory).
&lsquo;appendonly&rsquo;:Whether to turn on&rsquo;appendonlylog&rsquo;,open each write will remember a &lsquo;log&rsquo;, which will improve the data&rsquo;s ability to resistrisk, but affect efficiency.
&lsquo;appendfsync&rsquo;:&lsquo;appendonlylog&rsquo;howtosync todisk (threeoptions, one for each write, are to force the call to&rsquo;fsync&rsquo;,to enable&rsquo;fsync&rsquo; once persecond,and not to call&rsquo;fsync&rsquo;to wait for the system to synchronize itself).</content></entry><entry><title>Nutz source Jdoc has a garbled solution when filling in the prompt in the IDE</title><url>https://lisenhui.cn/en/2012/04/20/nutz-jdoc-chinese.html</url><categories><category>Useful</category></categories><tags><tag>Java</tag><tag>Useful</tag></tags><content type="html"> There was also a period of exposure to Nutz, with a constant understanding of its use, only to find that it is more powerful and the author&rsquo;s design is clever, especially like its JUnit test report, and the update speed is quite fast, to the present version of 1.b.44,ssh has the function can be said to be fully equipped. The quickest way for programmers to learn a new technology is the Demo-API,both of which are essential. Nutz is doing pretty well in this regard, for example, someone in Demo has contributed the entire CMS source code (thank you very much for sharing it and learning a lot from it), and the API provides common CHM formats and JAR packages. However, this JAR API in the implementation of the application is a little bit of a problem, let&rsquo;s go into more detail.
My development environment:
Operating system:Window7
Java Virtual Machine: JDK1.7
IDE Tool: Netbeans 7.1
Project encoding format:UTF-8
Create a simple WEB project with Netbeans, extract the Nutz-related files downloaded from GOOGLE CODE to create a new library reference necessary for development, these operations and display are normal, but when the code is automatically completed, found a problem, the code to complete the JDOC is actually garbled, as shown in the following image:
Hey, what&rsquo;s going on here? Re-examine your engineering coding properties to determine if there is nothing wrong with UTF-8, as shown in the following image:
Try to open the source view, but get a prompt saying, &ldquo;You can&rsquo;t safely open the file using gbK encoding format, do you want to continue opening it?&rdquo; ”
Did Nutz build JDOC using GBK encoding, it seems to have to connect to the GitHub library to download a library to see. Download it down to see that the encoding format of the project is also UTC-8, which is strange &ndash; where does the garbled code come from? It seems that we have to generate a JDOC to see, in the UTC-8 environment to generate JDOC should pay attention to the encoding format settings, as shown in the following figure,
After generating JDOC, directly modify the source code and JDOC connection of the Netbeans library and open the creation of the project using code auto-complement prompts everything to work
The problem is solved, but the cause of this problem really has to think about, the impact of different coding formats is really depressed. &ldquo;You can&rsquo;t safely open the file using the GBK encoding format, do you want to continue opening it?&rdquo; said the message &ldquo;Can&rsquo;t open the file safely without modification?&rdquo; Is it okay to modify Nutz&rsquo;s source code into a GBK encoding format as described in the information? So wrote a coding format rotation output program tested, the results show that the guess is correct, oh
In fact, this small program can not only convert nutz&rsquo;s source code, it can also convert any project&rsquo;s encoding format (only support JAVA files), note that by UTF-8 to GBK encoding format Oh, then slowly experience next Nutz to bring you the &ldquo;wonderful experience&rdquo; it, oh
PS: (http://dl.iteye.com/topics/download/a3e210f6-cdf8-3abe-9490-e6249ecaef0c)</content></entry><entry><title>Java program and RSR232 serial communication skills</title><url>https://lisenhui.cn/en/2012/03/24/java-hard-rsr232.html</url><categories><category>Java</category></categories><tags><tag>Java</tag><tag>RSR232</tag></tags><content type="html"> I&rsquo;ve been learning about J2EE application development and never thought about writing hardware interactions with JAVA, but I just like to try something new that I haven&rsquo;t come into contact with. Searched some resources on the Internet, learned that JAVA write serial communication is quite a lot, then began to prepare for the development of debugging environment. Software program development environment is not a problem, but this hardware environment is a bit difficult. Not to mention their own use of notebook where to string ah, and if you really take this string of hardware to their own will not make, then thought of the virtual machine, think this thing should also have a virtual bar, really with their own guesses as there is really this thing, by the way also downloaded a string of small assistant for debugging.
Here&rsquo;s a look at how the software environment is built:
:: Download &lsquo;comm.jar&rsquo;, &lsquo;win32com.dll&rsquo; and &lsquo;javax.comm.properties&rsquo;. (Accessories are available for download) Description: &lsquo;comm.jar&rsquo; provides the java API for communication, &lsquo;win32com.dll&rsquo; provides a local driver interface for &lsquo;comm.jar&rsquo; calls, &lsquo;javax.comm.properties&rsquo; is the class profile for this driver Copy &lsquo;javacomm.jar&rsquo; to &lsquo;X:\jre\lib\ext&rsquo; directory below; :: Copy &lsquo;javax.comm.properties&rsquo; to &lsquo;X:\jre.lib&rsquo; directory below; Copy &lsquo;win32com.dll&rsquo; to &lsquo;X:\jre&rsquo;bin&rsquo; directory below; :: Update the JDK environment inside the IDE, as shown below:
Then there is the hardware virtual environment installation virtual serial port, here I use VSPD6.0 (attachment provides download), after installation to start VSPD to add the ports we need, note that here is the way to add by group, such as COM1 and COM2 is a set of simultaneous additions, and so on.
Once all environments are ready, let&rsquo;s take a quick look at what comm .jar content. From the javadoc of the comm API alone, SUM provides us with only the following 13 classes or interfaces, as follows:
javax.comm.CommDriver javax.comm.CommPort javax.comm.ParallelPort javax.comm.SerialPort javax.comm.CommPortIdentifier javax.comm.CommPortOwnershipListener javax.comm.ParallelPortEvent javax.comm.SerialPortEvent javax.comm.ParallelPortEventListener (extends java.util.EventListener) javax.comm.SerialPortEventListener (extends java.util.EventListener) javax.comm.NoSuchPortException javax.comm.PortInUseException javax.comm.UnsupportedCommOperationException These classes and interfaces named at a glance to know its meaning, do not do a one-to-one introduction, you can go to the official website or the Internet to find more detailed information. Here&rsquo;s a test of whether the built environment is available, with the following main code:
Enumeration&lt;?> en = CommPortIdentifier.getPortIdentifiers(); CommPortIdentifier portId; while (en.hasMoreElements()) { portId (CommPortIdentifier) in.nextElement(); If the port type is serial, its port information is printed if (portId.getPortType() == CommPortIdentifier.PORT_SERIAL) { System.out.println(portId.getName()); } } After running the code, the console has the correct port to output (see figure below), indicating that all environments are ok to go down, otherwise check.
Finally, the problem of interaction with serial data is to be solved. The main difficulty with this is data reading, because we don&rsquo;t know when the port will have data coming or how long it will be. Typically, serial communication applications have two modes, one is to implement the SerialPortEventListener interface, to listen for and handle various serial events, and the other is to establish a separate receiving thread dedicated to receiving data. After referring to the code of many older generations, the following is the first way to write a simple assistant program, the specific implementation of the detailed code, as follows:
package com.elkan1788.view; import java.awt.BorderLayout; import java.awt.Button; import java.awt.Color; import java.awt.Font; import java.awt.GridLayout; import java.awt.Image; import java.awt.TextArea; import java.awt.TextField; import java.awt.event.ActionEvent; import java.awt.event.ActionListener; import java.io.IOException; import java.io.InputStream; import java.io.OutputStream; import java.util.ArrayList; import java.util.Enumeration; import java.util.List; import java.util.TooManyListenersException; import javax.comm.CommPortIdentifier; import javax.comm.NoSuchPortException; import javax.comm.PortInUseException; import javax.comm.SerialPort; import javax.comm.SerialPortEvent; import javax.comm.SerialPortEventListener; import javax.comm.UnsupportedCommOperationException; import javax.imageio.ImageIO; import javax.swing.JComboBox; import javax.swing.JFrame; import javax.swing.JLabel; import javax.swing.JOptionPane; import javax.swing.JPanel; import javax.swing.SwingConstants; import javax.swing.border.EmptyBorder; public class JavaRs232 extends JFrame implements ActionListener, SerialPortEventListener { /** * JDK Serial Version UID */ private static final long serialVersionUID = -7270865686330790103L; protected int WIN_WIDTH = 380; protected int WIN_HEIGHT = 300; private JComboBox&lt;?> portCombox, rateCombox, dataCombox, stopCombox, parityCombox; private Button openPortBtn, closePortBtn, sendMsgBtn; private TextField sendTf; private TextArea readTa; private JLabel statusLb; private String portname, rate, data, stop, parity; protected CommPortIdentifier portId; protected Enumeration&lt;?> ports; protected List&lt;String> portList; protected SerialPort serialPort; protected OutputStream outputStream = null; protected InputStream inputStream = null; protected String mesg; protected int sendCount, reciveCount; /** The default constructor */ public JavaRs232() { super ("Java RS-232 serial communication test program Van Dream Stardust"); setSize(WIN_WIDTH, WIN_HEIGHT); setLocationRelativeTo(null); Image icon = null; try { icon = ImageIO.read(JavaRs232.class.getResourceAsStream("/res/rs232.png")); } catch (IOException e) { showErrMesgbox(e.getMessage()); } setIconImage(icon); setResizable(false); scanPorts(); initComponents(); setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); setVisible(true); } /** Initialize each UI component :: @since March 22, 2012 11:56:39 PM */ public void initComponents() { Shared constants Font lbFont - new Font ("Microsoft Ya black," Font.TRUETYPE_FONT, 14); Create the left panel JPanel northPane = new JPanel(); northPane.setLayout(new GridLayout(1, 1)); Set the components of the left panel JPanel leftPane = new JPanel(); leftPane.setOpaque(false); leftPane.setLayout(new GridLayout(3,2)); JLabel portnameLb - new JLabel ("Serial slogan:" portnameLb.setFont(lbFont); portnameLb.setHorizontalAlignment(SwingConstants.RIGHT); portCombox = new JComboBox&lt;String>((String [])portList.toArray(new String[0])); portCombox.addActionListener(this); JLabel databitsLb - new JLabel ("data bit:"); databitsLb.setFont(lbFont); databitsLb.setHorizontalAlignment(SwingConstants.RIGHT); dataCombox = new JComboBox&lt;Integer>(new Integer[]{5, 6, 7, 8}); dataCombox.setSelectedIndex(3); dataCombox.addActionListener(this); JLabel ParityLb - new JLabel ("Check bit:"); parityLb.setFont(lbFont); parityLb.setHorizontalAlignment(SwingConstants.RIGHT); parityCombox = new JComboBox&lt;String>(new String[]{"NONE","ODD","EVEN","MARK","SPACE"}); parityCombox.addActionListener(this); Add components to the panel leftPane.add(portnameLb); leftPane.add(portCombox); leftPane.add(databitsLb); leftPane.add(dataCombox); leftPane.add(parityLb); leftPane.add(parityCombox); Create the right panel JPanel rightPane = new JPanel(); rightPane.setLayout(new GridLayout(3,2)); Set the components of the right panel JLabel BaudrateLb - new JLabel ("Porter rate:"); baudrateLb.setFont(lbFont); baudrateLb.setHorizontalAlignment(SwingConstants.RIGHT); rateCombox = new JComboBox&lt;Integer>(new Integer[]{2400,4800,9600,14400,19200,38400,56000}); rateCombox.setSelectedIndex(2); rateCombox.addActionListener(this); JLabel stopbitsLb s new JLabel ("stop bit:"); stopbitsLb.setFont(lbFont); stopbitsLb.setHorizontalAlignment(SwingConstants.RIGHT); stopCombox = new JComboBox&lt;String>(new String[]{"1","2","1.5"}); stopCombox.addActionListener(this); openPortBtn - new Button ("Open Port"); openPortBtn.addActionListener(this); closePortBtn - new Button ("close port"); closePortBtn.addActionListener(this); Add components to the panel rightPane.add(baudrateLb); rightPane.add(rateCombox); rightPane.add(stopbitsLb); rightPane.add(stopCombox); rightPane.add(openPortBtn); rightPane.add(closePortBtn); Add the left and right panel combinations to the north panel northPane.add(leftPane); northPane.add(rightPane); Create an intermediate panel JPanel centerPane = new JPanel(); Set up the components of the middle panel sendTf = new TextField(42); readTa = new TextArea(8,50); readTa.setEditable(false); readTa.setBackground(new Color(225,242,250)); centerPane.add(sendTf); sendMsgBtn - new Button ("Send"); sendMsgBtn.addActionListener(this); Add components to the panel centerPane.add(sendTf); centerPane.add(sendMsgBtn); centerPane.add(readTa); Set up the south component statusLb = new JLabel(); statusLb.setText(initStatus()); statusLb.setOpaque(true); Gets the container for the main form and consolidates the layout north, middle, and south of the three panels above JPanel contentPane = (JPanel)getContentPane(); contentPane.setLayout(new BorderLayout()); contentPane.setBorder(new EmptyBorder(0, 0, 0, 0)); contentPane.setOpaque(false); contentPane.add(northPane, BorderLayout.NORTH); contentPane.add(centerPane, BorderLayout.CENTER); contentPane.add(statusLb, BorderLayout.SOUTH); } /** The initialization status label displays text * @return String :: @since March 23, 2012 12:01:53 AM */ public String initStatus() { portname = portCombox.getSelectedItem().toString(); rate = rateCombox.getSelectedItem().toString(); data = dataCombox.getSelectedItem().toString(); stop = stopCombox.getSelectedItem().toString(); parity = parityCombox.getSelectedItem().toString(); StringBuffer str - new String Buffer ("Current string slogan:" str.append (portname).append ("porter rate:"); str.append (rate).append ("data bit:"); str.append (data.append ("stop bit"); str.append (stop.append("check bit:"); str.append(parity); return str.toString(); } /** :: Scan all COM ports on this machine :: @since March 23, 2012 12:02:42 AM */ public void scanPorts() { portList = new ArrayList&lt;String>(); Enumeration&lt;?> en = CommPortIdentifier.getPortIdentifiers(); CommPortIdentifier portId; while(en.hasMoreElements()){ portId (CommPortIdentifier) in.nextElement(); if(portId.getPortType() == CommPortIdentifier.PORT_SERIAL){ String name = portId.getName(); if(!portList.contains(name)) { portList.add(name); } } } if(null == portList || portList.isEmpty()) { ShowErrMesgbox ("No available serial port number found, program cannot start!"); System.exit(0); } } /** Open the serial port :: @since 2012-3-23 at 12:03:07 AM */ public void openSerialPort() { Gets the port you want to open try { portId = CommPortIdentifier.getPortIdentifier(portname); } catch (NoSuchPortException e) { showErrMesgbox ("Sorry, I didn't find the serial port number"); setComponentsEnabled(true); return ; } Open the port try { serialPort = (SerialPort) portId.open("JavaRs232", 2000); statusLb.setText (portname plus "serial port is open!"); } catch (PortInUseException e) { ShowErrMesgbox (portname plus "ports are occupied, please check!"); setComponentsEnabled(true); return ; } Set the port parameters try { int rate = Integer.parseInt(this.rate); int data = Integer.parseInt(this.data); int stop = stopCombox.getSelectedIndex()+1; int parity = parityCombox.getSelectedIndex(); serialPort.setSerialPortParams(rate,data,stop,parity); } catch (UnsupportedCommOperationException e) { showErrMesgbox(e.getMessage()); } Open the port's IO flow pipeline try { outputStream = serialPort.getOutputStream(); inputStream = serialPort.getInputStream(); } catch (IOException e) { showErrMesgbox(e.getMessage()); } Add a listener to the port try { serialPort.addEventListener(this); } catch (TooManyListenersException e) { showErrMesgbox(e.getMessage()); } serialPort.notifyOnDataAvailable(true); } /** Send data to the serial port :: @since 2012-3-23 at 12:05:00 AM */ public void sendDataToSeriaPort() { try { sendCount++; outputStream.write(mesg.getBytes()); outputStream.flush(); } catch (IOException e) { showErrMesgbox(e.getMessage()); } statusLb.setText ("Send: "sendCount plus" receive: ""-reciveCount"); } /** :: Close the serial port :: @since 2012-3-23 at 12:05:28 AM */ public void closeSerialPort() { try { if(outputStream != null) outputStream.close(); if(serialPort != null) serialPort.close(); serialPort = null; statusLb.setText (portname plus "serial port is closed!"); sendCount = 0; reciveCount = 0; sendTf.setText(""); readTa.setText(""); } catch (Exception e) { showErrMesgbox(e.getMessage()); } } /** :: Display an error or warning message :: @param msg information :: @since March 23, 2012 12:05:47 AM */ public void showErrMesgbox(String msg) { JOptionPane.showMessageDialog(this, msg); } /** :: Each component behavior event listens */ public void actionPerformed(ActionEvent e) { if(e.getSource() == portCombox || e.getSource() == rateCombox || e.getSource() == dataCombox || e.getSource() == stopCombox || e.getSource() == parityCombox){ statusLb.setText(initStatus()); } if(e.getSource() == openPortBtn){ setComponentsEnabled(false); openSerialPort(); } if(e.getSource() == closePortBtn){ if(serialPort != null){ closeSerialPort(); } setComponentsEnabled(true); } if(e.getSource() == sendMsgBtn){ if(serialPort == null){ ShowErrMesgbox ("Please open the serial port first!"); return ; } mesg = sendTf.getText(); if(null == mesg || mesg.isEmpty()){ ShowErrMesgbox ("Please enter what you want to send!"); return ; } sendDataToSeriaPort(); } } /** :: Port event monitoring */ public void serialEvent(SerialPortEvent event) { switch (event.getEventType()) { case SerialPortEvent.BI: case SerialPortEvent.OE: case SerialPortEvent.FE: case SerialPortEvent.PE: case SerialPortEvent.CD: case SerialPortEvent.CTS: case SerialPortEvent.DSR: case SerialPortEvent.RI: case SerialPortEvent.OUTPUT_BUFFER_EMPTY: break; case SerialPortEvent.DATA_AVAILABLE: byte[] readBuffer = new byte[50]; try { while (inputStream.available() > 0) { inputStream.read(readBuffer); } StringBuilder receivedMsg = new StringBuilder("/-- "); receivedMsg.append(new String(readBuffer).trim()).append(" --/\n"); readTa.append(receivedMsg.toString()); reciveCount++; statusLb.setText ("Send: "sendCount plus" receive: ""-reciveCount"); } catch (IOException e) { showErrMesgbox(e.getMessage()); } } } /** Set the switch status of each component :: @param the enabled status :: @since March 23, 2012 12:06:24 AM */ public void setComponentsEnabled(boolean enabled) { openPortBtn.setEnabled(enabled); openPortBtn.setEnabled(enabled); portCombox.setEnabled(enabled); rateCombox.setEnabled(enabled); dataCombox.setEnabled(enabled); stopCombox.setEnabled(enabled); parityCombox.setEnabled(enabled); } /** Run the main function * @param args :: @since 2012-3-23 at 12:06:45 AM */ public static void main(String[] args) { new JavaRs232(); } } The code is written, press the F11 key to enter the debugging state, everything is working well, please see figure:
:: Start the interface
Port detection
:: Communication testing
Finally take the time to beautify the program, the effect is more beautiful
PS: (http://dl.iteye.com/topics/download/80f67e6e-45eb-31ff-8086-da09f8d5762e)</content></entry><entry><title>Domestic technology power joins forces with Nutz, KindEditor, LHGDialog, My97DatePicker</title><url>https://lisenhui.cn/en/2012/01/13/nutz-ke-lhg-my97.html</url><categories><category>Useful</category></categories><tags><tag>Useful</tag><tag>KindEditor</tag></tags><content type="html"> For a while did not pay attention to the development of domestic IT technology, a few days ago in the study of an open source technology &lsquo;Nutz&rsquo; in the country want to practice a hand, but for a while do not know what to write good, think for a while or choose their own &ldquo;old friend&rdquo; &lsquo;KindEditor&rsquo;. Although it dare not be said to be a thorough understanding (personal JS level is limited, oh), but at least can also be very skilled use. Official website very early launched everyone&rsquo;s long-awaited &lsquo;KE4&rsquo;, but I did not update together, just this time to take it to experience, hip-hop. However, the thought of those &lsquo;KE&rsquo; application examples written earlier is monotonous, the last one is good to have &lsquo;EasyUI&rsquo; as the backing, but this UI framework for smaller than the hands-on project is still a bit big. So began to think about looking for other UI to see, suddenly up the previous use of the &lsquo;LHGDialog&rsquo; pop-up components are quite good, then went to its official website a circle. Didn&rsquo;t think it was really amazing ah, &lsquo;LHG&rsquo; is now also updated to 4 version, that effect is really exciting. Let&rsquo;s first appreciate the fruits of the labor that took more than 2 hours (now is the time for truth), ha ha&hellip;
In this has to praise the &lsquo;Nutz&rsquo; of the united states of high efficiency and simplicity, and the previous version of KE or the upload part of the &lsquo;JSP&rsquo; page translated into the background &lsquo;JAVA&rsquo; code, the only difference is that those same functions of the implementation code streamlined a lot ah, the official website example of the two &lsquo;JSP&rsquo; files have been compressed into a less than 400 lines of &lsquo;JAVA&rsquo; background code, the source code is as follows:
package org.nutz.kindeditor4.plugin; import java.io.File; import java.io.FileInputStream; import java.io.FileOutputStream; import java.io.IOException; import java.io.InputStream; import java.io.OutputStream; import java.text.SimpleDateFormat; import java.util.ArrayList; import java.util.Arrays; import java.util.Collections; import java.util.Comparator; import java.util.Date; import java.util.HashMap; import java.util.List; import java.util.Map; import javax.servlet.ServletContext; import org.nutz.ioc.loader.annotation.IocBean; import org.nutz.log.Log; import org.nutz.log.Logs; import org.nutz.mvc.annotation.AdaptBy; import org.nutz.mvc.annotation.At; import org.nutz.mvc.annotation.Param; import org.nutz.mvc.upload.UploadAdaptor; /** :: KindEditor online editor file upload, management module * @author Elkan(elkan1788@gmail.com) */ @At("/use/ke4plugin") @AdaptBy(type = UploadAdaptor.class) @IocBean public class KindEditor4Module { The log output object private static Log log = Logs.get(); The name of the file directory private String fileDir; The file suffix name private String fileExt; The current site context private String pageCtx; The real path to the site private String relPath; Upload the file save path private String savePath; Allows you to upload file suffix MAP arrays private static final HashMap&lt;String, String> extMap = new HashMap&lt;String, String>(); Allows you to upload an array of file size MAPS private static final HashMap&lt;String,Long> sizeMap = new HashMap&lt;String, Long>(); Upload the file to store the root private String filePath = "/attached/"; static { The initial suffix name MAP array extMap.put("image", "gif,jpg,jpeg,png,bmp"); extMap.put("flash", "swf,flv"); extMap.put("media", "swf,flv,mp3,wav,wma,wmv,mid,avi,mpg,asf,rm,rmvb"); extMap.put("file", "doc,docx,xls,xlsx,ppt,txt,zip,rar"); The initial file size MAP array sizeMap.put ("image," 100 - 1024l); sizeMap.put ("flash," 10 - 1024 - 1024l); sizeMap.put("average", 10 * 1024 * 1024l); sizeMap.put("file", 10 * 1024 * 1024l); } @At public Map&lt;String, Object> upload(@Param("imgFile") File tempFile, @Param("dir") String dir, ServletContext context) { The initial correlation variable Map&lt;String, Object> execute = new HashMap&lt;String, Object>(); pageCtx = context.getContextPath().concat(filePath); relPath = context.getRealPath(filePath); fileDir = dir; if (null == dir || dir.isEmpty()) { fileDir = "file"; } Check for uploaded files if (null == tempFile) { execute.put("error", 1); execute.put ("message", "Please choose to upload the file."); return execute; } Check to see if the upload file save directory exists or is writeable if (!isExistOrRwFolder()) { execute.put("error", 1); execute.put ("message", "Upload file save directory does not exist or there is no write permission, please check."); return execute; } Check that the directory name is correct if (!extMap.containsKey(fileDir)) { execute.put("error", 1); execute.put ("message", "Directory name is incorrect, please check."); return execute; } The file suffix name is calculated String tempFileName = tempFile.getName(); fileExt = tempFileName.substring(tempFileName.lastIndexOf(".") + 1); Check the type of upload file if(! Arrays.&lt;String>asList(extMap.get(fileDir).split(",")).contains(fileExt)){ execute.put("error", 1); execute.put ("message", "The format of the uploaded file is rejected, only allowed" and "fileDir" and "fileDir"); return execute; } Check the size of the uploaded file long maxSize = sizeMap.get(fileDir); if (tempFile.length() > maxSize) { execute.put("error", 1); String size = null; if(maxSize &lt; 1024) { size = maxSize + "B"; } if(maxSize > 1024 &amp;&amp; maxSize &lt; 1024 * 1024){ size = maxSize/1024 + "KB"; } if(maxSize > 1024 * 1024){ size = maxSize/(1024 * 1024) + "MB"; } execute.put ("message", "Upload file size exceeds the limit, only allows to upload files smaller than " s size . . . " ); return execute; } Generate a new file name and sort by date newSavePath(); Copy the uploaded files to the designated storage directory copy(tempFile, savePath); Calculate the file output path int point = savePath.lastIndexOf("/") - 8; StringBuilder url = new StringBuilder(pageCtx); url.append(fileDir).append("/"); url.append(savePath.substring(point)); Returns the output path of the uploaded file to the front end execute.put("error", 0); execute.put("url", url.toString()); return execute; } @At public Map&lt;String, Object> manager(@Param("dir") String dir, @Param("path") String path, @Param("order") String order, ServletContext context) { The initial correlation variable Map&lt;String, Object> execute = new HashMap&lt;String, Object>(); pageCtx = context.getContextPath().concat(filePath); relPath = context.getRealPath(filePath); fileDir = dir; if (null == dir || dir.isEmpty()) { fileDir = "file"; } if (!extMap.containsKey(fileDir)) { execute.put("error", 1); execute.put ("message", "Directory name is incorrect, please check."); return execute; } String tempPath = null == path ? fileDir.concat("/") : fileDir.concat("/"+path); String curPath = pageCtx.concat(tempPath); String curFileDir = relPath.concat("/"+tempPath); String curDir = path; String moveupDir = ""; Check that the current directory is root if (!"". equals(path)) { String str = curDir.substring(0, curDir.length() - 1); moveupDir = str.lastIndexOf("/") >= 0 ? str.substring(0, str.lastIndexOf("/") + 1): ""; } Check.. Command if(path.indexOf("..") >= 0){ execute.put("error", 1); execute.put ("message", "Not allowed:. command to return to the upper level."); return execute; } The last character is not / if (!"". equals(path) &amp;&amp; !path.endsWith("/")) { execute.put("error", 1); execute.put ("message", "file path is illegal."); return execute; } Check the current directory File curPathFile = new File(curFileDir); if (!curPathFile.isDirectory()) { execute.put("error", 1); execute.put ("message", "Current directory does not exist."); return execute; } Traversing the file information taken from the directory List&lt;HashMap> fileList = new ArrayList&lt;HashMap>(); if (curPathFile.listFiles() != null) { for (File file : curPathFile.listFiles()) { HashMap&lt;String, Object> hash = new HashMap&lt;String, Object>(); String fileName = file.getName(); if (file.isDirectory()) { hash.put("is_dir", true); hash.put("has_file", (file.listFiles() != null)); hash.put("filesize", 0L); hash.put("is_photo", false); hash.put("filetype", ""); } else if (file.isFile()) { fileExt = fileName.substring(fileName.lastIndexOf(".") + 1).toLowerCase(); hash.put("is_dir", false); hash.put("has_file", false); hash.put("filesize", file.length()); hash.put("is_photo", Arrays.&lt;String>asList(extMap.get("image").split(",")).contains(fileExt)); hash.put("filetype", fileExt); } hash.put("filename", fileName); hash.put("datetime", new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(file.lastModified())); fileList.add(hash); } } How files are sorted String tempOrder = order != null ? order.toLowerCase() : "name"; if ("size".equals(tempOrder)) { Collections.sort(fileList, new SizeComparator()); } else if ("type".equals(tempOrder)) { Collections.sort(fileList, new TypeComparator()); } else { Collections.sort(fileList, new NameComparator()); } Outputs the file information data after traversal execute.put("moveup_dir_path", moveupDir); execute.put("current_dir_path", curDir); execute.put("current_url", curPath); execute.put("total_count", fileList.size()); execute.put("file_list", fileList); return execute; } /** :: Determine whether the folder saved by the file upload exists or can be written :: @return "true" if it exists and can be written, otherwise "false" is returned */ public boolean isExistOrRwFolder(){ if(null == relPath || relPath.isEmpty()) { return false; } File folder = new File(relPath); if(!folder.exists()) return false; if(!folder.isDirectory()) return false; if(!folder.canWrite()) return false; return true; } /** :: Generate a new file name and manage by date */ public void newSavePath() { StringBuilder tempPath = new StringBuilder(relPath); tempPath.append("/").append(fileDir).append("/"); SimpleDateFormat folderNameFormat = new SimpleDateFormat("yyyyMMdd"); tempPath.append(folderNameFormat.format(new Date())); File temp = new File(tempPath.toString()); if(!temp.exists()) temp.mkdirs(); SimpleDateFormat fileNameFormat = new SimpleDateFormat("yyyyMMddkkmmss_S"); tempPath.append("/").append(fileNameFormat.format(new Date())); tempPath.append("."). append(fileExt); savePath = tempPath.toString().replaceAll("\\\\", "/"); } /** :: Copy the file :: @param src source file :: @param tar target path */ public void copy(File src, String tar) { Determines whether the source file or destination path is empty if (null == src || null == takes || tar.isEmpty()) { return; } InputStream srcIs = null; OutputStream tarOs = null; try { srcIs = new FileInputStream(src); File tarFile = new File(tar); tarOs = new FileOutputStream(tarFile); byte[] buffer = new byte[4096]; int n = 0; while (-1 != (n = srcIs.read(buffer))) { tarOs.write(buffer, 0, n); } } catch (IOException e) { log.error("Copy File is Fali, Because "+e); } finally { try { if (null != srcIs) { srcIs.close(); } if (null != tarOs) { tarOs.close(); } } catch (IOException e) { log.error("Close Stream is Fail, Because "+e); } } } /** Sort by file name */ public class NameComparator implements Comparator { @Override public int compare(Object a, Object b) { HashMah hashA = (HashMah) a; HashMap hashB = (HashMap) b; if (((Boolean) hashA.get("is_dir")) &amp;&amp; !( (Boolean) hashB.get("is_dir"))) { return -1; } else if (!( (Boolean) hashA.get("is_dir")) &amp;&amp; ((Boolean) hashB.get("is_dir"))) { return 1; } else { return ((String) hashA.get("filename")).compareTo((String) hashB.get("filename")); } } } /** Sort by file size */ public class SizeComparator implements Comparator { @Override public int compare(Object a, Object b) { HashMah hashA = (HashMah) a; HashMap hashB = (HashMap) b; if (((Boolean) hashA.get("is_dir")) &amp;&amp; !( (Boolean) hashB.get("is_dir"))) { return -1; } else if (!( (Boolean) hashA.get("is_dir")) &amp;&amp; ((Boolean) hashB.get("is_dir"))) { return 1; } else { if (((Long) hashA.get("filesize")) > ((Long) hashB.get("filesize"))) { return 1; } else if (((Long) hashA.get("filesize")) &lt; ((Long) hashB.get("filesize"))) { return -1; } else { return 0; } } } } /** Sort by file type */ public class TypeComparator implements Comparator { @Override public int compare(Object a, Object b) { HashMah hashA = (HashMah) a; HashMap hashB = (HashMap) b; if (((Boolean) hashA.get("is_dir")) &amp;&amp; !( (Boolean) hashB.get("is_dir"))) { return -1; } else if (!( (Boolean) hashA.get("is_dir")) &amp;&amp; ((Boolean) hashB.get("is_dir"))) { return 1; } else { return ((String) hashA.get("filetype")).compareTo((String) hashB.get("filetype")); } } } } Although the code is streamlined, but the function is a no ambiguity ah, and then enjoy the effect map.
Nutz for JAVA programmers, is in addition to SSH another choice, a good start, if you have to use believe that you will learn to love it, hip-hop, other not much to say, the following directly to the source code to serve it, because the time is tight (will have to take a bus home, Gaga, finally holiday) just write down, write not good also look at everyone understand.
PS: (http://dl.iteye.com/topics/download/f1325e10-20b7-3f71-a3e1-a43efd1574b5)</content></entry><entry><title>IP address query web interface call</title><url>https://lisenhui.cn/en/2011/11/18/whosip-tool.html</url><categories><category>Interface</category></categories><tags><tag>Interface</tag></tags><content type="html"> Today there happens to be a site to use an IP address display function, then think that there should be a free interface available, Baidu found the Pacific site to provide API, then the next is Code Time.
After reading its parameter description and call method, selected one of the jsFunction mode, now share the experience for everyone to refer to, the specific code and effect is as follows:
&lt;! DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd"> &lt;html> &lt;head> &lt;title>ip查询&lt;/title> &lt;script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.0/jquery.min.js">&lt;/script> &lt;script type="text/javascript"> $(function(){ $("&lt;span id='ipShow'>&lt;/span>"). appendTo("body"); $.getScript("http://whois.pconline.com.cn/jsFunction.jsp?callback=jsShow&amp;ip=61.235.82.163"); }); function jsShow(location){ $("#ipShow").html(location); } &lt;/script> &lt;/head> &lt;body> &lt;/body> &lt;/html> The effect is as follows:
The parameters are as follows:
There is no understanding of the place, you can leave a message to discuss.</content></entry><entry><title>JSP version of KindEidtor Online Editor Season 2 - Servlet and Struts2 Integrated Edition</title><url>https://lisenhui.cn/en/2011/10/17/kindeditor-jsp-struts2-servlet.html</url><categories><category>KindEditor</category></categories><tags><tag>KindEditor</tag><tag>Struts2</tag><tag>Servlet</tag></tags><content type="html"> Some time ago, I posted a post on the forum entitled &ldquo;JSP Edition&rsquo;s Complete KindEditor Online Editor (with attachment upload and image classification management features)&rdquo; (http://elkan1788.github.io/OnlineEditor/2011-03-24/) Kindeditor-jsp-complete .html) got a positive response from everyone, but I feel a little regret, there are a lot of people are not really talking about technical issues, but to me to ask for source code, to say that the real results of their own labor so in vain dedication, feel a little sorry for themselves, to know that our domestic technicians are not backed by money ah. Alas, I&rsquo;ve been too busy lately and I don&rsquo;t care too much about it. After seeing so many people left a mark, feel that they should also be selfless is it, we old Mao said right: to learn like Comrade Lei Feng, ha ha &hellip; In fact, above I have already said, this JAR inside the function I just put the official website JSP code adaptation only, nonsense first do not say much, the following directly on the code bar, can be secured Oh.
:: Image upload function code
package com.elkan.kindeditor.upload; import java.io.File; import java.io.IOException; import java.io.PrintWriter; import java.text.SimpleDateFormat; import java.util.Arrays; import java.util.Date; import java.util.Iterator; import java.util.List; import java.util.Random; import javax.servlet.ServletException; import javax.servlet.http.HttpServlet; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; import org.apache.commons.fileupload.FileItem; import org.apache.commons.fileupload.FileItemFactory; import org.apache.commons.fileupload.FileUploadException; import org.apache.commons.fileupload.disk.DiskFileItemFactory; import org.apache.commons.fileupload.servlet.ServletFileUpload; amount com.elkan.utils.ImageUtil; /** :: Implement a servlet for KindEditor image uploads * * @author SENHUI * * @since 2011/03/21 20:20:23 */ public class UploadImage extends HttpServlet { private static final long serialVersionUID = 5121794650920770832L; The maximum width of the uploaded image protected int MAX_WIDTH = -1; The maximum height at which the image is uploaded protected int MAX_HEIGHT = -1; The size of the uploaded image protected long MAX_SIZE = 1000000; Define the extension of the image that is allowed to be uploaded protected String[] IMAGETYPES = new String[] { "gif", "jpg", "jpeg", "png", "bmp" }; Define the upload image save directory path protected String UPLOAD_PATH = ""; Upload image settings information protected String id = ""; The TITLE property value of the uploaded image protected String imgTitle = ""; protected int imgWidth = -1; protected int imgHeight = -1; protected String imgBorder = ""; protected String resizeImg = ""; protected boolean isFlag = false; protected String tempTitle = ""; @SuppressWarnings("deprecation") @Override protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { response.setContentType("text/html; charset=UTF-8"); PrintWriter out = response.getWriter(); String savePath = this.getInitParameter("UPLOAD_PATH"); if (savePath == null || savePath.isEmpty()) { out.println (alert Msg ("You haven't set the directory path for uploading images to save!"); return; } Determines whether the size of the uploaded image is set if(this.getInitParameter("MAX_SIZE") != null){ MAX_SIZE = Integer.parseInt(this.getInitParameter("MAX_SIZE")); } Determines whether the type of uploaded image is set if(this.getInitParameter("IMAGETYPES") != null){ IMAGETYPES = toArray(this.getInitParameter("IMAGETYPES")); } The picture saves the catalog path String uploadPath = new StringBuffer(request.getSession().getServletContext().getRealPath("/")).append(savePath).toString(); Picture Save Catalog URL String saveUrl = new StringBuffer(request.getContextPath()).append("/").append(savePath).toString(); Check to see if the uploaded image exists if (! ServletFileUpload.isMultipartContent(request)) { out.println (alert Msg ("Please select the picture you want to upload!"); return; } Check the directory File uploadDir = new File(uploadPath); if (!uploadDir.isDirectory()) { out.println (alert Msg ("The directory saved by uploading the picture does not exist." )); return; } Check directory write permissions if (!uploadDir.canWrite()) { out.println (alert Msg ("The directory saved by uploading the image does not have write permission." )); return; } Prepare to upload the image FileItemFactory factory = new DiskFileItemFactory(); ServletFileUpload upload = new ServletFileUpload(factory); upload.setHeaderEncoding("UTF-8"); List&lt;?> items = null; String temp = null; try { items = upload.parseRequest(request); Iterator&lt;?> itr = items.iterator(); while (itr.hasNext()) { FileItem item = (FileItem) itr.next(); The original file name of the uploaded image String fileName = item.getName(); temp = (String) item.getName(); if(temp != null &amp;&amp; !isFlag){ temp = temp.substring(temp.lastIndexOf("\\")+1); tempTitle = temp; isFlag = true; } The ID of the KindEditor editor if(((String)item.getFieldName()).equals("id")){ id = item.getString(); } Ret tips for uploading images if(((String)item.getFieldName()).equals("imgTitle")){ imgTitle = item.getString(); if(imgTitle != null){ imgTitle = new String(imgTitle.getBytes("ISO8859-1"),"UTF-8"); } } Set the width of the picture if(((String)item.getFieldName()).equals("imgWidth")){ String imgWidth = item.getString(); if(imgWidth != null &amp;&amp; !imgWidth.isEmpty()){ this.imgWidth = Integer.parseInt(imgWidth); } } Set the height of the picture if(((String)item.getFieldName()).equals("imgHeight")){ String imgHeight = item.getString(); if(imgHeight != null &amp;&amp; !imgHeight.isEmpty()){ this.imgHeight = Integer.parseInt(imgHeight); } } Set the border of the picture if(((String)item.getFieldName()).equals("imgBorder")){ imgBorder = item.getString(); } long fileSize = item.getSize(); if (!item.isFormField()) { Check the file size if (fileSize > MAX_SIZE) { out.println (alert Msg ("Upload file size exceeds limit." )); return; } Check the extension String fileExt = fileName.substring(fileName.lastIndexOf(".") + 1).toLowerCase(); if (! Arrays.&lt;String> asList(IMAGETYPES).contains(fileExt)) { out.println ("Uploading an image extension is an extension that is not allowed). )); return; } Create a folder based on time SimpleDateFormat folderNameFormat = new SimpleDateFormat("yyyyMMdd"); String realPath = uploadPath + folderNameFormat.format(new Date()); File folder = new File(realPath); boolean flag = folder.exists(); Confirm that the folder already exists if(!flag){ flag = folder.mkdir(); } Create a folder and upload a picture if(flag){ SimpleDateFormat fileNameFormat = new SimpleDateFormat("yyyyMMddHHmmss"); String newFileName = fileNameFormat.format(new Date()) + "_"+ new Random().nextInt(1000) + "." + fileExt; File uploadedFile = new File(realPath, newFileName); item.write(uploadedFile); resizeImg = uploadedFile.getPath(); resizeImg = resizeImg.replaceAll("\\\\", "/"); saveUrl += folderNameFormat.format(new Date()) + "/" + newFileName; }else{ System.out.println ("Folder creation failed, make sure the disk is not written protected and there are enough blanks"); } } } Determines whether to set the maximum width and height of the picture String max_width = this.getInitParameter("MAX_WIDTH"); String max_height = this.getInitParameter("MAX_HEIGHT"); if((max_width != null &amp;&amp; !max_width.isEmpty())){ MAX_WIDTH = Integer.parseInt(max_width); } if(max_height != null &amp;&amp; !max_height.isEmpty()){ MAX_HEIGHT = Integer.parseInt(max_height); } if(imgTitle == null || imgTitle.isEmpty()){ imgTitle = tempTitle; } Determines whether you want to compress the picture if(MAX_WIDTH != -1 || MAX_HEIGHT != -1) { Compress the picture ImageUtil.resizeImg(resizeImg, resizeImg, MAX_WIDTH, MAX_HEIGHT); if(this.imgWidth > ImageUtil.ImgWidth){ this.imgWidth = ImageUtil.ImgWidth; } if(this.imgHeight > ImageUtil.ImgHeight){ this.imgHeight = ImageUtil.ImgHeight; } Return to the editor out.println(insertEditor(id, saveUrl, imgTitle, imgWidth, imgHeight, imgBorder)); }else{ Return to the editor out.println(insertEditor(id, saveUrl, imgTitle, imgWidth, imgHeight, imgBorder)); } } catch (FileUploadException e) { e.printStackTrace(); } catch (Exception e) { e.printStackTrace(); }finally{ out.flush(); out.close(); isFlag = false; } } @Override protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { doGet(request, response); } /** Output print upload failed JSON statement * :: @param message failure message * :: @return the failed JSON statement on the page */ public String alertMsg(String message) { StringBuffer sb = new StringBuffer("{\"error\":\"1\",\"message\":\""); sb.append(message).append("\"}"); return sb.toString(); } /** The output inserts a picture into the script of the editor statement * :: @param id editor ID * :: @param the browsing address of the image uploaded by saveUrl * :: @param information for imgTitle pictures * :: @param imgWidth to set the width of the picture * :: @param the width of the picture with imgHeight * :@param imgBorder to set the frame of the picture * :: @return script statement that inserts a picture into the editor */ public String insertEditor(String id, String saveUrl, String imgTitle, int imgWidth, int imgHeight, String imgBorder){ StringBuffer sb = new StringBuffer("&lt;script type\"text/javascript\">"); sb.append("parent. KE.plugin[\"image\"].insert(\"").append(id).append("\",\""); sb.append(saveUrl).append("\",\"").append(imgTitle).append("\",\""); sb.append(imgWidth).append("\",\"").append(imgHeight).append("\",\""); sb.append(imgBorder).append("\");"); sb.append("&lt;/script>"); return sb.toString(); } /** The output allows an array of image types to be uploaded * :: @param types of images allowed to be uploaded by fileType * :: @return allows you to upload image types */ public String[] toArray(String filesType){ if(filesType == null){ return null; } String[] types = filesType.split(","); String[] allowTypes = new String[types.length]; int i = 0; for(String type : types){ allowTypes[i] = type; i++; } return allowTypes; } } :: Upload an image management code
package com.elkan.kindeditor.upload; import java.io.File; import java.io.IOException; import java.io.PrintWriter; import java.text.SimpleDateFormat; import java.util.ArrayList; import java.util.Arrays; import java.util.Collections; import java.util.Comparator; import java.util.Hashtable; import java.util.List; import javax.servlet.ServletException; import javax.servlet.http.HttpServlet; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; public class UploadImageManager extends HttpServlet { private static final long serialVersionUID = -8359652838938248988L; Define the extension of the image that is allowed to be uploaded protected String[] FILETYPES = new String[] { "gif", "jpg", "jpeg", "png", "bmp" }; Define the upload image save directory path protected String UPLOAD_PATH = ""; public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { response.setContentType("text/html; charset=UTF-8"); PrintWriter out = response.getWriter(); String savePath = this.getInitParameter("UPLOAD_PATH"); if (savePath == null || savePath.isEmpty()) { out.println (alert Msg ("You haven't set a directory path to read uploaded images yet!"); return; } The picture saves the catalog path String rootPath = new StringBuffer(request.getSession().getServletContext().getRealPath("/")).append(savePath).toString(); Picture Save Catalog URL String rootUrl = new StringBuffer(request.getContextPath()).append("/").append(savePath).toString(); Depending on the path parameters, set the paths and URLs String path = request.getParameter("path") != null ? request.getParameter("path") : ""; String currentPath = rootPath + path; String currentUrl = rootUrl + path; String currentDirPath = path; String moveupDirPath = ""; if (!"". equals(path)) { String str = currentDirPath.substring(0, currentDirPath.length() - 1); moveupDirPath = str.lastIndexOf("/") >= 0? str.substring(0, str.lastIndexOf("/") + 1): ""; } Sort form, name or size or type String order = request.getParameter("order") != null ? request.getParameter("order").toLowerCase() : "name"; Not allowed to use: Move to the next level of the directory if (path.indexOf("..") >= 0) { out.println (alert Msg ("No move to the next level directory"); return; } The last character is not / if (!"". equals(path) &amp;&amp; !path.endsWith("/")) { out.println("Parameter is not valid."); return; } The directory does not exist or is not File currentPathFile = new File(currentPath); if(!currentPathFile.isDirectory()){ out.println("Directory does not exist."); return; } Traversing the file information taken from the directory List&lt;Hashtable&lt;?,?>> fileList = new ArrayList&lt;Hashtable&lt;?,?>>(); if(currentPathFile.listFiles() != null) { for (File file : currentPathFile.listFiles()) { Hashtable&lt;String, Object> hash = new Hashtable&lt;String, Object>(); String fileName = file.getName(); if(file.isDirectory()) { hash.put("is_dir", true); hash.put("has_file", (file.listFiles() != null)); hash.put("filesize", 0L); hash.put("is_photo", false); hash.put("filetype", ""); } else if(file.isFile()){ String fileExt = fileName.substring(fileName.lastIndexOf(".") + 1).toLowerCase(); hash.put("is_dir", false); hash.put("has_file", false); hash.put("filesize", file.length()); hash.put("is_photo", Arrays.&lt;String>asList(FILETYPES).contains(fileExt)); hash.put("filetype", fileExt); } hash.put("filename", fileName); hash.put("datetime", new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(file.lastModified())); fileList.add(hash); } } if ("size".equals(order)) { Collections.sort(fileList, new SizeComparator()); } else if ("type".equals(order)) { Collections.sort(fileList, new TypeComparator()); } else { Collections.sort(fileList, new NameComparator()); } out.println(toJSONString(currentUrl, currentDirPath, moveupDirPath, fileList)); out.flush(); out.close(); } public void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { doGet(request, response); } /** Output the script that prints the failed statement uploaded * :: @param message failure message * :: @return statement printed on the page */ public String alertMsg(String message) { StringBuffer sb = new StringBuffer("&lt;script type\"text/javascript\">"); sb.append("alert(\"").append(message).append("\");"); sb.append("&lt;/script>"); return sb.toString(); } public String toJSONString(String currentUrl, String currentDirPath, String moveupDirPath, List&lt;Hashtable&lt;?, ?>> fileList){ StringBuilder sb = new StringBuilder("{\"current_url\":\""); sb.append(currentUrl).append("\",").append("\"current_dir_path\":\""); sb.append(currentDirPath).append("\",\"moveup_dir_path\":\"").append(moveupDirPath).append("\","); sb.append("\"file_list\":["); int i = 0; sb.append("{"); for(Hashtable&lt;?,?> he : fileList){ if(i != (fileList.size() - 1)){ sb.append("\"filename\":\"").append(he.get("filename")).append("\","); sb.append("\"filesize\":").append(he.get("filesize")).append(","); sb.append("\"filetype\":\"").append(he.get("filetype")).append("\","); sb.append("\"has_file\":").append(he.get("has_file")).append(","); sb.append("\"is_dir\":").append(he.get("is_dir")).append(","); sb.append("\"is_photo\":").append(he.get("is_photo")).append(","); sb.append("\"datetime\":\"").append(he.get("datetime")).append("\""); sb.append("},{"); }else{ sb.append("\"filename\":\"").append(he.get("filename")).append("\","); sb.append("\"filesize\":").append(he.get("filesize")).append(","); sb.append("\"filetype\":\"").append(he.get("filetype")).append("\","); sb.append("\"has_file\":").append(he.get("has_file")).append(","); sb.append("\"is_dir\":").append(he.get("is_dir")).append(","); sb.append("\"is_photo\":").append(he.get("is_photo")).append(","); sb.append("\"datetime\":\"").append(he.get("datetime")).append("\""); sb.append("}"); } i++; } i = 0; sb.append("],\"total_count\":").append(fileList.size()).append("}"); return sb.toString(); } public class NameComparator implements Comparator&lt;Object> { public int compare(Object a, Object b) { Hashtable&lt;?, ?> hashA = (Hashtable&lt;?, ?>) a; Hashtable&lt;?, ?> hashB = (Hashtable&lt;?, ?>) b; if (((Boolean) hashA.get("is_dir")) &amp;&amp; ! ((Boolean) hashB.get("is_dir"))) { return -1; } else if (!( (Boolean) hashA.get("is_dir")) &amp;&amp; ((Boolean) hashB.get("is_dir"))) { return 1; } else { return ((String) hashA.get("filename")).compareTo((String) hashB.get("filename")); } } } public class SizeComparator implements Comparator&lt;Object> { public int compare(Object a, Object b) { Hashtable&lt;?, ?> hashA = (Hashtable&lt;?, ?>) a; Hashtable&lt;?, ?> hashB = (Hashtable&lt;?, ?>) b; if (((Boolean) hashA.get("is_dir")) &amp;&amp; ! ((Boolean) hashB.get("is_dir"))) { return -1; } else if (!( (Boolean) hashA.get("is_dir")) &amp;&amp; ((Boolean) hashB.get("is_dir"))) { return 1; } else { if (((Long) hashA.get("filesize")) > ((Long) hashB.get("filesize"))) { return 1; } else if (((Long) hashA.get("filesize")) &lt; ((Long) hashB.get("filesize"))) { return -1; } else { return 0; } } } } public class TypeComparator implements Comparator&lt;Object> { public int compare(Object a, Object b) { Hashtable&lt;?, ?> hashA = (Hashtable&lt;?, ?>) a; Hashtable&lt;?, ?> hashB = (Hashtable&lt;?, ?>) b; if (((Boolean) hashA.get("is_dir")) &amp;&amp; ! ((Boolean) hashB.get("is_dir"))) { return -1; } else if (!( (Boolean) hashA.get("is_dir")) &amp;&amp; ((Boolean) hashB.get("is_dir"))) { return 1; } else { return ((String) hashA.get("filetype")).compareTo((String) hashB.get("filetype")); } } } } :: Attachment upload code
package com.elkan.kindeditor.upload; import java.io.File; import java.io.IOException; import java.io.PrintWriter; import java.text.SimpleDateFormat; import java.util.Arrays; import java.util.Date; import java.util.Iterator; import java.util.List; import java.util.Random; import javax.servlet.ServletException; import javax.servlet.http.HttpServlet; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; import org.apache.commons.fileupload.FileItem; import org.apache.commons.fileupload.FileItemFactory; import org.apache.commons.fileupload.disk.DiskFileItemFactory; import org.apache.commons.fileupload.servlet.ServletFileUpload; public class UploadAccessory extends HttpServlet { private static final long serialVersionUID = 1L; The size of the uploaded file protected long MAX_SIZE = 1000000; Define the extension of the file that is allowed to be uploaded protected String[] FILETYPES = new String[]{"doc", "xls", "ppt", "pdf", "txt", "rar" , "zip"}; Define the upload file save directory path protected String UPLOAD_PATH = ""; protected String id = ""; protected String attachTitle = ""; protected boolean isFlag = false; protected String tempTitle = ""; public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { doPost(request, response); } public void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { response.setContentType("text/html; charset=UTF-8"); PrintWriter out = response.getWriter(); String savePath = this.getInitParameter("UPLOAD_PATH"); if (savePath == null || savePath.isEmpty()) { out.println (alert Msg ("You haven't set the directory path for uploading files to save!"); return; } Determines whether the size of the uploaded file is set if(this.getInitParameter("MAX_SIZE") != null){ MAX_SIZE = Integer.parseInt(this.getInitParameter("MAX_SIZE")); } Determines whether the type of upload file is set if(this.getInitParameter("FILETYPES") != null){ FILETYPES = toArray(this.getInitParameter("FILETYPES")); } The file saves the directory path String uploadPath = new StringBuffer(request.getSession().getServletContext().getRealPath("/")).append(savePath).toString(); The file saves the directory URL String saveUrl = new StringBuffer(request.getContextPath()).append("/").append(savePath).toString(); if(! ServletFileUpload.isMultipartContent(request)){ out.println ("Please select the file to upload." )); return; } Check the directory File uploadDir = new File(uploadPath); if(!uploadDir.isDirectory()){ out.println (alert Msg ("Upload directory does not exist." )); return; } Check directory write permissions if(!uploadDir.canWrite()){ out.println ("The current role does not have write permission to upload directories." )); return; } FileItemFactory factory = new DiskFileItemFactory(); ServletFileUpload upload = new ServletFileUpload(factory); upload.setHeaderEncoding("UTF-8"); String temp = null; String ext = null; try{ List&lt;?> items = upload.parseRequest(request); Iterator&lt;?> itr = items.iterator(); while (itr.hasNext()) { FileItem item = (FileItem) itr.next(); String fileName = item.getName(); temp = (String) item.getName(); if(temp != null &amp;&amp; !isFlag){ temp = temp.substring(temp.lastIndexOf("\\")+1); tempTitle = temp; isFlag = true; } The ID of the KindEditor editor if(((String)item.getFieldName()).equals("id")){ id = item.getString(); } Ret tips for uploading images if(((String)item.getFieldName()).equals("attachTitle")){ attachTitle = item.getString(); if(attachTitle != null){ attachTitle = new String(attachTitle.getBytes("ISO8859-1"),"UTF-8"); } } if (!item.isFormField()) { Check the file size if(item.getSize() > MAX_SIZE){ out.println (alert Msg ("Upload file size exceeds limit." )); return; } Check the extension String fileExt = fileName.substring(fileName.lastIndexOf(".") + 1).toLowerCase(); if(! Arrays.&lt;String>asList(FILETYPES).contains(fileExt)){ out.println (alert Msg ("Uploading file extensions is not allowed extensions." )); return; } Create a folder based on time SimpleDateFormat folderNameFormat = new SimpleDateFormat("yyyyMMdd"); String realPath = uploadPath + folderNameFormat.format(new Date()); File folder = new File(realPath); boolean flag = folder.exists(); Confirm that the folder already exists if(!flag){ flag = folder.mkdir(); } Create a folder and upload a picture if(flag){ SimpleDateFormat fileNameFormat = new SimpleDateFormat("yyyyMMddHHmmss"); String newFileName = fileNameFormat.format(new Date()) + "_"+ new Random().nextInt(1000) + "." + fileExt; File uploadedFile = new File(realPath, newFileName); item.write(uploadedFile); saveUrl += folderNameFormat.format(new Date()) + "/" + newFileName; ext = fileExt; }else{ System.out.println ("Folder creation failed, make sure the disk is not written protected and there are enough blanks"); } } } if(attachTitle == null || attachTitle.isEmpty()){ attachTitle = tempTitle; } out.println(insertAttach(id, saveUrl, attachTitle, ext)); }catch(Exception e){ e.printStackTrace(); }finally{ out.flush(); out.close(); isFlag = false; } } /** Output the script that prints the failed statement uploaded * :: @param message failure message * :: @return statement printed on the page */ public String alertMsg(String message){ StringBuilder sb = new StringBuilder("&lt;html>"); sb.append("&lt;head>").append("&lt;title>error&lt;/title>"); sb.append("&lt;meta http-equiv=\"content-type\" content=\"text/html; charset=utf-8\">"); sb.append("&lt;/head>"); sb.append("&lt;body>"); sb.append("&lt;script type=\"text/javascript\">"); sb.append("alert(\"").append(message).append("\");history.back();&lt;/script>"); sb.append("&lt;/body>").append("&lt;/html>"); return sb.toString(); } /** The output is a script that inserts attachments into editor statements * :: @param id editor ID * :: @param the address of the attachment to the url * :: @param the title property that was set when it was uploaded * :: @param the suffix name of the file uploaded by ext * :: @return script statement that inserts attachments into the editor */ public String insertAttach(String id, String url, String title, String ext){ StringBuilder sb = new StringBuilder("&lt;html>"); sb.append("&lt;head>").append("&lt;title>Insert Accessory&lt;/title>"); sb.append("&lt;meta http-equiv=\"content-type\" content=\"text/html; charset=utf-8\">"); sb.append("&lt;/head>"); sb.append("&lt;body>"); sb.append("&lt;script type=\"text/javascript\">"); sb.append("parent. KE.plugin[\"accessory\"].insert(\"").append(id).append("\",\""); sb.append(url).append("\",\"").append(title).append("\",\"").append(ext).append("\");&lt;/script>"); sb.append("&lt;/body>").append("&lt;/html>"); return sb.toString(); } /** The output allows an array of image types to be uploaded * :: @param types of images allowed to be uploaded by fileType * :: @return allows you to upload image types */ public String[] toArray(String filesType){ if(filesType == null){ return null; } String[] types = filesType.split(","); String[] allowTypes = new String[types.length]; int i = 0; for(String type : types){ allowTypes[i] = type; i++; } return allowTypes; } } :: Image compression code
package com.elkan.utils; import java.awt.Image; import java.awt.image.BufferedImage; import java.io.File; import java.io.FileOutputStream; import java.io.IOException; import javax.imageio.ImageIO; import com.sun.image.codec.jpeg.JPEGCodec; import com.sun.image.codec.jpeg.JPEGImageEncoder; /** :: How to process the picture * * @author SENHUI */ public class ImageUtil { public static int ImgWidth = -1; public static int ImgHeight = -1; /** :: Compress the picture * * @param imgsrc The source file * @param imgdist :: Target file * @param widthdist :: Wide * @param heightdist :: High */ public static void resizeImg(String imgsrc, String imgdist, int widthdist, int heightdist) { try { File srcfile = new File(imgsrc); if (!srcfile.exists()) { return; } Image src = ImageIO.read(srcfile); ImgWidth = src.getWidth(null); ImgHeight = src.getHeight(null); if(ImgWidth &lt; widthdist){ widthdist = ImgWidth; }else{ ImgWidth = widthdist; } if(ImgHeight &lt; heightdist){ heightdist = ImgHeight; }else{ ImgHeight = heightdist; } BufferedImage tag = new BufferedImage(widthdist, heightdist,BufferedImage.TYPE_INT_RGB); tag.getGraphics().drawImage(src.getScaledInstance(widthdist, heightdist,Image.SCALE_SMOOTH), 0, 0, null); FileOutputStream out = new FileOutputStream(imgdist); JPEGImageEncoder encoder = JPEGCodec.createJPEGEncoder(out); encoder.encode(tag); out.close(); } catch (IOException ex) { ex.printStackTrace(); } } } Oh, the source code published here, there are more detailed comments on the document, I believe you can see a clear. Now we have any custom functions to develop to use it, don&rsquo;t forget to share your developed Dongdong Oh, I&rsquo;m here waiting for your big drive ah, hip-hop &hellip; Whether you are satisfied with this article or not, leave a mark on it, the next time you use it, come and get it, huh? By the end to give everyone a stay job, how to upload the attachment classification management? For example: word put in the word folder directory&hellip; (This feature has been implemented, but not in the published code, let&rsquo;s think about it and see who&rsquo;s the best approach, huh).
PS: Answers to some questions
Why can&rsquo;t I use Struts2? A: The filter map that is generally configured for Struts2 in project development is all station resources, modified to simply filter your Struts2 access resources. (Now that the source code is published, you can write those upload methods into Action)
There are definition upload types everywhere, isn&rsquo;t it cumbersome? A: In the web.xml configuration upload type is the original release to take into account the reuse of the problem, in addition to the default upload file type should be enough, as long as the size and save path can be;
Can I add code highlighting? A: The original revision of this editor when the starting point is suitable for our project, so this function is not considered, but there is no code highlight plug-in on the Internet, can refer to the design.
Can I paste the picture inside the word document? A: This feature is really not Oh, I am not very familiar with the web front end, but I did develop the function of table merge, is still in the testing phase. If only I could develop this picture paste, but it seems to be plug-in support, alas, windows stuff is not very fun.</content></entry><entry><title>MyEclipse6.5 plus IDE Hanhua software</title><url>https://lisenhui.cn/en/2011/05/16/myeclipse-chinese-tool.html</url><categories><category>Tools</category></categories><tags><tag>MyEclipse</tag><tag>IDE</tag></tags><content type="html"> There are millions of languages and words in the world, but always feel that our square Chinese characters are better looking and more meaningful. And in the field of computers have been dominated by foreign countries, so many computers are the program in English version, sometimes it is really not convenient to use, so there is a large number of Chinese version of the program, these programs have been favored by the Chinese people.
The most commonly used IDEs in the JAVA world are &lsquo;Eclipse&rsquo;and&rsquo;MyEclisep&rsquo;,both of which are developed by foreigners, so it&rsquo;s no wonder that the IDE interfaceis in English. As another up-and-comer&rsquo;Netbeans&rsquo;development tool I think is more popular with domestic programming beginners, why? Quite simply, its interface Chinese.
Or first back to our theme today&rsquo;s&rsquo;MyEclisep' Hanhuaprogram, &lsquo;Eclipse&rsquo; Hanhua need not say more, they go directly to the official website to download a language pack to achieve the Chinese interface, but &lsquo;MyEclipse&rsquo; is not so simple, previously found a cow man wrote Hanhua package on the Internet, the trial effect is good, but the steps are a bit troublesome, last night suddenly wonder if you can do a fool&rsquo;s Hanhua program? Think of want to think that the feasibility of about 70%, the final measure or decide CODE, and finally in about 3 hours after the success of this Chinese program, the interface effect is as follows:
If you feel the need to download a go back to use it, here&rsquo;s how to use this software:
:: Directly unzip the downloaded compression package, remember not to break the directory structure or you will not be able to Hanhua, double-click &lsquo;MyEclipse&rsquo; Hanhua software (transparent) .exe (this requires the latest version of JDK) or &lsquo;MyEclipse&rsquo; Hanhua software (no transparency) .exe;
:: Browse and select the &lsquo;Common&rsquo; directory under the &lsquo;MyEclipse&rsquo; installation root, depending on where you install it;
:: Browse and select the &lsquo;bundles.info&rsquo; (plug-in-specified) file, which is listed under the &lsquo;MyEclipse
:: Browse and select the &lsquo;myeclipse .ini&rsquo; files, some of which are called &lsquo;MyEclipse&rsquo; . .. &lsquo;Directory; .
:: After clicking the Start Hanhua button, if successful, the following success prompt will appear, now you restart the next &lsquo;MyEclipse&rsquo; program to see, preferably with the &lsquo;-clean&rsquo; command;
Take a look at the &lsquo;MyEclipse&rsquo; interface after Hanhua:
May be because the Hanhua package is a bit old original, so it is not completely Hanhua, if you have a better Hanhua bag, I hope you can share with you. Finally, if you feel that the effect of Hanhua is not ideal and want to restore the English interface, just restore the corresponding folder directory under the &lsquo;bundles_backup.info&rsquo; and &lsquo;myeclipse_backup.ini&rsquo; file restart under the &lsquo;MyEclipse&rsquo; software on it.
Well, hanhua work is over here, now everyone wants to say something to follow the post to shoot bricks.</content></entry><entry><title>Open JSP KindEditor's attachment JAR package source code</title><url>https://lisenhui.cn/en/2011/05/05/kindeditor-jsp-source.html</url><categories><category>KindEditor</category></categories><tags><tag>Java</tag><tag>KindEditor</tag></tags><content type="html"> I didn&rsquo;t expect the response to be so strong when I wrote a JSP version of kindeditor editor in March. However, with the growth of the day and month, this version of the plug-in also exposed some BUG, such as: Struts2 how to integrate, web.xml file configuration upload properties inconvenient to modify and cumbersome, upload pictures (attachments) can not be saved in other disks &hellip; &hellip;&hellip; 。 Now usually developed projects are using KE as an online editor, in order to be better and more convenient to use this editor, in the rest of the time to refactor the original code re-encapsulation, in addition to the previous version of the BUG, but also unified the overall naming specifications, added some new features.
The version number of the current version of the plug-in is:Kindeditor-plugin0.4RELEASE, and the list of classes in the JAR package is as follows:
The main functions accomplished by this refactoring are the following: :: Reconstruct the selection button of upload attachment page press, imitation image upload; :: Increased Struts2 environmental integration; :: Increase the upload property configuration function to facilitate the site to be modified (not yet open); :: Add other disk storage functions, free choice of storage location for easy backup (not yet open); :: Add the text watermark function of the image (not yet open); :: More Kindeditor edited version for 3.5.6; :: Upload attachment classification management
If you want to apply this KE plug-in to your project, it&rsquo;s simple, if the servlet environment takes only one step, the Struts2 environment requires two steps, as follows:
Servlet environment: You only need to configure .xml parameters in the web environment &lt;?xml version="1.0" encoding="UTF-8"?> &lt;web-app version="2.5" xmlns="http://java.sun.com/xml/ns/javaee" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd"> &lt;servlet> &lt;servlet-name>KEUploadImgServlet&lt;/servlet-name> &lt;servlet-class>com.elkan.kindeditor.servlet.plugin.KEUploadImgServlet&lt;/servlet-class> &lt;init-param> &lt;param-name>IMGSAVEPATH&lt;/param-name> &lt;param-value>/upload/image/&lt;/param-value> &lt;/init-param> &lt;!-- By default, pass the picture size &lt;init-param> &lt;param-name>MAXSIZE&lt;/param-name> &lt;param-value>1048576&lt;/param-value> &lt;/init-param> The type of picture passed on by default &lt;init-param> &lt;param-name>IMGTYPES&lt;/param-name> &lt;param-value>jpg,jpeg,png,gif,bmp&lt;/param-value> &lt;/init-param> The picture is not compressed by default &lt;init-param> &lt;param-name>MAXWIDTH&lt;/param-name> &lt;param-value>&lt;/param-value> &lt;/init-param> &lt;init-param> &lt;param-name>MAXHEIGHT&lt;/param-name> &lt;param-value>&lt;/param-value> &lt;/init-param> --> &lt;/servlet> &lt;servlet> &lt;servlet-name>KEManageImgServlet&lt;/servlet-name> &lt;servlet-class>com.elkan.kindeditor.servlet.plugin.KEManageImgServlet&lt;/servlet-class> &lt;init-param> &lt;param-name>IMGSAVEPATH&lt;/param-name> &lt;param-value>/upload/image/&lt;/param-value> &lt;/init-param> &lt;/servlet> &lt;servlet> &lt;servlet-name>KEUploadAttachServlet&lt;/servlet-name> &lt;servlet-class>com.elkan.kindeditor.servlet.plugin.KEUploadAttachServlet&lt;/servlet-class> &lt;init-param> &lt;param-name>ATTACHSAVEPATH&lt;/param-name> &lt;param-value>/upload/attach/&lt;/param-value> &lt;/init-param> &lt;!-- Upload attachment size by default &lt;init-param> &lt;param-name>MAXSIZE&lt;/param-name> &lt;param-value>10485760&lt;/param-value> &lt;/init-param> The attachment type is uploaded by default &lt;init-param> &lt;param-name>ATTACHTYPES&lt;/param-name> &lt;param-value>**&lt;/param-value> &lt;/init-param> --> &lt;/servlet> &lt;servlet-mapping> &lt;servlet-name>KEUploadImgServlet&lt;/servlet-name> &lt;url-pattern>/keplugin/KEUploadImg.servlet&lt;/url-pattern> &lt;/servlet-mapping> &lt;servlet-mapping> &lt;servlet-name>KEManageImgServlet&lt;/servlet-name> &lt;url-pattern>/keplugin/KEManageImages.servlet&lt;/url-pattern> &lt;/servlet-mapping> &lt;servlet-mapping> &lt;servlet-name>KEUploadAttachServlet&lt;/servlet-name> &lt;url-pattern>/keplugin/KEUploadAttach.servlet&lt;/url-pattern> &lt;/servlet-mapping> &lt;welcome-file-list> &lt;welcome-file>index.jsp&lt;/welcome-file> &lt;/welcome-file-list> &lt;login-config> &lt;auth-method>BASIC&lt;/auth-method> &lt;/login-config> &lt;/web-app> KindEditor JS script configuration on the Jsp page:
KE.show({ id: "editorServlet", resizeMode: 0, allowFileManager : true, imageUploadJson: "/KEPlugin/keplugin/KEUploadImg.servlet", fileManagerJson: "/KEPlugin/keplugin/KEManageImages.servlet", The default is . . . for all types of files //accessoryTypes: "doc|docx", accessoryUploadJson: "/KEPlugin/keplugin/KEUploadAttach.servlet" }); Struts2 environment: Configure Struts2 .xml web environment first, as follows: &lt;?xml version="1.0" encoding="UTF-8"?> &lt;web-app version="2.5" xmlns="http://java.sun.com/xml/ns/javaee" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemalocation="http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd"> &lt;filter> &lt;filter-name>struts2&lt;/filter-name> &lt;filter-class>org.apache.struts2.dispatcher.ng.filter.StrutsPrepareAndExecuteFilter&lt;/filter-class> &lt;/filter> &lt;filter-mapping> &lt;filter-name>struts2&lt;/filter-name> &lt;url-pattern>*.action&lt;/url-pattern> &lt;/filter-mapping> &lt;welcome-file-list> &lt;welcome-file>index.jsp&lt;/welcome-file> &lt;/welcome-file-list> &lt;/web-app> Struts .xml file configuration as follows: (If an interceptor is set, please set the interceptor to allow ACTION by type KEStruts2Plugin)
&lt;!--?xml version="1.0" encoding="UTF-8" ?--> &lt;struts> &lt;constant name="struts.i18n.encoding" value="UTF-8">&lt;/constant> &lt;constant name="struts.action.extension" value="action">&lt;/constant> &lt;constant name="struts.configuration.xml.reload" value="true">&lt;/constant> &lt;constant name="struts.multipart.saveDir" value="\temp">&lt;/constant> &lt;constant name="struts.multipart.maxSize" value="104857600">&lt;/constant> &lt;package name="KEPlugin" extends="struts-default" namespace="/keplugin"> &lt;action name="keUploadImg" class="com.elkan.kindeditor.struts2.plugin.KEUploadImgAction"> The default !-- does not compress the picture -- &lt;!--&lt;param name="maxWidth">&lt;/param>--> &lt;!--&lt;param name="maxHeight">&lt;/param>--> The !-- upload image size by default &lt;!--&lt;param name="maxSize">102400&lt;/param>--> The default !-- is jpg, jpeg, png, gif, bmp type picture -- &lt;!--&lt;param name="imgTypes">jpg,jpeg,png,gif,bmp&lt;/param>--> &lt;param name="imgSavePath">/upload/image/&lt;/param> &lt;/action> &lt;action name="keUploadAttach" class="com.elkan.kindeditor.struts2.plugin.KEUploadAttachAction"> &lt;param name="attachSavePath">/upload/attach/&lt;/param> The default !-- upload attachment size -- &lt;!--&lt;param name="maxSize">10485760&lt;/param>--> The default !-- upload attachment type -- &lt;!--&lt;param name="attachTypes">*.*&lt;/param>--> &lt;/action> &lt;action name="keManagerImages" class="com.elkan.kindeditor.struts2.plugin.KEManageImgAction"> &lt;param name="imgSavePath">/upload/image/&lt;/param> &lt;/action> &lt;/package> &lt;/struts> KindEditor JS script configuration on the Jsp page .Struts2 version:
KE.show({ id: "editorStruts2", resizeMode: 0, allowFileManager : true, imageUploadJson: "/KEPlugin/keplugin/keUploadImg.action", fileManagerJson : "/KEPlugin/keplugin/keManagerImages.action", The default is . . . for all types of files //accessoryTypes: "doc|docx", accessoryUploadJson: "/KEPlugin/keplugin/keUploadAttach.action" }); This time also written with the JQuery EasyUI and Syntax Highlighter Grammar Highlight plug-ins, let&rsquo;s start by previewing Kindeditor&rsquo;s refreshing figure in EasyUI mode and shining on the scene&hellip;
:: Apply the sample home page
:: Servlet version of KE
:: Struts2 version of KE
:: Attachment upload page
:: On The picture management
:: KE editor preview effect
For more detailed application features, please see the attachment for download. Finally remember to support the development of domestic technology ah, have an opinion please shoot brick spit slot.
PS: Download the KindEditor app sample download (http://download.csdn.net/download/lisenhui_19/3689869).
Unzip the downloaded package into the Webapps directory of the Tomcat server, launch the Tomcat server, open the browser and enter it in the address bar: http://localhost:portnumber/KEPlugin/index.jspyou can see the application example above, Congratulation!</content></entry><entry><title>JSP's complete KindEditor online editor</title><url>https://lisenhui.cn/en/2011/03/24/kindeditor-jsp-complete.html</url><categories><category>KindEditor</category></categories><tags><tag>KindEditor</tag><tag>Front</tag></tags><content type="html"> I&rsquo;ve been using FCKEditor online editors before, and of course I&rsquo;ve used other online editors like eWebEditor, TinyMCE, CuteEditor,jHtmlArea, etc.,but they weren&rsquo;t used when the final project was released because they were either dull, too cumbersome to configure, too few features, poor browser compatibility, and so on. Last year, a chance chance gave me an opportunity to get to know kindEditor, an online editor that, as its name says, is a friendly editor that is not only simple in size, but also quite impressive in its function and skin. There is also a very important factor, it is our people&rsquo;s development of free tools, from the product release to date the pace of renewal has not stopped Oh. Here&rsquo;s a look at my perfect KindEditor.
At present, the official website has updated the KindEditor to version 3.5.2, from the 3.4 version of the official removed some of the less commonly used features to use the plugin form to enrich the KindEidtor, which providesthe basis for us to createa personality plug-in. In fact, as long as your JS foundation is solid enough, take a look at kindEditor&rsquo;s source code, you can fully improve the original basis of your desired functionality. Here&rsquo;s my perfect record:
A collection of dates, times, online previews and special character plug-ins, using 3.0 skins;
Rewrite the JSP page of image upload and management into SERVLET, and remove the JSON package;
Add the picture compression function to compress the excess width into the specified value;
Add upload attachments;
Add pictures, attachments by date folder classification management function;
Add the title attribute of uploaded images and attachments, which is the original file name by default;
Add the initial properties associated with uploading attachments
Modify the paste style from word to reduce the style.
I won&rsquo;t say much about how to use it, there are detailed APIs on the official website, and at the end of the article I&rsquo;ll give the perfect KindEditor and Demo to see the results first.
:: A well-developed KE catalog
:: Full function display
:: Browse the service catalog
The attachment shows the effect
:: Integration effect with Extjs
Finally, I would like to say that this editor is really very good, I believe you will like it after using it, ha ha, more support for the development of domestic software business.
PS: (http://dl.iteye.com/topics/download/d51d975a-6003-385b-921b-22c05ed3bad6)</content></entry><entry><title>Struts2 and jQuery implementation of non - refresh paging problem</title><url>https://lisenhui.cn/en/2010/11/03/jquery-ajax-struts2.html</url><categories><category>Front</category></categories><tags><tag>Juqery</tag><tag>Front</tag><tag>Struts</tag></tags><content type="html"> Recently, I am doing a non-refresh website management background, and it as my graduation design theme, but in the code implementation encountered a little problem, I would like to ask you a two. My design idea is roughly like this: output the generated data in the background in JSON format, and write the transmitted data in the foreground with the help of jQuery&rsquo;s Ajax function. This way in the realization of data add, change, delete function will not be difficult, but in the data query will be troublesome, how to achieve the data without refreshing paging? I checked what some people did online, but generally found that their code was a bit cumbersome and not in line with my original design. By looking at the JQUERY API I came up with a feasible solution (now realize some function) : in the query page countless according to first create a table style, through the JQUERY CLONE can work when querying data copy this form style at the same time will pass the data fill the background and remove the line of countless according to the style of the form.
The JS related code of the foreground is as follows:
// Displays the data being queried function dataSource(){ $.ajax({ url:"${pageContext.request.contextPath}/jsonservlet", type:"post", data:{}, dataType:"json", error:function(){alert("Server communication failure, please refresh the page later. ^_^");}, success:function(data){ insertTr(data); } }); } // Paging jumps to query data function goPage(thePage){ $.ajax({ url:"${pageContext.request.contextPath}/jsonservlet", type:"post", data:{page:thePage}, dataType:"json", error:function(){alert("Server communication failure, please refresh the page later. ^_^");}, success:function(data){ insertTr(data); } }); } // Populate each row in the table function insertTr(data) { //Read the number in tr var r = $("#datasource tr").size(); var list = data.dataSource; $.each(list, function(i, r) { //Clones existing table styles and attributes var row = $("#source").clone(); //Fill the table with values row.find("#id").text(r.id) row.find("#name").text(r.name); row.find("#time").text(r.time); //Add this row to the table row.appendTo("#datasource"); }); // Remove the first row, because it has only styles and no data $("#datasource").children("tr:first").remove(); } In practice, this scheme is found to be feasible, but one problem arises: how to remove the current data and populate the page with new data when the data is paged?I&rsquo;ve tried a lot of things but I still can&rsquo;t get the non-refresh paging effect I want. I hope you can help me see what&rsquo;s wrong with it.Thanks.
PS: (finally solved by itself, the solution is as follows)
var r = $("#datasource tr").size(); Just add the following code after the code above:
if(r > 1){ $("#datasource").find("tr:not(:first)").remove(); } &ldquo;Download Code&rdquo;</content></entry><entry><title>JQuery+ strusts1.x implements Ajax login without refresh</title><url>https://lisenhui.cn/en/2010/09/05/jquery-ajax-struts1.html</url><categories><category>Front</category></categories><tags><tag>Juqery</tag><tag>Front</tag><tag>Struts</tag></tags><content type="html"> In today&rsquo;s increasingly mature technology development, people not only pursue technological innovation and development, but also pay more attention to the convenience of interaction with users.When programmers are still confused about the issue of data refresh before and after interaction, Ajax came out, it won the majority of programmers with the advantages of convenience and quickness.After a few years of development, it has gradually become an essential tool in our development, I will talk about a Struts1 &lsquo;+&rsquo; Ajax login example.
JS plugin used: jQuery 1.3.2 Chinese version, jQuery. Form2.43
Let&rsquo;s take a look at the core JS code in the page:
function submitForm() { // Use jQuery. Form plugin to serialize the form data var form = $("form[name=AdminLoginForm]"); // Configure the parameters of ajaxForm in jQuery.form // success Callback function when the operation succeeds // resetForm Refresh the form or not // dataType The type of data returned by the receiving server, including script, XML, JSON, etc var options = { success: showResponse, resetForm: false, dataType: "script" }; // ajax Send the form data to the server form.ajaxForm(options); return false; } //callback function function showResponse(responseText, statusText) { if (statusText == "success") { alert(responseText); } else { alert("Due to communication problems, please log in later！"); } } In the above code, we can see that using jQuery and jQuery.form, we can interact with the background data in just three lines of code. JQuery is a very powerful plugin for JS. I like it very much. It is easy to call and has a good code style. You can study it when you are free.
Let&rsquo;s move on to the Struts Action code:
public ActionForward execute(ActionMapping mapping, ActionForm form, HttpServletRequest request, HttpServletResponse response) throws Exception { // Output mode and encoding format response.setContentType("text/html; charset=utf-8"); PrintWriter out = response.getWriter(); // Get form data AdminLoginForm adminLogin = (AdminLoginForm) form; // Gets the captcha generated by the server String validateCode = request.getSession().getAttribute("validateCode").toString(); try { // Determine whether the verification code entered by the user is correct if (adminLogin.getVerifycode().equalsIgnoreCase(validateCode)) { // The state of the user name boolean isUser = false; // Verify that the user name exists if(!adminLogin.getUsername().equalsIgnoreCase("elkan")){ out.print("The user name you entered does not exist, please reenter!"); return null; }else{ isUser = true; } // Verify that the password is valid if(adminLogin.getUserpswd().equalsIgnoreCase("lisenhui2010") &amp;&amp; isUser){ out.print("Login successful!"); }else{ out.print("Wrong password, please re-enter!"); return null; } } else { out.print("Incorrect verification code, please reenter!"); return null; } } catch (Exception e) { out.print(e.toString()); } return null; } Finally, there is the following struts-config.xml configuration file:
&lt;action-mappings> &lt;action input="/webstage/adminLogin.jsp" name="AdminLoginForm" path="/AdminLogin" scope="request" type="com.elkan.struts.actions.AdminLogin" validate="false"/> &lt;/action-mappings> Having said that, let&rsquo;s take a look at the above code to achieve the effect：
See the above effect, do you also want to show their skills, then hurry to do it, I believe that with the above code tips to do a DEMO should not be very difficult, if there is any problem you can leave a message to me. Thanks for your support.</content></entry></search>