<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Big data on Loving life and dreams.</title><link>https://lisenhui.cn/en/tags/big-data/</link><description>Recent content in Big data on Loving life and dreams.</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Mon, 28 Sep 2020 17:02:33 +0000</lastBuildDate><atom:link href="https://lisenhui.cn/en/tags/big-data/index.xml" rel="self" type="application/rss+xml"/><item><title>Several pit records during CDH6 installation</title><link>https://lisenhui.cn/en/2020/09/28/install-cdh-issues-notes.html</link><pubDate>Mon, 28 Sep 2020 17:02:33 +0000</pubDate><guid>https://lisenhui.cn/en/2020/09/28/install-cdh-issues-notes.html</guid><description>&lt;p>In fact, CDH environment deployment and installation is not difficult, is the so-called mature can make a coincidence. But it just doesn&amp;rsquo;t happen that it&amp;rsquo;s been too long to operate, that is, there will be some &amp;ldquo;strange&amp;rdquo; problems, and then take some effort to solve the problem, and then record them down the road, to avoid recides later.&lt;/p></description></item><item><title>Temporary files cannot be created on the HDFS Data node</title><link>https://lisenhui.cn/en/2019/03/21/unable-create-tmp-file-in-hdfs-nodes.html</link><pubDate>Thu, 21 Mar 2019 19:04:51 +0000</pubDate><guid>https://lisenhui.cn/en/2019/03/21/unable-create-tmp-file-in-hdfs-nodes.html</guid><description>&lt;p>On the newly created &amp;lsquo;Hadoop&amp;rsquo; edge node, an attempt was made to insert data through the &amp;lsquo;Hive CLI&amp;rsquo; mode, resulting in no intended success, but instead an exception was caught as follows:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-java" data-lang="java">&lt;span style="color:#a0a000">FAILED:&lt;/span> SemanticException &lt;span style="color:#666">[&lt;/span>Error 10293&lt;span style="color:#666">]:&lt;/span> Unable to create temp file &lt;span style="color:#a2f;font-weight:bold">for&lt;/span> insert values File &lt;span style="color:#666">/&lt;/span>tmp&lt;span style="color:#666">/&lt;/span>hive&lt;span style="color:#666">/&lt;/span>kylin&lt;span style="color:#666">/&lt;/span>9c84de0a&lt;span style="color:#666">-&lt;/span>fca2&lt;span style="color:#666">-&lt;/span>4d3c&lt;span style="color:#666">-&lt;/span>8f72&lt;span style="color:#666">-&lt;/span>47436a4adb83&lt;span style="color:#666">/&lt;/span>_tmp_space&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">db&lt;/span>&lt;span style="color:#666">/&lt;/span>Values__Tmp__Table__1&lt;span style="color:#666">/&lt;/span>data_file could only be replicated to 0 nodes instead of &lt;span style="color:#00a000">minReplication&lt;/span> &lt;span style="color:#666">(=&lt;/span>1&lt;span style="color:#666">).&lt;/span> There are 1 &lt;span style="color:#00a000">datanode&lt;/span>&lt;span style="color:#666">(&lt;/span>s&lt;span style="color:#666">)&lt;/span> running and 1 &lt;span style="color:#00a000">node&lt;/span>&lt;span style="color:#666">(&lt;/span>s&lt;span style="color:#666">)&lt;/span> are excluded in &lt;span style="color:#a2f;font-weight:bold">this&lt;/span> operation&lt;span style="color:#666">.&lt;/span>
at org&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">apache&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">hadoop&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">hdfs&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">server&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">blockmanagement&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">BlockManager&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">chooseTarget4NewBlock&lt;/span>&lt;span style="color:#666">(&lt;/span>BlockManager&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">java&lt;/span>&lt;span style="color:#666">:&lt;/span>1720&lt;span style="color:#666">)&lt;/span>
at org&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">apache&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">hadoop&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">hdfs&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">server&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">namenode&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">FSNamesystem&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">getAdditionalBlock&lt;/span>&lt;span style="color:#666">(&lt;/span>FSNamesystem&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">java&lt;/span>&lt;span style="color:#666">:&lt;/span>3440&lt;span style="color:#666">)&lt;/span>
at org&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">apache&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">hadoop&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">hdfs&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">server&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">namenode&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">NameNodeRpcServer&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">addBlock&lt;/span>&lt;span style="color:#666">(&lt;/span>NameNodeRpcServer&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">java&lt;/span>&lt;span style="color:#666">:&lt;/span>686&lt;span style="color:#666">)&lt;/span>
at org&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">apache&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">hadoop&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">hdfs&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">server&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">namenode&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">AuthorizationProviderProxyClientProtocol&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">addBlock&lt;/span>&lt;span style="color:#666">(&lt;/span>AuthorizationProviderProxyClientProtocol&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">java&lt;/span>&lt;span style="color:#666">:&lt;/span>217&lt;span style="color:#666">)&lt;/span>
at org&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">apache&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">hadoop&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">hdfs&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">protocolPB&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">ClientNamenodeProtocolServerSideTranslatorPB&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">addBlock&lt;/span>&lt;span style="color:#666">(&lt;/span>ClientNamenodeProtocolServerSideTranslatorPB&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">java&lt;/span>&lt;span style="color:#666">:&lt;/span>506&lt;span style="color:#666">)&lt;/span>
at org&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">apache&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">hadoop&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">hdfs&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">protocol&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">proto&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">callBlockingMethod&lt;/span>&lt;span style="color:#666">(&lt;/span>ClientNamenodeProtocolProtos&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">java&lt;/span>&lt;span style="color:#666">)&lt;/span>
at org&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">apache&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">hadoop&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">ipc&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">ProtobufRpcEngine$Server$ProtoBufRpcInvoker&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">call&lt;/span>&lt;span style="color:#666">(&lt;/span>ProtobufRpcEngine&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">java&lt;/span>&lt;span style="color:#666">:&lt;/span>617&lt;span style="color:#666">)&lt;/span>
at org&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">apache&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">hadoop&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">ipc&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">RPC$Server&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">call&lt;/span>&lt;span style="color:#666">(&lt;/span>RPC&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">java&lt;/span>&lt;span style="color:#666">:&lt;/span>1073&lt;span style="color:#666">)&lt;/span>
at org&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">apache&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">hadoop&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">ipc&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">Server$Handler$1&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">run&lt;/span>&lt;span style="color:#666">(&lt;/span>Server&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">java&lt;/span>&lt;span style="color:#666">:&lt;/span>2226&lt;span style="color:#666">)&lt;/span>
at org&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">apache&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">hadoop&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">ipc&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">Server$Handler$1&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">run&lt;/span>&lt;span style="color:#666">(&lt;/span>Server&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">java&lt;/span>&lt;span style="color:#666">:&lt;/span>2222&lt;span style="color:#666">)&lt;/span>
at java&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">security&lt;/span>&lt;span style="color:#666">.&lt;/span> AccessController&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">doPrivileged&lt;/span>&lt;span style="color:#666">(&lt;/span>Native Method&lt;span style="color:#666">)&lt;/span>
at javax&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">security&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">auth&lt;/span>&lt;span style="color:#666">.&lt;/span> Subject&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">doAs&lt;/span>&lt;span style="color:#666">(&lt;/span>Subject&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">java&lt;/span>&lt;span style="color:#666">:&lt;/span>415&lt;span style="color:#666">)&lt;/span>
at org&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">apache&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">hadoop&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">security&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">UserGroupInformation&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">doAs&lt;/span>&lt;span style="color:#666">(&lt;/span>UserGroupInformation&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">java&lt;/span>&lt;span style="color:#666">:&lt;/span>1917&lt;span style="color:#666">)&lt;/span>
at org&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">apache&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">hadoop&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">ipc&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">Server$Handler&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">run&lt;/span>&lt;span style="color:#666">(&lt;/span>Server&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">java&lt;/span>&lt;span style="color:#666">:&lt;/span>2220&lt;span style="color:#666">)&lt;/span>
&lt;span style="color:#a0a000">
&lt;/span>&lt;span style="color:#a0a000">ERROR:&lt;/span> Current user has no permission to create Hive table in working directory&lt;span style="color:#666">:&lt;/span> &lt;span style="color:#666">/&lt;/span>user&lt;span style="color:#666">/&lt;/span>kylin
&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Apache Nifi builds pseudo-clusters and certificate logins in a Windows environment</title><link>https://lisenhui.cn/en/2017/10/26/nifi-windows-local-cluster.html</link><pubDate>Thu, 26 Oct 2017 17:50:52 +0000</pubDate><guid>https://lisenhui.cn/en/2017/10/26/nifi-windows-local-cluster.html</guid><description>&lt;p>Some time ago did about the &amp;lsquo;Apache Nifi&amp;rsquo;distributed clusterset-up sharing, but a lot of times to build distributed cluster machine resources is a problem, and now the stand-alone configuration is quite good, so now do a share about building a pseudo-distributed cluster on Windows, and through another way to achieve the authorization of the &amp;ldquo;Apache Nifi.&lt;/p>
&lt;p>The system environment and software version&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Windows8.1&lt;/p>
&lt;/li>
&lt;li>
&lt;p>JDK1.8.0_131&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Nifi-1.4.0&lt;/p>
&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">Nifi installation directory&lt;/th>
&lt;th style="text-align:center">The WEB port&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">xxx\nifi-ncm&lt;/td>
&lt;td style="text-align:center">9443&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">xxx\nifi-cluster01&lt;/td>
&lt;td style="text-align:center">9444&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">xxx\nifi-cluster02&lt;/td>
&lt;td style="text-align:center">9445&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>(Other versions can refer to this article)
Another problem in testing, using&amp;rsquo;Zookeyer&amp;rsquo; embedded in &amp;lsquo;Apache Nifi&amp;rsquo;to build pseudo-clusters, always prompts for port occupancy issues, so give up using only single-node startup.&lt;/p></description></item><item><title>Apache Nifi clustering and user authentication with kerberos</title><link>https://lisenhui.cn/en/2017/10/22/ninfi-cluster-deploy-with-kerberos.html</link><pubDate>Sun, 22 Oct 2017 11:42:29 +0000</pubDate><guid>https://lisenhui.cn/en/2017/10/22/ninfi-cluster-deploy-with-kerberos.html</guid><description>&lt;p>Recently, some of the lessons learned from the installation configuration are being shared with the help of the .Apache NIFI for contact with data streaming processing. This article is primarily about clusters and user rights, and the description of the &amp;ldquo;Apache NIFI&amp;rdquo; (&lt;a href="https://nifi.apache.org/">https://nifi.apache.org/&lt;/a>) is not much of a description, with a direct reference to the official home page as follows:&lt;/p>
&lt;p>&lt;img src="http://siteimgs.lisenhui.cn/2017/10-22-Apache-NiFi-01.png-alias" alt="NiFi-01.png">&lt;/p></description></item><item><title>An issue with the service in Ambari that is functioning properly but the service stops</title><link>https://lisenhui.cn/en/2017/10/18/ambari-monitor-status-issues.html</link><pubDate>Wed, 18 Oct 2017 16:13:36 +0000</pubDate><guid>https://lisenhui.cn/en/2017/10/18/ambari-monitor-status-issues.html</guid><description>&lt;p>Many times the maintenance of the environment is indeed a headache event, which does not originally show the monitoring of normal services on Ambari&amp;rsquo;s Dashboard page, there is a strange phenomenon: in the machine query service running process is normal, but partial Ambari&amp;rsquo;s UI interface is displayed as a stop, but the port check shows normal. Here&amp;rsquo;s the picture:&lt;/p>
&lt;p>&lt;img src="http://siteimgs.lisenhui.cn/2017/10-18-ambari-red.png-alias" alt="alert_stopped.png">&lt;/p></description></item><item><title>Problems with HiveServer2 due to the JDBC version</title><link>https://lisenhui.cn/en/2017/10/17/hive2-jdbc-connector-issues.html</link><pubDate>Tue, 17 Oct 2017 17:33:04 +0000</pubDate><guid>https://lisenhui.cn/en/2017/10/17/hive2-jdbc-connector-issues.html</guid><description>&lt;p>Previously, &amp;lsquo;HDP&amp;rsquo; was used to build and manage the &amp;lsquo;Hadoop&amp;rsquo; environment, and there were no difficult problems when the installation was debugged, but this time it was a strange problem when it was deployed on the &amp;lsquo;Centos6x&amp;rsquo; system:&lt;/p>
&lt;p>On the face of it, the Hive service is functioning normally, the process is running normally, the page UI is normal, and the logs are not output incorrectly. Simple table-building statements can be executed, which can be caused by throwing exceptions when importing local/HDFSdata. The wrong stack information is as follows:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-fallback" data-lang="fallback">com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &amp;#39;OPTION SQL_SELECT_LIMIT=DEFAULT&amp;#39; at line 1
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Another problem is that the HDFS data import prompt file does not exist using the&amp;rsquo;HiveView&amp;rsquo;UI provided by Ambari,with the error message as follows:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-fallback" data-lang="fallback">org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: SemanticException Line 1:17 Invalid path &amp;#39;&amp;#39;/tmp/xxx/xxxxx.csv&amp;#39;&amp;#39;: No files matching path hdfs:/...
&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>The job types and examples supported by Azkaban</title><link>https://lisenhui.cn/en/2017/09/09/azkaban-execute-jobs.html</link><pubDate>Sat, 09 Sep 2017 18:45:42 +0000</pubDate><guid>https://lisenhui.cn/en/2017/09/09/azkaban-execute-jobs.html</guid><description>&lt;p>In the introduction to the official documentation, it is understood that &amp;lsquo;Azkaban&amp;rsquo; supportsa wide range of types of work, such as: &amp;lsquo;Command&amp;rsquo;,&amp;lsquo;HadoopShell&amp;rsquo;,&amp;lsquo;Python&amp;rsquo;, &amp;lsquo;Java&amp;rsquo;, &amp;lsquo;Hive&amp;rsquo;, &amp;lsquo;Pig&amp;rsquo; and so on. However, here we mainly only to explain the &amp;lsquo;Python&amp;rsquo; and &amp;lsquo;Java&amp;rsquo;job type tasks, other types of work, such as&amp;rsquo;Commnad&amp;rsquo;,&amp;lsquo;Hive&amp;rsquo;,&amp;lsquo;HadoopShell&amp;rsquo;relatively simple without explanation, if necessary, you can practice ityourself.&lt;/p>
&lt;p>Regardless of which task is submitted, &amp;lsquo;Azkaban&amp;rsquo; is managed by uploading a compressed package by default, so it is recommended that you get into the habit ofnot packing the executed files (code) into the &amp;lsquo;Azkaban&amp;rsquo; engineering package. The benefits are obvious, such as:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>The project is created so fast that you don&amp;rsquo;t need to pass on some files to execute&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Avoid modifying the &amp;lsquo;I&amp;rsquo;m max_allow_packet&amp;rsquo;parameterin &amp;lsquo;MySQL&amp;rsquo;to resolve the issue of failed transmissions on engineering files&lt;/p>
&lt;/li>
&lt;li>
&lt;p>In a distributed environment, when performing Task eliminates the hassle of copying engineering packages in different nodes&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>Timed scheduling tasker Azkaban installation</title><link>https://lisenhui.cn/en/2017/09/08/azkaban-install-use-share.html</link><pubDate>Fri, 08 Sep 2017 14:29:42 +0000</pubDate><guid>https://lisenhui.cn/en/2017/09/08/azkaban-install-use-share.html</guid><description>&lt;p>Background and introduction&lt;/p>
&lt;p>In the big data complex ETL or other data processing processes, some tasks need to be performed on a timed basis, although Linux brings itsown&amp;rsquo;cron' commandfunction, but still can not meet the biggest point is that it does notprovide centralized management and visual editing. In fact, in the big data ecology has been integrated with a timed scheduling framework &amp;lsquo;Oozie&amp;rsquo;, but the practice found that its learning costs are not low, the process of distribution is more complex. After trying other distribution worker scheduling frameworks, such as Ali&amp;rsquo;s Zeus &amp;lsquo;Zeus&amp;rsquo;, or &amp;lsquo;Azkaban&amp;rsquo;, which is used by more people in the community.&lt;/p>
&lt;p>&amp;lsquo;Azkaban3&amp;rsquo; is still relatively large compared to the changes made in the previous version, and interested parties can be found on its official website, &lt;a href="https://azkaban.github.io/azkaban/docs/latest/">Azkaban&lt;/a>. Next, mainly to share the installation of &amp;lsquo;Azkaban 3&amp;rsquo;, the following is &amp;lsquo;Azkaban 3&amp;rsquo; system architecture design:&lt;/p>
&lt;p>&lt;img src="http://siteimgs.lisenhui.cn/2017/09-08-Azkaban-Install00.png-alias" alt="Azkaban-Install00">&lt;/p>
&lt;p>The three components in the figure are an important part of &amp;lsquo;Azkaban3&amp;rsquo;:&lt;/p>
&lt;ul>
&lt;li>MySQL relationship data storage data&lt;/li>
&lt;li>Web Server GUI management service provider&lt;/li>
&lt;li>Executor Server Distributed Node Service&lt;/li>
&lt;/ul></description></item><item><title>The integrated MySQL data in Hue shows garbled code</title><link>https://lisenhui.cn/en/2017/08/15/hue-rdbms-mysql-chinese.html</link><pubDate>Tue, 15 Aug 2017 15:13:39 +0000</pubDate><guid>https://lisenhui.cn/en/2017/08/15/hue-rdbms-mysql-chinese.html</guid><description>&lt;p>Hue is a Web applications that enables you to easily interact with an Hadoop cluster. Hue applications let you browse HDFS, Jobs, run Hive, Pig and Cloudera Impala queries, manage the Hive Metastore, HBase, Sqoop, ZooKeeper, MapReduce jobs, and create and schedule worklows with Oozie.&lt;/p>
&lt;p>For more information and presentations on HUE,visit its officialwebsite: &lt;a href="http://gethue.com">http://gethue.com&lt;/a>.&lt;/p></description></item><item><title>Kylin integrates Zeppelin presentation data</title><link>https://lisenhui.cn/en/2017/06/02/kylin-integrate-with-zeppelin.html</link><pubDate>Fri, 02 Jun 2017 18:03:23 +0000</pubDate><guid>https://lisenhui.cn/en/2017/06/02/kylin-integrate-with-zeppelin.html</guid><description>&lt;p>In fact, kylin&amp;rsquo;s own WEB UI has integrated the recommended graphical reports, with common line, column, and pie charts, which are perfectly sufficient for the initial presentation of the data. If you want a richer presentation, consider using other tools, and try the officially recommended Apache Zeppelin now.&lt;/p></description></item><item><title>The Sqoop tool imports data into the Hive note</title><link>https://lisenhui.cn/en/2017/05/24/sqoop-import-data-to-hive.html</link><pubDate>Wed, 24 May 2017 20:18:53 +0000</pubDate><guid>https://lisenhui.cn/en/2017/05/24/sqoop-import-data-to-hive.html</guid><description>&lt;p>Recently, the construction of a data warehouse is being messed up, just some of the dimension table data needs to come from RDBMS data, in the HADOOP environment is the most popular than Apache&amp;rsquo;s Sqoop tool, according to the official documentation operation is also smooth, but when to apply to the business scenario when the problem arises.&lt;/p></description></item><item><title>Zookeyer could not load transaction logs after crash</title><link>https://lisenhui.cn/en/2017/05/15/zookeeper-unload-data-exception.html</link><pubDate>Mon, 15 May 2017 12:34:21 +0000</pubDate><guid>https://lisenhui.cn/en/2017/05/15/zookeeper-unload-data-exception.html</guid><description>&lt;p>Today in a production HDP environment, encountered a very strange thing. Mingming set up 2 zookeyer cluster, but it is inexplicable missing, and HDP services are not reported wrong, carefully check the environment or did not find abnormal information, really do not understand.&lt;/p>
&lt;p>Let&amp;rsquo;s put it this way: The production environment zookeyer crashes and the log finds that disk space is full. At first thought it was a very simple operation, delete the useless log files to free up disk space (this is having to spit under the SLOT HDP log files are super, helpless production environment and dare not set aside a longer time), and then restart zookeyer happy waiting for the service to return to normal. This time, however, there was no hint of success, and the unusually constant service connections to zookeyer failed. At this time is really depressed, space is clearly already sufficient. The exception information is as follows:&lt;/p></description></item><item><title>Install HDP2.6 (1)-Ambari Server offline</title><link>https://lisenhui.cn/en/2017/04/17/offline-install-hdp-ambari-notes.html</link><pubDate>Mon, 17 Apr 2017 19:52:31 +0000</pubDate><guid>https://lisenhui.cn/en/2017/04/17/offline-install-hdp-ambari-notes.html</guid><description>&lt;ol>
&lt;li>Reference documentation&lt;/li>
&lt;/ol>
&lt;p>FYI: &lt;a href="https://docs.hortonworks.com/HDPDocuments/Ambari-2.5.0.3/bk_ambari-installation/content/ch_Getting_Ready.html">HDP Install Documents&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.0/bk_command-line-installation/content/prepare-environment.html#ref-2822d0e9-bd88-4714-910a-750c5b95a996">HDP Install Manual&lt;/a>&lt;/p></description></item></channel></rss>