<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>大数据 on 热爱生活与梦想</title><link>https://lisenhui.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/</link><description>Recent content in 大数据 on 热爱生活与梦想</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Mon, 28 Sep 2020 17:02:33 +0000</lastBuildDate><atom:link href="https://lisenhui.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/index.xml" rel="self" type="application/rss+xml"/><item><title>安装CDH6过程中几个入坑记录</title><link>https://lisenhui.cn/2020/09/28/install-cdh-issues-notes.html</link><pubDate>Mon, 28 Sep 2020 17:02:33 +0000</pubDate><guid>https://lisenhui.cn/2020/09/28/install-cdh-issues-notes.html</guid><description>&lt;p>其实CDH环境部署安装并非是什么难事，正所谓是熟能生巧嘛。但正好不巧的就是太久没有操作过，便是会遇到一些“奇奇怪怪”的问题，而后花费些功夫才能解决好，事后也就顺道把它们记录下来，避免以后再犯。&lt;/p></description></item><item><title>不能在HDFS Data节点上创建临时文件</title><link>https://lisenhui.cn/2019/03/21/unable-create-tmp-file-in-hdfs-nodes.html</link><pubDate>Thu, 21 Mar 2019 19:04:51 +0000</pubDate><guid>https://lisenhui.cn/2019/03/21/unable-create-tmp-file-in-hdfs-nodes.html</guid><description>&lt;p>在新创建的&lt;code>Hadoop&lt;/code>边缘节点上，尝试通过&lt;code>Hive CLI&lt;/code>模式进行数据插入操作，结果没有出现意想中的成功信息，反倒是捕获到如下的异常：&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-java" data-lang="java">&lt;span style="color:#a0a000">FAILED:&lt;/span> SemanticException &lt;span style="color:#666">[&lt;/span>Error 10293&lt;span style="color:#666">]:&lt;/span> Unable to create temp file &lt;span style="color:#a2f;font-weight:bold">for&lt;/span> insert values File &lt;span style="color:#666">/&lt;/span>tmp&lt;span style="color:#666">/&lt;/span>hive&lt;span style="color:#666">/&lt;/span>kylin&lt;span style="color:#666">/&lt;/span>9c84de0a&lt;span style="color:#666">-&lt;/span>fca2&lt;span style="color:#666">-&lt;/span>4d3c&lt;span style="color:#666">-&lt;/span>8f72&lt;span style="color:#666">-&lt;/span>47436a4adb83&lt;span style="color:#666">/&lt;/span>_tmp_space&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">db&lt;/span>&lt;span style="color:#666">/&lt;/span>Values__Tmp__Table__1&lt;span style="color:#666">/&lt;/span>data_file could only be replicated to 0 nodes instead of &lt;span style="color:#00a000">minReplication&lt;/span> &lt;span style="color:#666">(=&lt;/span>1&lt;span style="color:#666">).&lt;/span> There are 1 &lt;span style="color:#00a000">datanode&lt;/span>&lt;span style="color:#666">(&lt;/span>s&lt;span style="color:#666">)&lt;/span> running and 1 &lt;span style="color:#00a000">node&lt;/span>&lt;span style="color:#666">(&lt;/span>s&lt;span style="color:#666">)&lt;/span> are excluded in &lt;span style="color:#a2f;font-weight:bold">this&lt;/span> operation&lt;span style="color:#666">.&lt;/span>
at org&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">apache&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">hadoop&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">hdfs&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">server&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">blockmanagement&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">BlockManager&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">chooseTarget4NewBlock&lt;/span>&lt;span style="color:#666">(&lt;/span>BlockManager&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">java&lt;/span>&lt;span style="color:#666">:&lt;/span>1720&lt;span style="color:#666">)&lt;/span>
at org&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">apache&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">hadoop&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">hdfs&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">server&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">namenode&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">FSNamesystem&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">getAdditionalBlock&lt;/span>&lt;span style="color:#666">(&lt;/span>FSNamesystem&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">java&lt;/span>&lt;span style="color:#666">:&lt;/span>3440&lt;span style="color:#666">)&lt;/span>
at org&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">apache&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">hadoop&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">hdfs&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">server&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">namenode&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">NameNodeRpcServer&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">addBlock&lt;/span>&lt;span style="color:#666">(&lt;/span>NameNodeRpcServer&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">java&lt;/span>&lt;span style="color:#666">:&lt;/span>686&lt;span style="color:#666">)&lt;/span>
at org&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">apache&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">hadoop&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">hdfs&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">server&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">namenode&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">AuthorizationProviderProxyClientProtocol&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">addBlock&lt;/span>&lt;span style="color:#666">(&lt;/span>AuthorizationProviderProxyClientProtocol&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">java&lt;/span>&lt;span style="color:#666">:&lt;/span>217&lt;span style="color:#666">)&lt;/span>
at org&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">apache&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">hadoop&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">hdfs&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">protocolPB&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">ClientNamenodeProtocolServerSideTranslatorPB&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">addBlock&lt;/span>&lt;span style="color:#666">(&lt;/span>ClientNamenodeProtocolServerSideTranslatorPB&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">java&lt;/span>&lt;span style="color:#666">:&lt;/span>506&lt;span style="color:#666">)&lt;/span>
at org&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">apache&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">hadoop&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">hdfs&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">protocol&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">proto&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">callBlockingMethod&lt;/span>&lt;span style="color:#666">(&lt;/span>ClientNamenodeProtocolProtos&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">java&lt;/span>&lt;span style="color:#666">)&lt;/span>
at org&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">apache&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">hadoop&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">ipc&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">ProtobufRpcEngine$Server$ProtoBufRpcInvoker&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">call&lt;/span>&lt;span style="color:#666">(&lt;/span>ProtobufRpcEngine&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">java&lt;/span>&lt;span style="color:#666">:&lt;/span>617&lt;span style="color:#666">)&lt;/span>
at org&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">apache&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">hadoop&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">ipc&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">RPC$Server&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">call&lt;/span>&lt;span style="color:#666">(&lt;/span>RPC&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">java&lt;/span>&lt;span style="color:#666">:&lt;/span>1073&lt;span style="color:#666">)&lt;/span>
at org&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">apache&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">hadoop&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">ipc&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">Server$Handler$1&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">run&lt;/span>&lt;span style="color:#666">(&lt;/span>Server&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">java&lt;/span>&lt;span style="color:#666">:&lt;/span>2226&lt;span style="color:#666">)&lt;/span>
at org&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">apache&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">hadoop&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">ipc&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">Server$Handler$1&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">run&lt;/span>&lt;span style="color:#666">(&lt;/span>Server&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">java&lt;/span>&lt;span style="color:#666">:&lt;/span>2222&lt;span style="color:#666">)&lt;/span>
at java&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">security&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">AccessController&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">doPrivileged&lt;/span>&lt;span style="color:#666">(&lt;/span>Native Method&lt;span style="color:#666">)&lt;/span>
at javax&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">security&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">auth&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">Subject&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">doAs&lt;/span>&lt;span style="color:#666">(&lt;/span>Subject&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">java&lt;/span>&lt;span style="color:#666">:&lt;/span>415&lt;span style="color:#666">)&lt;/span>
at org&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">apache&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">hadoop&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">security&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">UserGroupInformation&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">doAs&lt;/span>&lt;span style="color:#666">(&lt;/span>UserGroupInformation&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">java&lt;/span>&lt;span style="color:#666">:&lt;/span>1917&lt;span style="color:#666">)&lt;/span>
at org&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">apache&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">hadoop&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">ipc&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">Server$Handler&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">run&lt;/span>&lt;span style="color:#666">(&lt;/span>Server&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">java&lt;/span>&lt;span style="color:#666">:&lt;/span>2220&lt;span style="color:#666">)&lt;/span>
&lt;span style="color:#a0a000">
&lt;/span>&lt;span style="color:#a0a000">ERROR:&lt;/span> Current user has no permission to create Hive table in working directory&lt;span style="color:#666">:&lt;/span> &lt;span style="color:#666">/&lt;/span>user&lt;span style="color:#666">/&lt;/span>kylin
&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Apache Nifi在Windows环境下搭建伪群集及证书登录</title><link>https://lisenhui.cn/2017/10/26/nifi-windows-local-cluster.html</link><pubDate>Thu, 26 Oct 2017 17:50:52 +0000</pubDate><guid>https://lisenhui.cn/2017/10/26/nifi-windows-local-cluster.html</guid><description>&lt;p>前些时间做了关于&lt;code>Apache Nifi&lt;/code>分布式集群的搭建分享，但很多时候要搭建分布式集群机器资源是个问题，而现在的单机的配置还是相当不错的，故现在就做个关于Windows上搭建个伪分布式集群的分享，同时通过另外一种方式实现&lt;strong>Apache Nifi&lt;/strong>的授权认证。&lt;/p>
&lt;h1 id="系统环境及软件版本">系统环境及软件版本&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>Windows8.1&lt;/p>
&lt;/li>
&lt;li>
&lt;p>JDK1.8.0_131&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Nifi-1.4.0&lt;/p>
&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">Nifi安装目录&lt;/th>
&lt;th style="text-align:center">WEB端口&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">xxx\nifi-ncm&lt;/td>
&lt;td style="text-align:center">9443&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">xxx\nifi-cluster01&lt;/td>
&lt;td style="text-align:center">9444&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">xxx\nifi-cluster02&lt;/td>
&lt;td style="text-align:center">9445&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;blockquote>
&lt;p>(其它版本可参考此篇文章)
另在测试中发个问题，使用&lt;code>Apache Nifi&lt;/code>内嵌的&lt;code>Zookeeper&lt;/code>搭建伪集群里启动总是提示端口占用的问题，故放弃只采用了单结点启动。&lt;/p>
&lt;/blockquote></description></item><item><title>Apache Nifi集群搭建及用kerberos实现用户认证</title><link>https://lisenhui.cn/2017/10/22/ninfi-cluster-deploy-with-kerberos.html</link><pubDate>Sun, 22 Oct 2017 11:42:29 +0000</pubDate><guid>https://lisenhui.cn/2017/10/22/ninfi-cluster-deploy-with-kerberos.html</guid><description>&lt;p>最近这段时间在接触数据流式处理方面的事宜，用到了&lt;strong>Apache NIFI&lt;/strong>现把安装配置中学习的一些经验分享下。此篇文章主要是针对集群及用户权限方面，关于&lt;a href="https://nifi.apache.org/">Apache NIFI&lt;/a>的介绍就不做过多的说明，直接引用官方的首页的说明如下图所示：&lt;/p>
&lt;p>&lt;img src="http://myblog.lisenhui.cn/2017/10-22-Apache-NiFi-01.png-alias" alt="NiFi-01.png">&lt;/p></description></item><item><title>关于Ambari中服务运行正常UI却显示服务停止的问题</title><link>https://lisenhui.cn/2017/10/18/ambari-monitor-status-issues.html</link><pubDate>Wed, 18 Oct 2017 16:13:36 +0000</pubDate><guid>https://lisenhui.cn/2017/10/18/ambari-monitor-status-issues.html</guid><description>&lt;p>很多时候环境的维护的确是件头痛的事件，这不本来在Ambari的Dashboard页面显示正常服务的监控，实然间出现了个奇怪的现象： 在机器查询服务的运行进程是正常的，可偏偏Ambari的UI界面却显示状为停止，但端口检查又显示正常的。如下图：&lt;/p>
&lt;p>&lt;img src="http://myblog.lisenhui.cn/2017/10-18-ambari-red.png-alias" alt="alert_stopped.png">&lt;/p></description></item><item><title>HiveServer2因JDBC版本引起的问题</title><link>https://lisenhui.cn/2017/10/17/hive2-jdbc-connector-issues.html</link><pubDate>Tue, 17 Oct 2017 17:33:04 +0000</pubDate><guid>https://lisenhui.cn/2017/10/17/hive2-jdbc-connector-issues.html</guid><description>&lt;p>之前一直都是用&lt;code>HDP&lt;/code>来搭建和管理&lt;strong>Hadoop&lt;/strong>环境，在安装完成调试时也未曾出现过棘手的问题，但这次在&lt;code>Centos6x&lt;/code>系统上布署好后却是遇到奇怪的问题：&lt;/p>
&lt;blockquote>
&lt;p>表面上看来&lt;strong>Hive&lt;/strong>服务是正常运行的，进程运行正常，页面UI也正常，日志也没错误输出。简单的建表的语句都能执行，可偏偏在导入本地/&lt;strong>HDFS&lt;/strong>数据时，便就抛出异常啦。错误的堆栈信息如下：&lt;/p>
&lt;/blockquote>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-fallback" data-lang="fallback">com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &amp;#39;OPTION SQL_SELECT_LIMIT=DEFAULT&amp;#39; at line 1
&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>另外一个问题在使用&lt;strong>Ambari&lt;/strong>提供的&lt;code>HiveView&lt;/code> UI进行HDFS数据导入提示文件不存在，错误信息如下：&lt;/p>
&lt;/blockquote>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-fallback" data-lang="fallback">org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: SemanticException Line 1:17 Invalid path &amp;#39;&amp;#39;/tmp/xxx/xxxxx.csv&amp;#39;&amp;#39;: No files matching path hdfs:/...
&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Azkaban所支持的Job类型及示例</title><link>https://lisenhui.cn/2017/09/09/azkaban-execute-jobs.html</link><pubDate>Sat, 09 Sep 2017 18:45:42 +0000</pubDate><guid>https://lisenhui.cn/2017/09/09/azkaban-execute-jobs.html</guid><description>&lt;p>在官方文档的介绍中，了解到&lt;code>Azkaban&lt;/code>所支持的工作类型还是很丰富的，如：&lt;code>Command&lt;/code>，&lt;code>HadoopShell&lt;/code>，&lt;code>Python&lt;/code>，&lt;code>Java&lt;/code>，&lt;code>Hive&lt;/code>，&lt;code>Pig&lt;/code>等等。不过在此我们主要具体只来讲解下&lt;code>Python&lt;/code>与&lt;code>Java&lt;/code>的工作类型任务，其它工作类型的话，比如&lt;code>Commnad&lt;/code>，&lt;code>Hive&lt;/code>，&lt;code>HadoopShell&lt;/code>相对比较简单就不做详解，有需要的话可以自行实践一下。&lt;/p>
&lt;p>不管提交哪一种任务，&lt;code>Azkaban&lt;/code>默认都是通过上传压缩包来管理，那么在此建议大家养成一个习惯，不要所执行的文件(代码)打包到&lt;code>Azkaban&lt;/code>的工程包里面。这样带来的好处是显而易见的，比如：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>工程创建的速度快，不需要上传执行部分文件&lt;/p>
&lt;/li>
&lt;li>
&lt;p>避免了修改&lt;code>MySQL&lt;/code>中的&lt;code>max_allow_packet&lt;/code>参数以解决工程文件上传失败的问题&lt;/p>
&lt;/li>
&lt;li>
&lt;p>在分布式布署环境中，当执行Task免去了在不同节点中拷贝工程包的麻烦&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>定时调度任务器Azkaban安装</title><link>https://lisenhui.cn/2017/09/08/azkaban-install-use-share.html</link><pubDate>Fri, 08 Sep 2017 14:29:42 +0000</pubDate><guid>https://lisenhui.cn/2017/09/08/azkaban-install-use-share.html</guid><description>&lt;h1 id="背景与介绍">背景与介绍&lt;/h1>
&lt;p>在大数据繁杂的ETL或其它数据处理过程当中，有些任务是需要定时执行的，虽然Linux自带了&lt;code>cron&lt;/code>命令功能，但是仍不能满足最大的一点就是它不能提供集中式的管理和可视化的编辑。其实在大数据的生态当中已集成有个定时调度框架&lt;code>Oozie&lt;/code>，只是实践下来发现其学习成本不低，布署的过程也较复杂。在尝试过其它分布工调度框架后（如阿里的宙斯&lt;code>Zeus&lt;/code>），还是选择了社区较多人使用的&lt;code>Azkaban&lt;/code>。&lt;/p>
&lt;p>&lt;code>Azkaban3&lt;/code>相对于上个版本所做的更改还是比较大的，感兴趣的话可以到其官方网站&lt;a href="https://azkaban.github.io/azkaban/docs/latest/">Azkaban&lt;/a>了解下。接下来主要还是分享下&lt;code>Azkaban3&lt;/code>的安装布署，下面是&lt;code>Azkaban3&lt;/code>的系统架构设计图：&lt;/p>
&lt;p>&lt;img src="http://myblog.lisenhui.cn/2017/09-08-Azkaban-Install00.png-alias" alt="Azkaban-Install00">&lt;/p>
&lt;p>图中的3个组件便是&lt;code>Azkaban3&lt;/code>的重要组成部分：&lt;/p>
&lt;ul>
&lt;li>MySQL关系数据存储数据&lt;/li>
&lt;li>Web Server GUI管理服务提供者&lt;/li>
&lt;li>Executor Server 分布式节点服务布署&lt;/li>
&lt;/ul></description></item><item><title>Hue中集成MySQL数据显示乱码</title><link>https://lisenhui.cn/2017/08/15/hue-rdbms-mysql-chinese.html</link><pubDate>Tue, 15 Aug 2017 15:13:39 +0000</pubDate><guid>https://lisenhui.cn/2017/08/15/hue-rdbms-mysql-chinese.html</guid><description>&lt;p>Hue is a Web applications that enables you to easily interact with an Hadoop cluster. Hue applications let you browse HDFS, Jobs, run Hive, Pig and Cloudera Impala queries, manage the Hive Metastore, HBase, Sqoop, ZooKeeper, MapReduce jobs, and create and schedule worklows with Oozie.&lt;/p>
&lt;p>更加关于HUE的介绍及演示可访问其官方网站：&lt;a href="http://gethue.com">http://gethue.com&lt;/a>&lt;/p></description></item><item><title>Kylin集成Zeppelin展示数据</title><link>https://lisenhui.cn/2017/06/02/kylin-integrate-with-zeppelin.html</link><pubDate>Fri, 02 Jun 2017 18:03:23 +0000</pubDate><guid>https://lisenhui.cn/2017/06/02/kylin-integrate-with-zeppelin.html</guid><description>&lt;p>实际上kylin自带的WEB UI已经集成了建议的图形报表，有常见的线形，柱形及饼图，用于数据的初步展示是完全够用的。如果要更加丰富的展示，那可以考虑使用别的工具，现在就试试官方推荐的Apache Zeppelin。&lt;/p></description></item><item><title>Sqoop工具导入数据到Hive小记</title><link>https://lisenhui.cn/2017/05/24/sqoop-import-data-to-hive.html</link><pubDate>Wed, 24 May 2017 20:18:53 +0000</pubDate><guid>https://lisenhui.cn/2017/05/24/sqoop-import-data-to-hive.html</guid><description>&lt;p>最近正在捣鼓构建数据仓库的事宜，正好有部分维度表的数据需要来自于RDBMS的数据，在HADOOP环境最流行的莫过于Apache的Sqoop工具，按官方的文档操作下来也很顺畅的，不过当要应用到业务场景上时问题便出现了。&lt;/p></description></item><item><title>Zookeeper崩溃后无法加载事务日志</title><link>https://lisenhui.cn/2017/05/15/zookeeper-unload-data-exception.html</link><pubDate>Mon, 15 May 2017 12:34:21 +0000</pubDate><guid>https://lisenhui.cn/2017/05/15/zookeeper-unload-data-exception.html</guid><description>&lt;p>今天在生产的HDP环境中，遇到一件非常诡异的事情。明明搭建了2台zookeeper集群，却是莫明其妙的不见了，而且HDP服务还不报错，认真的检查过环境还是没有找到异常的信息，真是说不明白了。&lt;/p>
&lt;p>言归正传， 还是说说后面遇的问题吧： 生产环境zookeeper崩溃，查看日志发现是磁盘空间已经写满。起初以为是很简单的操作，删除无用的日志文件释放磁盘空间（这是不得不吐槽下HDP的日志文件是超多的，奈何生产环境又不敢不预留长些的时间），然后重启zookeeper满心欢喜的等待着服务恢复正常。然而这次没有看到成功的提示，异常不断各服务连接zookeeper都失败了。这时真的是郁闷了，空间明明已经是充足的。异常信息如下：&lt;/p></description></item><item><title>离线安装HDP2.6(1)-Ambari Server</title><link>https://lisenhui.cn/2017/04/17/offline-install-hdp-ambari-notes.html</link><pubDate>Mon, 17 Apr 2017 19:52:31 +0000</pubDate><guid>https://lisenhui.cn/2017/04/17/offline-install-hdp-ambari-notes.html</guid><description>&lt;h2 id="1参考文档">1.参考文档&lt;/h2>
&lt;p>FYI: &lt;a href="https://docs.hortonworks.com/HDPDocuments/Ambari-2.5.0.3/bk_ambari-installation/content/ch_Getting_Ready.html">HDP Install Documents&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.0/bk_command-line-installation/content/prepare-environment.html#ref-2822d0e9-bd88-4714-910a-750c5b95a996">HDP Install Manual&lt;/a>&lt;/p></description></item></channel></rss>